{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"( latest release ) ( all releases ) The Unity Machine Learning Agents Toolkit (ML-Agents) is an open-source project that enables games and simulations to serve as environments for training intelligent agents. We provide implementations (based on PyTorch) of state-of-the-art algorithms to enable game developers and hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games. Researchers can also use the provided simple-to-use Python API to train Agents using reinforcement learning, imitation learning, neuroevolution, or any other methods. These trained agents can be used for multiple purposes, including controlling NPC behavior (in a variety of settings such as multi-agent and adversarial), automated testing of game builds and evaluating different game design decisions pre-release. The ML-Agents Toolkit is mutually beneficial for both game developers and AI researchers as it provides a central platform where advances in AI can be evaluated on Unity\u2019s rich environments and then made accessible to the wider research and game developer communities. Features 18+ example Unity environments Support for multiple environment configurations and training scenarios Flexible Unity SDK that can be integrated into your game or custom Unity scene Support for training single-agent, multi-agent cooperative, and multi-agent competitive scenarios via several Deep Reinforcement Learning algorithms (PPO, SAC, MA-POCA, self-play). Support for learning from demonstrations through two Imitation Learning algorithms (BC and GAIL). Easily definable Curriculum Learning scenarios for complex tasks Train robust agents using environment randomization Flexible agent control with On Demand Decision Making Train using multiple concurrent Unity environment instances Utilizes the Unity Inference Engine to provide native cross-platform support Unity environment control from Python Wrap Unity learning environments as a gym See our ML-Agents Overview page for detailed descriptions of all these features. Releases & Documentation Our latest, stable release is Release 19 . Click here to get started with the latest release of ML-Agents. The table below lists all our releases, including our main branch which is under active development and may be unstable. A few helpful guidelines: - The Versioning page overviews how we manage our GitHub releases and the versioning process for each of the ML-Agents components. - The Releases page contains details of the changes between releases. - The Migration page contains details on how to upgrade from earlier releases of the ML-Agents Toolkit. - The Documentation links in the table below include installation and usage instructions specific to each release. Remember to always use the documentation that corresponds to the release version you're using. - The com.unity.ml-agents package is verified for Unity 2020.1 and later. Verified packages releases are numbered 1.0.x. Version Release Date Source Documentation Download Python Package Unity Package main (unstable) -- source docs download -- -- Release 19 January 14, 2022 source docs download 0.28.0 2.2.1 Verified Package 1.0.8 May 26, 2021 source docs download 0.16.1 1.0.8 If you are a researcher interested in a discussion of Unity as an AI platform, see a pre-print of our reference paper on Unity and the ML-Agents Toolkit . If you use Unity or the ML-Agents Toolkit to conduct research, we ask that you cite the following paper as a reference: Juliani, A., Berges, V., Teng, E., Cohen, A., Harper, J., Elion, C., Goy, C., Gao, Y., Henry, H., Mattar, M., Lange, D. (2020). Unity: A General Platform for Intelligent Agents. arXiv preprint arXiv:1809.02627 . https://github.com/Unity-Technologies/ml-agents. Additional Resources We have a Unity Learn course, ML-Agents: Hummingbirds , that provides a gentle introduction to Unity and the ML-Agents Toolkit. We've also partnered with CodeMonkeyUnity to create a series of tutorial videos on how to implement and use the ML-Agents Toolkit. We have also published a series of blog posts that are relevant for ML-Agents: (July 12, 2021) ML-Agents plays Dodgeball (May 5, 2021) ML-Agents v2.0 release: Now supports training complex cooperative behaviors (December 28, 2020) Happy holidays from the Unity ML-Agents team! (November 20, 2020) How Eidos-Montr\u00e9al created Grid Sensors to improve observations for training agents (November 11, 2020) 2020 AI@Unity interns shoutout (May 12, 2020) Announcing ML-Agents Unity Package v1.0! (February 28, 2020) Training intelligent adversaries using self-play with ML-Agents (November 11, 2019) Training your agents 7 times faster with ML-Agents (October 21, 2019) The AI@Unity interns help shape the world (April 15, 2019) Unity ML-Agents Toolkit v0.8: Faster training on real games (March 1, 2019) Unity ML-Agents Toolkit v0.7: A leap towards cross-platform inference (December 17, 2018) ML-Agents Toolkit v0.6: Improved usability of Brains and Imitation Learning (October 2, 2018) Puppo, The Corgi: Cuteness Overload with the Unity ML-Agents Toolkit (September 11, 2018) ML-Agents Toolkit v0.5, new resources for AI researchers available now (June 26, 2018) Solving sparse-reward tasks with Curiosity (June 19, 2018) Unity ML-Agents Toolkit v0.4 and Udacity Deep Reinforcement Learning Nanodegree (May 24, 2018) Imitation Learning in Unity: The Workflow (March 15, 2018) ML-Agents Toolkit v0.3 Beta released: Imitation Learning, feedback-driven features, and more (December 11, 2017) Using Machine Learning Agents in a real game: a beginner\u2019s guide (December 8, 2017) Introducing ML-Agents Toolkit v0.2: Curriculum Learning, new environments, and more (September 19, 2017) Introducing: Unity Machine Learning Agents Toolkit Overviewing reinforcement learning concepts ( multi-armed bandit and Q-learning ) More from Unity Unity Robotics Unity Computer Vision Unity Game Simulation Community and Feedback The ML-Agents Toolkit is an open-source project and we encourage and welcome contributions. If you wish to contribute, be sure to review our contribution guidelines and code of conduct . For problems with the installation and setup of the ML-Agents Toolkit, or discussions about how to best setup or train your agents, please create a new thread on the Unity ML-Agents forum and make sure to include as much detail as possible. If you run into any other problems using the ML-Agents Toolkit or have a specific feature request, please submit a GitHub issue . Please tell us which samples you would like to see shipped with the ML-Agents Unity package by replying to this forum thread . Your opinion matters a great deal to us. Only by hearing your thoughts on the Unity ML-Agents Toolkit can we continue to improve and grow. Please take a few minutes to let us know about it . For any other questions or feedback, connect directly with the ML-Agents team at ml-agents@unity3d.com. Privacy In order to improve the developer experience for Unity ML-Agents Toolkit, we have added in-editor analytics. Please refer to \"Information that is passively collected by Unity\" in the Unity Privacy Policy . License Apache License 2.0","title":"Home"},{"location":"#features","text":"18+ example Unity environments Support for multiple environment configurations and training scenarios Flexible Unity SDK that can be integrated into your game or custom Unity scene Support for training single-agent, multi-agent cooperative, and multi-agent competitive scenarios via several Deep Reinforcement Learning algorithms (PPO, SAC, MA-POCA, self-play). Support for learning from demonstrations through two Imitation Learning algorithms (BC and GAIL). Easily definable Curriculum Learning scenarios for complex tasks Train robust agents using environment randomization Flexible agent control with On Demand Decision Making Train using multiple concurrent Unity environment instances Utilizes the Unity Inference Engine to provide native cross-platform support Unity environment control from Python Wrap Unity learning environments as a gym See our ML-Agents Overview page for detailed descriptions of all these features.","title":"Features"},{"location":"#releases-documentation","text":"Our latest, stable release is Release 19 . Click here to get started with the latest release of ML-Agents. The table below lists all our releases, including our main branch which is under active development and may be unstable. A few helpful guidelines: - The Versioning page overviews how we manage our GitHub releases and the versioning process for each of the ML-Agents components. - The Releases page contains details of the changes between releases. - The Migration page contains details on how to upgrade from earlier releases of the ML-Agents Toolkit. - The Documentation links in the table below include installation and usage instructions specific to each release. Remember to always use the documentation that corresponds to the release version you're using. - The com.unity.ml-agents package is verified for Unity 2020.1 and later. Verified packages releases are numbered 1.0.x. Version Release Date Source Documentation Download Python Package Unity Package main (unstable) -- source docs download -- -- Release 19 January 14, 2022 source docs download 0.28.0 2.2.1 Verified Package 1.0.8 May 26, 2021 source docs download 0.16.1 1.0.8 If you are a researcher interested in a discussion of Unity as an AI platform, see a pre-print of our reference paper on Unity and the ML-Agents Toolkit . If you use Unity or the ML-Agents Toolkit to conduct research, we ask that you cite the following paper as a reference: Juliani, A., Berges, V., Teng, E., Cohen, A., Harper, J., Elion, C., Goy, C., Gao, Y., Henry, H., Mattar, M., Lange, D. (2020). Unity: A General Platform for Intelligent Agents. arXiv preprint arXiv:1809.02627 . https://github.com/Unity-Technologies/ml-agents.","title":"Releases &amp; Documentation"},{"location":"#additional-resources","text":"We have a Unity Learn course, ML-Agents: Hummingbirds , that provides a gentle introduction to Unity and the ML-Agents Toolkit. We've also partnered with CodeMonkeyUnity to create a series of tutorial videos on how to implement and use the ML-Agents Toolkit. We have also published a series of blog posts that are relevant for ML-Agents: (July 12, 2021) ML-Agents plays Dodgeball (May 5, 2021) ML-Agents v2.0 release: Now supports training complex cooperative behaviors (December 28, 2020) Happy holidays from the Unity ML-Agents team! (November 20, 2020) How Eidos-Montr\u00e9al created Grid Sensors to improve observations for training agents (November 11, 2020) 2020 AI@Unity interns shoutout (May 12, 2020) Announcing ML-Agents Unity Package v1.0! (February 28, 2020) Training intelligent adversaries using self-play with ML-Agents (November 11, 2019) Training your agents 7 times faster with ML-Agents (October 21, 2019) The AI@Unity interns help shape the world (April 15, 2019) Unity ML-Agents Toolkit v0.8: Faster training on real games (March 1, 2019) Unity ML-Agents Toolkit v0.7: A leap towards cross-platform inference (December 17, 2018) ML-Agents Toolkit v0.6: Improved usability of Brains and Imitation Learning (October 2, 2018) Puppo, The Corgi: Cuteness Overload with the Unity ML-Agents Toolkit (September 11, 2018) ML-Agents Toolkit v0.5, new resources for AI researchers available now (June 26, 2018) Solving sparse-reward tasks with Curiosity (June 19, 2018) Unity ML-Agents Toolkit v0.4 and Udacity Deep Reinforcement Learning Nanodegree (May 24, 2018) Imitation Learning in Unity: The Workflow (March 15, 2018) ML-Agents Toolkit v0.3 Beta released: Imitation Learning, feedback-driven features, and more (December 11, 2017) Using Machine Learning Agents in a real game: a beginner\u2019s guide (December 8, 2017) Introducing ML-Agents Toolkit v0.2: Curriculum Learning, new environments, and more (September 19, 2017) Introducing: Unity Machine Learning Agents Toolkit Overviewing reinforcement learning concepts ( multi-armed bandit and Q-learning )","title":"Additional Resources"},{"location":"#more-from-unity","text":"Unity Robotics Unity Computer Vision Unity Game Simulation","title":"More from Unity"},{"location":"#community-and-feedback","text":"The ML-Agents Toolkit is an open-source project and we encourage and welcome contributions. If you wish to contribute, be sure to review our contribution guidelines and code of conduct . For problems with the installation and setup of the ML-Agents Toolkit, or discussions about how to best setup or train your agents, please create a new thread on the Unity ML-Agents forum and make sure to include as much detail as possible. If you run into any other problems using the ML-Agents Toolkit or have a specific feature request, please submit a GitHub issue . Please tell us which samples you would like to see shipped with the ML-Agents Unity package by replying to this forum thread . Your opinion matters a great deal to us. Only by hearing your thoughts on the Unity ML-Agents Toolkit can we continue to improve and grow. Please take a few minutes to let us know about it . For any other questions or feedback, connect directly with the ML-Agents team at ml-agents@unity3d.com.","title":"Community and Feedback"},{"location":"#privacy","text":"In order to improve the developer experience for Unity ML-Agents Toolkit, we have added in-editor analytics. Please refer to \"Information that is passively collected by Unity\" in the Unity Privacy Policy .","title":"Privacy"},{"location":"#license","text":"Apache License 2.0","title":"License"},{"location":"API-Reference/","text":"API Reference Our developer-facing C# classes have been documented to be compatible with Doxygen for auto-generating HTML documentation. To generate the API reference, download Doxygen and run the following command within the docs/ directory: doxygen dox-ml-agents.conf dox-ml-agents.conf is a Doxygen configuration file for the ML-Agents Toolkit that includes the classes that have been properly formatted. The generated HTML files will be placed in the html/ subdirectory. Open index.html within that subdirectory to navigate to the API reference home. Note that html/ is already included in the repository's .gitignore file. In the near future, we aim to expand our documentation to include the Python classes.","title":"API Reference"},{"location":"API-Reference/#api-reference","text":"Our developer-facing C# classes have been documented to be compatible with Doxygen for auto-generating HTML documentation. To generate the API reference, download Doxygen and run the following command within the docs/ directory: doxygen dox-ml-agents.conf dox-ml-agents.conf is a Doxygen configuration file for the ML-Agents Toolkit that includes the classes that have been properly formatted. The generated HTML files will be placed in the html/ subdirectory. Open index.html within that subdirectory to navigate to the API reference home. Note that html/ is already included in the repository's .gitignore file. In the near future, we aim to expand our documentation to include the Python classes.","title":"API Reference"},{"location":"Background-Machine-Learning/","text":"Background: Machine Learning Given that a number of users of the ML-Agents Toolkit might not have a formal machine learning background, this page provides an overview to facilitate the understanding of the ML-Agents Toolkit. However, we will not attempt to provide a thorough treatment of machine learning as there are fantastic resources online. Machine learning, a branch of artificial intelligence, focuses on learning patterns from data. The three main classes of machine learning algorithms include: unsupervised learning, supervised learning and reinforcement learning. Each class of algorithm learns from a different type of data. The following paragraphs provide an overview for each of these classes of machine learning, as well as introductory examples. Unsupervised Learning The goal of unsupervised learning is to group or cluster similar items in a data set. For example, consider the players of a game. We may want to group the players depending on how engaged they are with the game. This would enable us to target different groups (e.g. for highly-engaged players we might invite them to be beta testers for new features, while for unengaged players we might email them helpful tutorials). Say that we wish to split our players into two groups. We would first define basic attributes of the players, such as the number of hours played, total money spent on in-app purchases and number of levels completed. We can then feed this data set (three attributes for every player) to an unsupervised learning algorithm where we specify the number of groups to be two. The algorithm would then split the data set of players into two groups where the players within each group would be similar to each other. Given the attributes we used to describe each player, in this case, the output would be a split of all the players into two groups, where one group would semantically represent the engaged players and the second group would semantically represent the unengaged players. With unsupervised learning, we did not provide specific examples of which players are considered engaged and which are considered unengaged. We just defined the appropriate attributes and relied on the algorithm to uncover the two groups on its own. This type of data set is typically called an unlabeled data set as it is lacking these direct labels. Consequently, unsupervised learning can be helpful in situations where these labels can be expensive or hard to produce. In the next paragraph, we overview supervised learning algorithms which accept input labels in addition to attributes. Supervised Learning In supervised learning , we do not want to just group similar items but directly learn a mapping from each item to the group (or class) that it belongs to. Returning to our earlier example of clustering players, let's say we now wish to predict which of our players are about to churn (that is stop playing the game for the next 30 days). We can look into our historical records and create a data set that contains attributes of our players in addition to a label indicating whether they have churned or not. Note that the player attributes we use for this churn prediction task may be different from the ones we used for our earlier clustering task. We can then feed this data set (attributes and label for each player) into a supervised learning algorithm which would learn a mapping from the player attributes to a label indicating whether that player will churn or not. The intuition is that the supervised learning algorithm will learn which values of these attributes typically correspond to players who have churned and not churned (for example, it may learn that players who spend very little and play for very short periods will most likely churn). Now given this learned model, we can provide it the attributes of a new player (one that recently started playing the game) and it would output a predicted label for that player. This prediction is the algorithms expectation of whether the player will churn or not. We can now use these predictions to target the players who are expected to churn and entice them to continue playing the game. As you may have noticed, for both supervised and unsupervised learning, there are two tasks that need to be performed: attribute selection and model selection. Attribute selection (also called feature selection) pertains to selecting how we wish to represent the entity of interest, in this case, the player. Model selection, on the other hand, pertains to selecting the algorithm (and its parameters) that perform the task well. Both of these tasks are active areas of machine learning research and, in practice, require several iterations to achieve good performance. We now switch to reinforcement learning, the third class of machine learning algorithms, and arguably the one most relevant for the ML-Agents Toolkit. Reinforcement Learning Reinforcement learning can be viewed as a form of learning for sequential decision making that is commonly associated with controlling robots (but is, in fact, much more general). Consider an autonomous firefighting robot that is tasked with navigating into an area, finding the fire and neutralizing it. At any given moment, the robot perceives the environment through its sensors (e.g. camera, heat, touch), processes this information and produces an action (e.g. move to the left, rotate the water hose, turn on the water). In other words, it is continuously making decisions about how to interact in this environment given its view of the world (i.e. sensors input) and objective (i.e. neutralizing the fire). Teaching a robot to be a successful firefighting machine is precisely what reinforcement learning is designed to do. More specifically, the goal of reinforcement learning is to learn a policy , which is essentially a mapping from observations to actions . An observation is what the robot can measure from its environment (in this case, all its sensory inputs) and an action, in its most raw form, is a change to the configuration of the robot (e.g. position of its base, position of its water hose and whether the hose is on or off). The last remaining piece of the reinforcement learning task is the reward signal . The robot is trained to learn a policy that maximizes its overall rewards. When training a robot to be a mean firefighting machine, we provide it with rewards (positive and negative) indicating how well it is doing on completing the task. Note that the robot does not know how to put out fires before it is trained. It learns the objective because it receives a large positive reward when it puts out the fire and a small negative reward for every passing second. The fact that rewards are sparse (i.e. may not be provided at every step, but only when a robot arrives at a success or failure situation), is a defining characteristic of reinforcement learning and precisely why learning good policies can be difficult (and/or time-consuming) for complex environments. Learning a policy usually requires many trials and iterative policy updates. More specifically, the robot is placed in several fire situations and over time learns an optimal policy which allows it to put out fires more effectively. Obviously, we cannot expect to train a robot repeatedly in the real world, particularly when fires are involved. This is precisely why the use of Unity as a simulator serves as the perfect training grounds for learning such behaviors. While our discussion of reinforcement learning has centered around robots, there are strong parallels between robots and characters in a game. In fact, in many ways, one can view a non-playable character (NPC) as a virtual robot, with its own observations about the environment, its own set of actions and a specific objective. Thus it is natural to explore how we can train behaviors within Unity using reinforcement learning. This is precisely what the ML-Agents Toolkit offers. The video linked below includes a reinforcement learning demo showcasing training character behaviors using the ML-Agents Toolkit. Similar to both unsupervised and supervised learning, reinforcement learning also involves two tasks: attribute selection and model selection. Attribute selection is defining the set of observations for the robot that best help it complete its objective, while model selection is defining the form of the policy (mapping from observations to actions) and its parameters. In practice, training behaviors is an iterative process that may require changing the attribute and model choices. Training and Inference One common aspect of all three branches of machine learning is that they all involve a training phase and an inference phase . While the details of the training and inference phases are different for each of the three, at a high-level, the training phase involves building a model using the provided data, while the inference phase involves applying this model to new, previously unseen, data. More specifically: For our unsupervised learning example, the training phase learns the optimal two clusters based on the data describing existing players, while the inference phase assigns a new player to one of these two clusters. For our supervised learning example, the training phase learns the mapping from player attributes to player label (whether they churned or not), and the inference phase predicts whether a new player will churn or not based on that learned mapping. For our reinforcement learning example, the training phase learns the optimal policy through guided trials, and in the inference phase, the agent observes and takes actions in the wild using its learned policy. To briefly summarize: all three classes of algorithms involve training and inference phases in addition to attribute and model selections. What ultimately separates them is the type of data available to learn from. In unsupervised learning our data set was a collection of attributes, in supervised learning our data set was a collection of attribute-label pairs, and, lastly, in reinforcement learning our data set was a collection of observation-action-reward tuples. Deep Learning Deep learning is a family of algorithms that can be used to address any of the problems introduced above. More specifically, they can be used to solve both attribute and model selection tasks. Deep learning has gained popularity in recent years due to its outstanding performance on several challenging machine learning tasks. One example is AlphaGo , a computer Go program, that leverages deep learning, that was able to beat Lee Sedol (a Go world champion). A key characteristic of deep learning algorithms is their ability to learn very complex functions from large amounts of training data. This makes them a natural choice for reinforcement learning tasks when a large amount of data can be generated, say through the use of a simulator or engine such as Unity. By generating hundreds of thousands of simulations of the environment within Unity, we can learn policies for very complex environments (a complex environment is one where the number of observations an agent perceives and the number of actions they can take are large). Many of the algorithms we provide in ML-Agents use some form of deep learning, built on top of the open-source library, PyTorch .","title":"Machine Learning"},{"location":"Background-Machine-Learning/#background-machine-learning","text":"Given that a number of users of the ML-Agents Toolkit might not have a formal machine learning background, this page provides an overview to facilitate the understanding of the ML-Agents Toolkit. However, we will not attempt to provide a thorough treatment of machine learning as there are fantastic resources online. Machine learning, a branch of artificial intelligence, focuses on learning patterns from data. The three main classes of machine learning algorithms include: unsupervised learning, supervised learning and reinforcement learning. Each class of algorithm learns from a different type of data. The following paragraphs provide an overview for each of these classes of machine learning, as well as introductory examples.","title":"Background: Machine Learning"},{"location":"Background-Machine-Learning/#unsupervised-learning","text":"The goal of unsupervised learning is to group or cluster similar items in a data set. For example, consider the players of a game. We may want to group the players depending on how engaged they are with the game. This would enable us to target different groups (e.g. for highly-engaged players we might invite them to be beta testers for new features, while for unengaged players we might email them helpful tutorials). Say that we wish to split our players into two groups. We would first define basic attributes of the players, such as the number of hours played, total money spent on in-app purchases and number of levels completed. We can then feed this data set (three attributes for every player) to an unsupervised learning algorithm where we specify the number of groups to be two. The algorithm would then split the data set of players into two groups where the players within each group would be similar to each other. Given the attributes we used to describe each player, in this case, the output would be a split of all the players into two groups, where one group would semantically represent the engaged players and the second group would semantically represent the unengaged players. With unsupervised learning, we did not provide specific examples of which players are considered engaged and which are considered unengaged. We just defined the appropriate attributes and relied on the algorithm to uncover the two groups on its own. This type of data set is typically called an unlabeled data set as it is lacking these direct labels. Consequently, unsupervised learning can be helpful in situations where these labels can be expensive or hard to produce. In the next paragraph, we overview supervised learning algorithms which accept input labels in addition to attributes.","title":"Unsupervised Learning"},{"location":"Background-Machine-Learning/#supervised-learning","text":"In supervised learning , we do not want to just group similar items but directly learn a mapping from each item to the group (or class) that it belongs to. Returning to our earlier example of clustering players, let's say we now wish to predict which of our players are about to churn (that is stop playing the game for the next 30 days). We can look into our historical records and create a data set that contains attributes of our players in addition to a label indicating whether they have churned or not. Note that the player attributes we use for this churn prediction task may be different from the ones we used for our earlier clustering task. We can then feed this data set (attributes and label for each player) into a supervised learning algorithm which would learn a mapping from the player attributes to a label indicating whether that player will churn or not. The intuition is that the supervised learning algorithm will learn which values of these attributes typically correspond to players who have churned and not churned (for example, it may learn that players who spend very little and play for very short periods will most likely churn). Now given this learned model, we can provide it the attributes of a new player (one that recently started playing the game) and it would output a predicted label for that player. This prediction is the algorithms expectation of whether the player will churn or not. We can now use these predictions to target the players who are expected to churn and entice them to continue playing the game. As you may have noticed, for both supervised and unsupervised learning, there are two tasks that need to be performed: attribute selection and model selection. Attribute selection (also called feature selection) pertains to selecting how we wish to represent the entity of interest, in this case, the player. Model selection, on the other hand, pertains to selecting the algorithm (and its parameters) that perform the task well. Both of these tasks are active areas of machine learning research and, in practice, require several iterations to achieve good performance. We now switch to reinforcement learning, the third class of machine learning algorithms, and arguably the one most relevant for the ML-Agents Toolkit.","title":"Supervised Learning"},{"location":"Background-Machine-Learning/#reinforcement-learning","text":"Reinforcement learning can be viewed as a form of learning for sequential decision making that is commonly associated with controlling robots (but is, in fact, much more general). Consider an autonomous firefighting robot that is tasked with navigating into an area, finding the fire and neutralizing it. At any given moment, the robot perceives the environment through its sensors (e.g. camera, heat, touch), processes this information and produces an action (e.g. move to the left, rotate the water hose, turn on the water). In other words, it is continuously making decisions about how to interact in this environment given its view of the world (i.e. sensors input) and objective (i.e. neutralizing the fire). Teaching a robot to be a successful firefighting machine is precisely what reinforcement learning is designed to do. More specifically, the goal of reinforcement learning is to learn a policy , which is essentially a mapping from observations to actions . An observation is what the robot can measure from its environment (in this case, all its sensory inputs) and an action, in its most raw form, is a change to the configuration of the robot (e.g. position of its base, position of its water hose and whether the hose is on or off). The last remaining piece of the reinforcement learning task is the reward signal . The robot is trained to learn a policy that maximizes its overall rewards. When training a robot to be a mean firefighting machine, we provide it with rewards (positive and negative) indicating how well it is doing on completing the task. Note that the robot does not know how to put out fires before it is trained. It learns the objective because it receives a large positive reward when it puts out the fire and a small negative reward for every passing second. The fact that rewards are sparse (i.e. may not be provided at every step, but only when a robot arrives at a success or failure situation), is a defining characteristic of reinforcement learning and precisely why learning good policies can be difficult (and/or time-consuming) for complex environments. Learning a policy usually requires many trials and iterative policy updates. More specifically, the robot is placed in several fire situations and over time learns an optimal policy which allows it to put out fires more effectively. Obviously, we cannot expect to train a robot repeatedly in the real world, particularly when fires are involved. This is precisely why the use of Unity as a simulator serves as the perfect training grounds for learning such behaviors. While our discussion of reinforcement learning has centered around robots, there are strong parallels between robots and characters in a game. In fact, in many ways, one can view a non-playable character (NPC) as a virtual robot, with its own observations about the environment, its own set of actions and a specific objective. Thus it is natural to explore how we can train behaviors within Unity using reinforcement learning. This is precisely what the ML-Agents Toolkit offers. The video linked below includes a reinforcement learning demo showcasing training character behaviors using the ML-Agents Toolkit. Similar to both unsupervised and supervised learning, reinforcement learning also involves two tasks: attribute selection and model selection. Attribute selection is defining the set of observations for the robot that best help it complete its objective, while model selection is defining the form of the policy (mapping from observations to actions) and its parameters. In practice, training behaviors is an iterative process that may require changing the attribute and model choices.","title":"Reinforcement Learning"},{"location":"Background-Machine-Learning/#training-and-inference","text":"One common aspect of all three branches of machine learning is that they all involve a training phase and an inference phase . While the details of the training and inference phases are different for each of the three, at a high-level, the training phase involves building a model using the provided data, while the inference phase involves applying this model to new, previously unseen, data. More specifically: For our unsupervised learning example, the training phase learns the optimal two clusters based on the data describing existing players, while the inference phase assigns a new player to one of these two clusters. For our supervised learning example, the training phase learns the mapping from player attributes to player label (whether they churned or not), and the inference phase predicts whether a new player will churn or not based on that learned mapping. For our reinforcement learning example, the training phase learns the optimal policy through guided trials, and in the inference phase, the agent observes and takes actions in the wild using its learned policy. To briefly summarize: all three classes of algorithms involve training and inference phases in addition to attribute and model selections. What ultimately separates them is the type of data available to learn from. In unsupervised learning our data set was a collection of attributes, in supervised learning our data set was a collection of attribute-label pairs, and, lastly, in reinforcement learning our data set was a collection of observation-action-reward tuples.","title":"Training and Inference"},{"location":"Background-Machine-Learning/#deep-learning","text":"Deep learning is a family of algorithms that can be used to address any of the problems introduced above. More specifically, they can be used to solve both attribute and model selection tasks. Deep learning has gained popularity in recent years due to its outstanding performance on several challenging machine learning tasks. One example is AlphaGo , a computer Go program, that leverages deep learning, that was able to beat Lee Sedol (a Go world champion). A key characteristic of deep learning algorithms is their ability to learn very complex functions from large amounts of training data. This makes them a natural choice for reinforcement learning tasks when a large amount of data can be generated, say through the use of a simulator or engine such as Unity. By generating hundreds of thousands of simulations of the environment within Unity, we can learn policies for very complex environments (a complex environment is one where the number of observations an agent perceives and the number of actions they can take are large). Many of the algorithms we provide in ML-Agents use some form of deep learning, built on top of the open-source library, PyTorch .","title":"Deep Learning"},{"location":"Background-PyTorch/","text":"Background: PyTorch As discussed in our machine learning background page , many of the algorithms we provide in the ML-Agents Toolkit leverage some form of deep learning. More specifically, our implementations are built on top of the open-source library PyTorch . In this page we provide a brief overview of PyTorch and TensorBoard that we leverage within the ML-Agents Toolkit. PyTorch PyTorch is an open source library for performing computations using data flow graphs, the underlying representation of deep learning models. It facilitates training and inference on CPUs and GPUs in a desktop, server, or mobile device. Within the ML-Agents Toolkit, when you train the behavior of an agent, the output is a model (.onnx) file that you can then associate with an Agent. Unless you implement a new algorithm, the use of PyTorch is mostly abstracted away and behind the scenes. TensorBoard One component of training models with PyTorch is setting the values of certain model attributes (called hyperparameters ). Finding the right values of these hyperparameters can require a few iterations. Consequently, we leverage a visualization tool called TensorBoard . It allows the visualization of certain agent attributes (e.g. reward) throughout training which can be helpful in both building intuitions for the different hyperparameters and setting the optimal values for your Unity environment. We provide more details on setting the hyperparameters in the Training ML-Agents page. If you are unfamiliar with TensorBoard we recommend our guide on using TensorBoard with ML-Agents or this tutorial .","title":"PyTorch"},{"location":"Background-PyTorch/#background-pytorch","text":"As discussed in our machine learning background page , many of the algorithms we provide in the ML-Agents Toolkit leverage some form of deep learning. More specifically, our implementations are built on top of the open-source library PyTorch . In this page we provide a brief overview of PyTorch and TensorBoard that we leverage within the ML-Agents Toolkit.","title":"Background: PyTorch"},{"location":"Background-PyTorch/#pytorch","text":"PyTorch is an open source library for performing computations using data flow graphs, the underlying representation of deep learning models. It facilitates training and inference on CPUs and GPUs in a desktop, server, or mobile device. Within the ML-Agents Toolkit, when you train the behavior of an agent, the output is a model (.onnx) file that you can then associate with an Agent. Unless you implement a new algorithm, the use of PyTorch is mostly abstracted away and behind the scenes.","title":"PyTorch"},{"location":"Background-PyTorch/#tensorboard","text":"One component of training models with PyTorch is setting the values of certain model attributes (called hyperparameters ). Finding the right values of these hyperparameters can require a few iterations. Consequently, we leverage a visualization tool called TensorBoard . It allows the visualization of certain agent attributes (e.g. reward) throughout training which can be helpful in both building intuitions for the different hyperparameters and setting the optimal values for your Unity environment. We provide more details on setting the hyperparameters in the Training ML-Agents page. If you are unfamiliar with TensorBoard we recommend our guide on using TensorBoard with ML-Agents or this tutorial .","title":"TensorBoard"},{"location":"Background-Unity/","text":"Background: Unity If you are not familiar with the Unity Engine , we highly recommend the Unity Manual and Tutorials page . The Roll-a-ball tutorial is a fantastic resource to learn all the basic concepts of Unity to get started with the ML-Agents Toolkit: Editor Interface Scene GameObject Rigidbody Camera Scripting Physics Ordering of event functions (e.g. FixedUpdate, Update) Prefabs","title":"Unity"},{"location":"Background-Unity/#background-unity","text":"If you are not familiar with the Unity Engine , we highly recommend the Unity Manual and Tutorials page . The Roll-a-ball tutorial is a fantastic resource to learn all the basic concepts of Unity to get started with the ML-Agents Toolkit: Editor Interface Scene GameObject Rigidbody Camera Scripting Physics Ordering of event functions (e.g. FixedUpdate, Update) Prefabs","title":"Background: Unity"},{"location":"Custom-SideChannels/","text":"Custom Side Channels You can create your own side channel in C# and Python and use it to communicate custom data structures between the two. This can be useful for situations in which the data to be sent is too complex or structured for the built-in EnvironmentParameters , or is not related to any specific agent, and therefore inappropriate as an agent observation. Overview In order to use a side channel, it must be implemented as both Unity and Python classes. Unity side The side channel will have to implement the SideChannel abstract class and the following method. OnMessageReceived(IncomingMessage msg) : You must implement this method and read the data from IncomingMessage. The data must be read in the order that it was written. The side channel must also assign a ChannelId property in the constructor. The ChannelId is a Guid (or UUID in Python) used to uniquely identify a side channel. This Guid must be the same on C# and Python. There can only be one side channel of a certain id during communication. To send data from C# to Python, create an OutgoingMessage instance, add data to it, call the base.QueueMessageToSend(msg) method inside the side channel, and call the OutgoingMessage.Dispose() method. To register a side channel on the Unity side, call SideChannelManager.RegisterSideChannel with the side channel as only argument. Python side The side channel will have to implement the SideChannel abstract class. You must implement : on_message_received(self, msg: \"IncomingMessage\") -> None : You must implement this method and read the data from IncomingMessage. The data must be read in the order that it was written. The side channel must also assign a channel_id property in the constructor. The channel_id is a UUID (referred in C# as Guid) used to uniquely identify a side channel. This number must be the same on C# and Python. There can only be one side channel of a certain id during communication. To assign the channel_id call the abstract class constructor with the appropriate channel_id as follows: super().__init__(my_channel_id) To send a byte array from Python to C#, create an OutgoingMessage instance, add data to it, and call the super().queue_message_to_send(msg) method inside the side channel. To register a side channel on the Python side, pass the side channel as argument when creating the UnityEnvironment object. One of the arguments of the constructor ( side_channels ) is a list of side channels. Example implementation Below is a simple implementation of a side channel that will exchange ASCII encoded strings between a Unity environment and Python. Example Unity C# code The first step is to create the StringLogSideChannel class within the Unity project. Here is an implementation of a StringLogSideChannel that will listen for messages from python and print them to the Unity debug log, as well as send error messages from Unity to python. using UnityEngine; using Unity.MLAgents; using Unity.MLAgents.SideChannels; using System.Text; using System; public class StringLogSideChannel : SideChannel { public StringLogSideChannel() { ChannelId = new Guid(\"621f0a70-4f87-11ea-a6bf-784f4387d1f7\"); } protected override void OnMessageReceived(IncomingMessage msg) { var receivedString = msg.ReadString(); Debug.Log(\"From Python : \" + receivedString); } public void SendDebugStatementToPython(string logString, string stackTrace, LogType type) { if (type == LogType.Error) { var stringToSend = type.ToString() + \": \" + logString + \"\\n\" + stackTrace; using (var msgOut = new OutgoingMessage()) { msgOut.WriteString(stringToSend); QueueMessageToSend(msgOut); } } } } Once we have defined our custom side channel class, we need to ensure that it is instantiated and registered. This can typically be done wherever the logic of the side channel makes sense to be associated, for example on a MonoBehaviour object that might need to access data from the side channel. Here we show a simple MonoBehaviour object which instantiates and registers the new side channel. If you have not done it already, make sure that the MonoBehaviour which registers the side channel is attached to a GameObject which will be live in your Unity scene. using UnityEngine; using Unity.MLAgents; public class RegisterStringLogSideChannel : MonoBehaviour { StringLogSideChannel stringChannel; public void Awake() { // We create the Side Channel stringChannel = new StringLogSideChannel(); // When a Debug.Log message is created, we send it to the stringChannel Application.logMessageReceived += stringChannel.SendDebugStatementToPython; // The channel must be registered with the SideChannelManager class SideChannelManager.RegisterSideChannel(stringChannel); } public void OnDestroy() { // De-register the Debug.Log callback Application.logMessageReceived -= stringChannel.SendDebugStatementToPython; if (Academy.IsInitialized){ SideChannelManager.UnregisterSideChannel(stringChannel); } } public void Update() { // Optional : If the space bar is pressed, raise an error ! if (Input.GetKeyDown(KeyCode.Space)) { Debug.LogError(\"This is a fake error. Space bar was pressed in Unity.\"); } } } Example Python code Now that we have created the necessary Unity C# classes, we can create their Python counterparts. from mlagents_envs.environment import UnityEnvironment from mlagents_envs.side_channel.side_channel import ( SideChannel, IncomingMessage, OutgoingMessage, ) import numpy as np import uuid # Create the StringLogChannel class class StringLogChannel(SideChannel): def __init__(self) -> None: super().__init__(uuid.UUID(\"621f0a70-4f87-11ea-a6bf-784f4387d1f7\")) def on_message_received(self, msg: IncomingMessage) -> None: \"\"\" Note: We must implement this method of the SideChannel interface to receive messages from Unity \"\"\" # We simply read a string from the message and print it. print(msg.read_string()) def send_string(self, data: str) -> None: # Add the string to an OutgoingMessage msg = OutgoingMessage() msg.write_string(data) # We call this method to queue the data we want to send super().queue_message_to_send(msg) We can then instantiate the new side channel, launch a UnityEnvironment with that side channel active, and send a series of messages to the Unity environment from Python using it. # Create the channel string_log = StringLogChannel() # We start the communication with the Unity Editor and pass the string_log side channel as input env = UnityEnvironment(side_channels=[string_log]) env.reset() string_log.send_string(\"The environment was reset\") group_name = list(env.behavior_specs.keys())[0] # Get the first group_name group_spec = env.behavior_specs[group_name] for i in range(1000): decision_steps, terminal_steps = env.get_steps(group_name) # We send data to Unity : A string with the number of Agent at each string_log.send_string( f\"Step {i} occurred with {len(decision_steps)} deciding agents and \" f\"{len(terminal_steps)} terminal agents\" ) env.step() # Move the simulation forward env.close() Now, if you run this script and press Play the Unity Editor when prompted, the console in the Unity Editor will display a message at every Python step. Additionally, if you press the Space Bar in the Unity Engine, a message will appear in the terminal.","title":"Custom Side Channels"},{"location":"Custom-SideChannels/#custom-side-channels","text":"You can create your own side channel in C# and Python and use it to communicate custom data structures between the two. This can be useful for situations in which the data to be sent is too complex or structured for the built-in EnvironmentParameters , or is not related to any specific agent, and therefore inappropriate as an agent observation.","title":"Custom Side Channels"},{"location":"Custom-SideChannels/#overview","text":"In order to use a side channel, it must be implemented as both Unity and Python classes.","title":"Overview"},{"location":"Custom-SideChannels/#unity-side","text":"The side channel will have to implement the SideChannel abstract class and the following method. OnMessageReceived(IncomingMessage msg) : You must implement this method and read the data from IncomingMessage. The data must be read in the order that it was written. The side channel must also assign a ChannelId property in the constructor. The ChannelId is a Guid (or UUID in Python) used to uniquely identify a side channel. This Guid must be the same on C# and Python. There can only be one side channel of a certain id during communication. To send data from C# to Python, create an OutgoingMessage instance, add data to it, call the base.QueueMessageToSend(msg) method inside the side channel, and call the OutgoingMessage.Dispose() method. To register a side channel on the Unity side, call SideChannelManager.RegisterSideChannel with the side channel as only argument.","title":"Unity side"},{"location":"Custom-SideChannels/#python-side","text":"The side channel will have to implement the SideChannel abstract class. You must implement : on_message_received(self, msg: \"IncomingMessage\") -> None : You must implement this method and read the data from IncomingMessage. The data must be read in the order that it was written. The side channel must also assign a channel_id property in the constructor. The channel_id is a UUID (referred in C# as Guid) used to uniquely identify a side channel. This number must be the same on C# and Python. There can only be one side channel of a certain id during communication. To assign the channel_id call the abstract class constructor with the appropriate channel_id as follows: super().__init__(my_channel_id) To send a byte array from Python to C#, create an OutgoingMessage instance, add data to it, and call the super().queue_message_to_send(msg) method inside the side channel. To register a side channel on the Python side, pass the side channel as argument when creating the UnityEnvironment object. One of the arguments of the constructor ( side_channels ) is a list of side channels.","title":"Python side"},{"location":"Custom-SideChannels/#example-implementation","text":"Below is a simple implementation of a side channel that will exchange ASCII encoded strings between a Unity environment and Python.","title":"Example implementation"},{"location":"Custom-SideChannels/#example-unity-c-code","text":"The first step is to create the StringLogSideChannel class within the Unity project. Here is an implementation of a StringLogSideChannel that will listen for messages from python and print them to the Unity debug log, as well as send error messages from Unity to python. using UnityEngine; using Unity.MLAgents; using Unity.MLAgents.SideChannels; using System.Text; using System; public class StringLogSideChannel : SideChannel { public StringLogSideChannel() { ChannelId = new Guid(\"621f0a70-4f87-11ea-a6bf-784f4387d1f7\"); } protected override void OnMessageReceived(IncomingMessage msg) { var receivedString = msg.ReadString(); Debug.Log(\"From Python : \" + receivedString); } public void SendDebugStatementToPython(string logString, string stackTrace, LogType type) { if (type == LogType.Error) { var stringToSend = type.ToString() + \": \" + logString + \"\\n\" + stackTrace; using (var msgOut = new OutgoingMessage()) { msgOut.WriteString(stringToSend); QueueMessageToSend(msgOut); } } } } Once we have defined our custom side channel class, we need to ensure that it is instantiated and registered. This can typically be done wherever the logic of the side channel makes sense to be associated, for example on a MonoBehaviour object that might need to access data from the side channel. Here we show a simple MonoBehaviour object which instantiates and registers the new side channel. If you have not done it already, make sure that the MonoBehaviour which registers the side channel is attached to a GameObject which will be live in your Unity scene. using UnityEngine; using Unity.MLAgents; public class RegisterStringLogSideChannel : MonoBehaviour { StringLogSideChannel stringChannel; public void Awake() { // We create the Side Channel stringChannel = new StringLogSideChannel(); // When a Debug.Log message is created, we send it to the stringChannel Application.logMessageReceived += stringChannel.SendDebugStatementToPython; // The channel must be registered with the SideChannelManager class SideChannelManager.RegisterSideChannel(stringChannel); } public void OnDestroy() { // De-register the Debug.Log callback Application.logMessageReceived -= stringChannel.SendDebugStatementToPython; if (Academy.IsInitialized){ SideChannelManager.UnregisterSideChannel(stringChannel); } } public void Update() { // Optional : If the space bar is pressed, raise an error ! if (Input.GetKeyDown(KeyCode.Space)) { Debug.LogError(\"This is a fake error. Space bar was pressed in Unity.\"); } } }","title":"Example Unity C# code"},{"location":"Custom-SideChannels/#example-python-code","text":"Now that we have created the necessary Unity C# classes, we can create their Python counterparts. from mlagents_envs.environment import UnityEnvironment from mlagents_envs.side_channel.side_channel import ( SideChannel, IncomingMessage, OutgoingMessage, ) import numpy as np import uuid # Create the StringLogChannel class class StringLogChannel(SideChannel): def __init__(self) -> None: super().__init__(uuid.UUID(\"621f0a70-4f87-11ea-a6bf-784f4387d1f7\")) def on_message_received(self, msg: IncomingMessage) -> None: \"\"\" Note: We must implement this method of the SideChannel interface to receive messages from Unity \"\"\" # We simply read a string from the message and print it. print(msg.read_string()) def send_string(self, data: str) -> None: # Add the string to an OutgoingMessage msg = OutgoingMessage() msg.write_string(data) # We call this method to queue the data we want to send super().queue_message_to_send(msg) We can then instantiate the new side channel, launch a UnityEnvironment with that side channel active, and send a series of messages to the Unity environment from Python using it. # Create the channel string_log = StringLogChannel() # We start the communication with the Unity Editor and pass the string_log side channel as input env = UnityEnvironment(side_channels=[string_log]) env.reset() string_log.send_string(\"The environment was reset\") group_name = list(env.behavior_specs.keys())[0] # Get the first group_name group_spec = env.behavior_specs[group_name] for i in range(1000): decision_steps, terminal_steps = env.get_steps(group_name) # We send data to Unity : A string with the number of Agent at each string_log.send_string( f\"Step {i} occurred with {len(decision_steps)} deciding agents and \" f\"{len(terminal_steps)} terminal agents\" ) env.step() # Move the simulation forward env.close() Now, if you run this script and press Play the Unity Editor when prompted, the console in the Unity Editor will display a message at every Python step. Additionally, if you press the Space Bar in the Unity Engine, a message will appear in the terminal.","title":"Example Python code"},{"location":"FAQ/","text":"Frequently Asked Questions Installation problems Environment Permission Error If you directly import your Unity environment without building it in the editor, you might need to give it additional permissions to execute it. If you receive such a permission error on macOS, run: chmod -R 755 *.app or on Linux: chmod -R 755 *.x86_64 On Windows, you can find instructions . Environment Connection Timeout If you are able to launch the environment from UnityEnvironment but then receive a timeout error like this: UnityAgentsException: The Communicator was unable to connect. Please make sure the External process is ready to accept communication with Unity. There may be a number of possible causes: Cause : There may be no agent in the scene Cause : On OSX, the firewall may be preventing communication with the environment. Solution : Add the built environment binary to the list of exceptions on the firewall by following instructions . Cause : An error happened in the Unity Environment preventing communication. Solution : Look into the log files generated by the Unity Environment to figure what error happened. Cause : You have assigned HTTP_PROXY and HTTPS_PROXY values in your environment variables. Solution : Remove these values and try again. Cause : You are running in a headless environment (e.g. remotely connected to a server). Solution : Pass --no-graphics to mlagents-learn , or no_graphics=True to RemoteRegistryEntry.make() or the UnityEnvironment initializer. If you need graphics for visual observations, you will need to set up xvfb (or equivalent). Communication port {} still in use If you receive an exception \"Couldn't launch new environment because communication port {} is still in use. \" , you can change the worker number in the Python script when calling UnityEnvironment(file_name=filename, worker_id=X) Mean reward : nan If you receive a message Mean reward : nan when attempting to train a model using PPO, this is due to the episodes of the Learning Environment not terminating. In order to address this, set Max Steps for the Agents within the Scene Inspector to a value greater than 0. Alternatively, it is possible to manually set done conditions for episodes from within scripts for custom episode-terminating events. \"File name\" cannot be opened because the developer cannot be verified. If you have downloaded the repository using the github website on macOS 10.15 (Catalina) or later, you may see this error when attempting to play scenes in the Unity project. Workarounds include installing the package using the Unity Package Manager (this is the officially supported approach - see here ), or following the instructions here to verify the relevant files on your machine on a file-by-file basis.","title":"FAQs"},{"location":"FAQ/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"FAQ/#installation-problems","text":"","title":"Installation problems"},{"location":"FAQ/#environment-permission-error","text":"If you directly import your Unity environment without building it in the editor, you might need to give it additional permissions to execute it. If you receive such a permission error on macOS, run: chmod -R 755 *.app or on Linux: chmod -R 755 *.x86_64 On Windows, you can find instructions .","title":"Environment Permission Error"},{"location":"FAQ/#environment-connection-timeout","text":"If you are able to launch the environment from UnityEnvironment but then receive a timeout error like this: UnityAgentsException: The Communicator was unable to connect. Please make sure the External process is ready to accept communication with Unity. There may be a number of possible causes: Cause : There may be no agent in the scene Cause : On OSX, the firewall may be preventing communication with the environment. Solution : Add the built environment binary to the list of exceptions on the firewall by following instructions . Cause : An error happened in the Unity Environment preventing communication. Solution : Look into the log files generated by the Unity Environment to figure what error happened. Cause : You have assigned HTTP_PROXY and HTTPS_PROXY values in your environment variables. Solution : Remove these values and try again. Cause : You are running in a headless environment (e.g. remotely connected to a server). Solution : Pass --no-graphics to mlagents-learn , or no_graphics=True to RemoteRegistryEntry.make() or the UnityEnvironment initializer. If you need graphics for visual observations, you will need to set up xvfb (or equivalent).","title":"Environment Connection Timeout"},{"location":"FAQ/#communication-port-still-in-use","text":"If you receive an exception \"Couldn't launch new environment because communication port {} is still in use. \" , you can change the worker number in the Python script when calling UnityEnvironment(file_name=filename, worker_id=X)","title":"Communication port {} still in use"},{"location":"FAQ/#mean-reward-nan","text":"If you receive a message Mean reward : nan when attempting to train a model using PPO, this is due to the episodes of the Learning Environment not terminating. In order to address this, set Max Steps for the Agents within the Scene Inspector to a value greater than 0. Alternatively, it is possible to manually set done conditions for episodes from within scripts for custom episode-terminating events.","title":"Mean reward : nan"},{"location":"FAQ/#file-name-cannot-be-opened-because-the-developer-cannot-be-verified","text":"If you have downloaded the repository using the github website on macOS 10.15 (Catalina) or later, you may see this error when attempting to play scenes in the Unity project. Workarounds include installing the package using the Unity Package Manager (this is the officially supported approach - see here ), or following the instructions here to verify the relevant files on your machine on a file-by-file basis.","title":"\"File name\" cannot be opened because the developer cannot be verified."},{"location":"Getting-Started/","text":"Getting Started Guide This guide walks through the end-to-end process of opening one of our example environments in Unity, training an Agent in it, and embedding the trained model into the Unity environment. After reading this tutorial, you should be able to train any of the example environments. If you are not familiar with the Unity Engine , view our Background: Unity page for helpful pointers. Additionally, if you're not familiar with machine learning, view our Background: Machine Learning page for a brief overview and helpful pointers. For this guide, we'll use the 3D Balance Ball environment which contains a number of agent cubes and balls (which are all copies of each other). Each agent cube tries to keep its ball from falling by rotating either horizontally or vertically. In this environment, an agent cube is an Agent that receives a reward for every step that it balances the ball. An agent is also penalized with a negative reward for dropping the ball. The goal of the training process is to have the agents learn to balance the ball on their head. Let's get started! Installation If you haven't already, follow the installation instructions . Afterwards, open the Unity Project that contains all the example environments: Open the Package Manager Window by navigating to Window -> Package Manager in the menu. Navigate to the ML-Agents Package and click on it. Find the 3D Ball sample and click Import . In the Project window, go to the Assets/ML-Agents/Examples/3DBall/Scenes folder and open the 3DBall scene file. Understanding a Unity Environment An agent is an autonomous actor that observes and interacts with an environment . In the context of Unity, an environment is a scene containing one or more Agent objects, and, of course, the other entities that an agent interacts with. Note: In Unity, the base object of everything in a scene is the GameObject . The GameObject is essentially a container for everything else, including behaviors, graphics, physics, etc. To see the components that make up a GameObject, select the GameObject in the Scene window, and open the Inspector window. The Inspector shows every component on a GameObject. The first thing you may notice after opening the 3D Balance Ball scene is that it contains not one, but several agent cubes. Each agent cube in the scene is an independent agent, but they all share the same Behavior. 3D Balance Ball does this to speed up training since all twelve agents contribute to training in parallel. Agent The Agent is the actor that observes and takes actions in the environment. In the 3D Balance Ball environment, the Agent components are placed on the twelve \"Agent\" GameObjects. The base Agent object has a few properties that affect its behavior: Behavior Parameters \u2014 Every Agent must have a Behavior. The Behavior determines how an Agent makes decisions. Max Step \u2014 Defines how many simulation steps can occur before the Agent's episode ends. In 3D Balance Ball, an Agent restarts after 5000 steps. Behavior Parameters : Vector Observation Space Before making a decision, an agent collects its observation about its state in the world. The vector observation is a vector of floating point numbers which contain relevant information for the agent to make decisions. The Behavior Parameters of the 3D Balance Ball example uses a Space Size of 8. This means that the feature vector containing the Agent's observations contains eight elements: the x and z components of the agent cube's rotation and the x , y , and z components of the ball's relative position and velocity. Behavior Parameters : Actions An Agent is given instructions in the form of actions. ML-Agents Toolkit classifies actions into two types: continuous and discrete. The 3D Balance Ball example is programmed to use continuous actions, which are a vector of floating-point numbers that can vary continuously. More specifically, it uses a Space Size of 2 to control the amount of x and z rotations to apply to itself to keep the ball balanced on its head. Running a pre-trained model We include pre-trained models for our agents ( .onnx files) and we use the Unity Inference Engine to run these models inside Unity. In this section, we will use the pre-trained model for the 3D Ball example. In the Project window, go to the Assets/ML-Agents/Examples/3DBall/Prefabs folder. Expand 3DBall and click on the Agent prefab. You should see the Agent prefab in the Inspector window. Note : The platforms in the 3DBall scene were created using the 3DBall prefab. Instead of updating all 12 platforms individually, you can update the 3DBall prefab instead. In the Project window, drag the 3DBall Model located in Assets/ML-Agents/Examples/3DBall/TFModels into the Model property under Behavior Parameters (Script) component in the Agent GameObject Inspector window. You should notice that each Agent under each 3DBall in the Hierarchy windows now contains 3DBall as Model on the Behavior Parameters . Note : You can modify multiple game objects in a scene by selecting them all at once using the search bar in the Scene Hierarchy. Set the Inference Device to use for this model as CPU . Click the Play button in the Unity Editor and you will see the platforms balance the balls using the pre-trained model. Training a new model with Reinforcement Learning While we provide pre-trained models for the agents in this environment, any environment you make yourself will require training agents from scratch to generate a new model file. In this section we will demonstrate how to use the reinforcement learning algorithms that are part of the ML-Agents Python package to accomplish this. We have provided a convenient command mlagents-learn which accepts arguments used to configure both training and inference phases. Training the environment Open a command or terminal window. Navigate to the folder where you cloned the ml-agents repository. Note : If you followed the default installation , then you should be able to run mlagents-learn from any directory. Run mlagents-learn config/ppo/3DBall.yaml --run-id=first3DBallRun . config/ppo/3DBall.yaml is the path to a default training configuration file that we provide. The config/ppo folder includes training configuration files for all our example environments, including 3DBall. run-id is a unique name for this training session. When the message \"Start training by pressing the Play button in the Unity Editor\" is displayed on the screen, you can press the Play button in Unity to start training in the Editor. If mlagents-learn runs correctly and starts training, you should see something like this: INFO:mlagents_envs: 'Ball3DAcademy' started successfully! Unity Academy name: Ball3DAcademy INFO:mlagents_envs:Connected new brain: Unity brain name: 3DBallLearning Number of Visual Observations (per agent): 0 Vector Observation space size (per agent): 8 Number of stacked Vector Observation: 1 INFO:mlagents_envs:Hyperparameters for the PPO Trainer of brain 3DBallLearning: batch_size: 64 beta: 0.001 buffer_size: 12000 epsilon: 0.2 gamma: 0.995 hidden_units: 128 lambd: 0.99 learning_rate: 0.0003 max_steps: 5.0e4 normalize: True num_epoch: 3 num_layers: 2 time_horizon: 1000 sequence_length: 64 summary_freq: 1000 use_recurrent: False memory_size: 256 use_curiosity: False curiosity_strength: 0.01 curiosity_enc_size: 128 output_path: ./results/first3DBallRun/3DBallLearning INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 1000. Mean Reward: 1.242. Std of Reward: 0.746. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 2000. Mean Reward: 1.319. Std of Reward: 0.693. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 3000. Mean Reward: 1.804. Std of Reward: 1.056. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 4000. Mean Reward: 2.151. Std of Reward: 1.432. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 5000. Mean Reward: 3.175. Std of Reward: 2.250. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 6000. Mean Reward: 4.898. Std of Reward: 4.019. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 7000. Mean Reward: 6.716. Std of Reward: 5.125. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 8000. Mean Reward: 12.124. Std of Reward: 11.929. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 9000. Mean Reward: 18.151. Std of Reward: 16.871. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 10000. Mean Reward: 27.284. Std of Reward: 28.667. Training. Note how the Mean Reward value printed to the screen increases as training progresses. This is a positive sign that training is succeeding. Note : You can train using an executable rather than the Editor. To do so, follow the instructions in Using an Executable . Observing Training Progress Once you start training using mlagents-learn in the way described in the previous section, the ml-agents directory will contain a results directory. In order to observe the training process in more detail, you can use TensorBoard. From the command line run: tensorboard --logdir results Then navigate to localhost:6006 in your browser to view the TensorBoard summary statistics as shown below. For the purposes of this section, the most important statistic is Environment/Cumulative Reward which should increase throughout training, eventually converging close to 100 which is the maximum reward the agent can accumulate. Embedding the model into the Unity Environment Once the training process completes, and the training process saves the model (denoted by the Saved Model message) you can add it to the Unity project and use it with compatible Agents (the Agents that generated the model). Note: Do not just close the Unity Window once the Saved Model message appears. Either wait for the training process to close the window or press Ctrl+C at the command-line prompt. If you close the window manually, the .onnx file containing the trained model is not exported into the ml-agents folder. If you've quit the training early using Ctrl+C and want to resume training, run the same command again, appending the --resume flag: mlagents-learn config/ppo/3DBall.yaml --run-id=first3DBallRun --resume Your trained model will be at results/<run-identifier>/<behavior_name>.onnx where <behavior_name> is the name of the Behavior Name of the agents corresponding to the model. This file corresponds to your model's latest checkpoint. You can now embed this trained model into your Agents by following the steps below, which is similar to the steps described above . Move your model file into Project/Assets/ML-Agents/Examples/3DBall/TFModels/ . Open the Unity Editor, and select the 3DBall scene as described above. Select the 3DBall prefab Agent object. Drag the <behavior_name>.onnx file from the Project window of the Editor to the Model placeholder in the Ball3DAgent inspector window. Press the Play button at the top of the Editor. Next Steps For more information on the ML-Agents Toolkit, in addition to helpful background, check out the ML-Agents Toolkit Overview page. For a \"Hello World\" introduction to creating your own Learning Environment, check out the Making a New Learning Environment page. For an overview on the more complex example environments that are provided in this toolkit, check out the Example Environments page. For more information on the various training options available, check out the Training ML-Agents page.","title":"Getting Started Guide"},{"location":"Getting-Started/#getting-started-guide","text":"This guide walks through the end-to-end process of opening one of our example environments in Unity, training an Agent in it, and embedding the trained model into the Unity environment. After reading this tutorial, you should be able to train any of the example environments. If you are not familiar with the Unity Engine , view our Background: Unity page for helpful pointers. Additionally, if you're not familiar with machine learning, view our Background: Machine Learning page for a brief overview and helpful pointers. For this guide, we'll use the 3D Balance Ball environment which contains a number of agent cubes and balls (which are all copies of each other). Each agent cube tries to keep its ball from falling by rotating either horizontally or vertically. In this environment, an agent cube is an Agent that receives a reward for every step that it balances the ball. An agent is also penalized with a negative reward for dropping the ball. The goal of the training process is to have the agents learn to balance the ball on their head. Let's get started!","title":"Getting Started Guide"},{"location":"Getting-Started/#installation","text":"If you haven't already, follow the installation instructions . Afterwards, open the Unity Project that contains all the example environments: Open the Package Manager Window by navigating to Window -> Package Manager in the menu. Navigate to the ML-Agents Package and click on it. Find the 3D Ball sample and click Import . In the Project window, go to the Assets/ML-Agents/Examples/3DBall/Scenes folder and open the 3DBall scene file.","title":"Installation"},{"location":"Getting-Started/#understanding-a-unity-environment","text":"An agent is an autonomous actor that observes and interacts with an environment . In the context of Unity, an environment is a scene containing one or more Agent objects, and, of course, the other entities that an agent interacts with. Note: In Unity, the base object of everything in a scene is the GameObject . The GameObject is essentially a container for everything else, including behaviors, graphics, physics, etc. To see the components that make up a GameObject, select the GameObject in the Scene window, and open the Inspector window. The Inspector shows every component on a GameObject. The first thing you may notice after opening the 3D Balance Ball scene is that it contains not one, but several agent cubes. Each agent cube in the scene is an independent agent, but they all share the same Behavior. 3D Balance Ball does this to speed up training since all twelve agents contribute to training in parallel.","title":"Understanding a Unity Environment"},{"location":"Getting-Started/#agent","text":"The Agent is the actor that observes and takes actions in the environment. In the 3D Balance Ball environment, the Agent components are placed on the twelve \"Agent\" GameObjects. The base Agent object has a few properties that affect its behavior: Behavior Parameters \u2014 Every Agent must have a Behavior. The Behavior determines how an Agent makes decisions. Max Step \u2014 Defines how many simulation steps can occur before the Agent's episode ends. In 3D Balance Ball, an Agent restarts after 5000 steps.","title":"Agent"},{"location":"Getting-Started/#behavior-parameters-vector-observation-space","text":"Before making a decision, an agent collects its observation about its state in the world. The vector observation is a vector of floating point numbers which contain relevant information for the agent to make decisions. The Behavior Parameters of the 3D Balance Ball example uses a Space Size of 8. This means that the feature vector containing the Agent's observations contains eight elements: the x and z components of the agent cube's rotation and the x , y , and z components of the ball's relative position and velocity.","title":"Behavior Parameters : Vector Observation Space"},{"location":"Getting-Started/#behavior-parameters-actions","text":"An Agent is given instructions in the form of actions. ML-Agents Toolkit classifies actions into two types: continuous and discrete. The 3D Balance Ball example is programmed to use continuous actions, which are a vector of floating-point numbers that can vary continuously. More specifically, it uses a Space Size of 2 to control the amount of x and z rotations to apply to itself to keep the ball balanced on its head.","title":"Behavior Parameters : Actions"},{"location":"Getting-Started/#running-a-pre-trained-model","text":"We include pre-trained models for our agents ( .onnx files) and we use the Unity Inference Engine to run these models inside Unity. In this section, we will use the pre-trained model for the 3D Ball example. In the Project window, go to the Assets/ML-Agents/Examples/3DBall/Prefabs folder. Expand 3DBall and click on the Agent prefab. You should see the Agent prefab in the Inspector window. Note : The platforms in the 3DBall scene were created using the 3DBall prefab. Instead of updating all 12 platforms individually, you can update the 3DBall prefab instead. In the Project window, drag the 3DBall Model located in Assets/ML-Agents/Examples/3DBall/TFModels into the Model property under Behavior Parameters (Script) component in the Agent GameObject Inspector window. You should notice that each Agent under each 3DBall in the Hierarchy windows now contains 3DBall as Model on the Behavior Parameters . Note : You can modify multiple game objects in a scene by selecting them all at once using the search bar in the Scene Hierarchy. Set the Inference Device to use for this model as CPU . Click the Play button in the Unity Editor and you will see the platforms balance the balls using the pre-trained model.","title":"Running a pre-trained model"},{"location":"Getting-Started/#training-a-new-model-with-reinforcement-learning","text":"While we provide pre-trained models for the agents in this environment, any environment you make yourself will require training agents from scratch to generate a new model file. In this section we will demonstrate how to use the reinforcement learning algorithms that are part of the ML-Agents Python package to accomplish this. We have provided a convenient command mlagents-learn which accepts arguments used to configure both training and inference phases.","title":"Training a new model with Reinforcement Learning"},{"location":"Getting-Started/#training-the-environment","text":"Open a command or terminal window. Navigate to the folder where you cloned the ml-agents repository. Note : If you followed the default installation , then you should be able to run mlagents-learn from any directory. Run mlagents-learn config/ppo/3DBall.yaml --run-id=first3DBallRun . config/ppo/3DBall.yaml is the path to a default training configuration file that we provide. The config/ppo folder includes training configuration files for all our example environments, including 3DBall. run-id is a unique name for this training session. When the message \"Start training by pressing the Play button in the Unity Editor\" is displayed on the screen, you can press the Play button in Unity to start training in the Editor. If mlagents-learn runs correctly and starts training, you should see something like this: INFO:mlagents_envs: 'Ball3DAcademy' started successfully! Unity Academy name: Ball3DAcademy INFO:mlagents_envs:Connected new brain: Unity brain name: 3DBallLearning Number of Visual Observations (per agent): 0 Vector Observation space size (per agent): 8 Number of stacked Vector Observation: 1 INFO:mlagents_envs:Hyperparameters for the PPO Trainer of brain 3DBallLearning: batch_size: 64 beta: 0.001 buffer_size: 12000 epsilon: 0.2 gamma: 0.995 hidden_units: 128 lambd: 0.99 learning_rate: 0.0003 max_steps: 5.0e4 normalize: True num_epoch: 3 num_layers: 2 time_horizon: 1000 sequence_length: 64 summary_freq: 1000 use_recurrent: False memory_size: 256 use_curiosity: False curiosity_strength: 0.01 curiosity_enc_size: 128 output_path: ./results/first3DBallRun/3DBallLearning INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 1000. Mean Reward: 1.242. Std of Reward: 0.746. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 2000. Mean Reward: 1.319. Std of Reward: 0.693. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 3000. Mean Reward: 1.804. Std of Reward: 1.056. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 4000. Mean Reward: 2.151. Std of Reward: 1.432. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 5000. Mean Reward: 3.175. Std of Reward: 2.250. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 6000. Mean Reward: 4.898. Std of Reward: 4.019. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 7000. Mean Reward: 6.716. Std of Reward: 5.125. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 8000. Mean Reward: 12.124. Std of Reward: 11.929. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 9000. Mean Reward: 18.151. Std of Reward: 16.871. Training. INFO:mlagents.trainers: first3DBallRun: 3DBallLearning: Step: 10000. Mean Reward: 27.284. Std of Reward: 28.667. Training. Note how the Mean Reward value printed to the screen increases as training progresses. This is a positive sign that training is succeeding. Note : You can train using an executable rather than the Editor. To do so, follow the instructions in Using an Executable .","title":"Training the environment"},{"location":"Getting-Started/#observing-training-progress","text":"Once you start training using mlagents-learn in the way described in the previous section, the ml-agents directory will contain a results directory. In order to observe the training process in more detail, you can use TensorBoard. From the command line run: tensorboard --logdir results Then navigate to localhost:6006 in your browser to view the TensorBoard summary statistics as shown below. For the purposes of this section, the most important statistic is Environment/Cumulative Reward which should increase throughout training, eventually converging close to 100 which is the maximum reward the agent can accumulate.","title":"Observing Training Progress"},{"location":"Getting-Started/#embedding-the-model-into-the-unity-environment","text":"Once the training process completes, and the training process saves the model (denoted by the Saved Model message) you can add it to the Unity project and use it with compatible Agents (the Agents that generated the model). Note: Do not just close the Unity Window once the Saved Model message appears. Either wait for the training process to close the window or press Ctrl+C at the command-line prompt. If you close the window manually, the .onnx file containing the trained model is not exported into the ml-agents folder. If you've quit the training early using Ctrl+C and want to resume training, run the same command again, appending the --resume flag: mlagents-learn config/ppo/3DBall.yaml --run-id=first3DBallRun --resume Your trained model will be at results/<run-identifier>/<behavior_name>.onnx where <behavior_name> is the name of the Behavior Name of the agents corresponding to the model. This file corresponds to your model's latest checkpoint. You can now embed this trained model into your Agents by following the steps below, which is similar to the steps described above . Move your model file into Project/Assets/ML-Agents/Examples/3DBall/TFModels/ . Open the Unity Editor, and select the 3DBall scene as described above. Select the 3DBall prefab Agent object. Drag the <behavior_name>.onnx file from the Project window of the Editor to the Model placeholder in the Ball3DAgent inspector window. Press the Play button at the top of the Editor.","title":"Embedding the model into the Unity Environment"},{"location":"Getting-Started/#next-steps","text":"For more information on the ML-Agents Toolkit, in addition to helpful background, check out the ML-Agents Toolkit Overview page. For a \"Hello World\" introduction to creating your own Learning Environment, check out the Making a New Learning Environment page. For an overview on the more complex example environments that are provided in this toolkit, check out the Example Environments page. For more information on the various training options available, check out the Training ML-Agents page.","title":"Next Steps"},{"location":"Glossary/","text":"ML-Agents Toolkit Glossary Academy - Singleton object which controls timing, reset, and training/inference settings of the environment. Action - The carrying-out of a decision on the part of an agent within the environment. Agent - Unity Component which produces observations and takes actions in the environment. Agents actions are determined by decisions produced by a Policy. Decision - The specification produced by a Policy for an action to be carried out given an observation. Editor - The Unity Editor, which may include any pane (e.g. Hierarchy, Scene, Inspector). Environment - The Unity scene which contains Agents. Experience - Corresponds to a tuple of [Agent observations, actions, rewards] of a single Agent obtained after a Step. External Coordinator - ML-Agents class responsible for communication with outside processes (in this case, the Python API). FixedUpdate - Unity method called each time the game engine is stepped. ML-Agents logic should be placed here. Frame - An instance of rendering the main camera for the display. Corresponds to each Update call of the game engine. Observation - Partial information describing the state of the environment available to a given agent. (e.g. Vector, Visual) Policy - The decision making mechanism for producing decisions from observations, typically a neural network model. Reward - Signal provided at every step used to indicate desirability of an agent\u2019s action within the current state of the environment. State - The underlying properties of the environment (including all agents within it) at a given time. Step - Corresponds to an atomic change of the engine that happens between Agent decisions. Trainer - Python class which is responsible for training a given group of Agents. Update - Unity function called each time a frame is rendered. ML-Agents logic should not be placed here.","title":"ML-Agents Toolkit Glossary"},{"location":"Glossary/#ml-agents-toolkit-glossary","text":"Academy - Singleton object which controls timing, reset, and training/inference settings of the environment. Action - The carrying-out of a decision on the part of an agent within the environment. Agent - Unity Component which produces observations and takes actions in the environment. Agents actions are determined by decisions produced by a Policy. Decision - The specification produced by a Policy for an action to be carried out given an observation. Editor - The Unity Editor, which may include any pane (e.g. Hierarchy, Scene, Inspector). Environment - The Unity scene which contains Agents. Experience - Corresponds to a tuple of [Agent observations, actions, rewards] of a single Agent obtained after a Step. External Coordinator - ML-Agents class responsible for communication with outside processes (in this case, the Python API). FixedUpdate - Unity method called each time the game engine is stepped. ML-Agents logic should be placed here. Frame - An instance of rendering the main camera for the display. Corresponds to each Update call of the game engine. Observation - Partial information describing the state of the environment available to a given agent. (e.g. Vector, Visual) Policy - The decision making mechanism for producing decisions from observations, typically a neural network model. Reward - Signal provided at every step used to indicate desirability of an agent\u2019s action within the current state of the environment. State - The underlying properties of the environment (including all agents within it) at a given time. Step - Corresponds to an atomic change of the engine that happens between Agent decisions. Trainer - Python class which is responsible for training a given group of Agents. Update - Unity function called each time a frame is rendered. ML-Agents logic should not be placed here.","title":"ML-Agents Toolkit Glossary"},{"location":"Installation-Anaconda-Windows/","text":"Installing ML-Agents Toolkit for Windows (Deprecated) :warning: Note: We no longer use this guide ourselves and so it may not work correctly. We've decided to keep it up just in case it is helpful to you. The ML-Agents Toolkit supports Windows 10. While it might be possible to run the ML-Agents Toolkit using other versions of Windows, it has not been tested on other versions. Furthermore, the ML-Agents Toolkit has not been tested on a Windows VM such as Bootcamp or Parallels. To use the ML-Agents Toolkit, you install Python and the required Python packages as outlined below. This guide also covers how set up GPU-based training (for advanced users). GPU-based training is not currently required for the ML-Agents Toolkit. However, training on a GPU might be required by future versions and features. Step 1: Install Python via Anaconda Download and install Anaconda for Windows. By using Anaconda, you can manage separate environments for different distributions of Python. Python 3.6.1 or higher is required as we no longer support Python 2. In this guide, we are using Python version 3.6 and Anaconda version 5.1 ( 64-bit or 32-bit direct links). We recommend the default advanced installation options . However, select the options appropriate for your specific situation. After installation, you must open Anaconda Navigator to finish the setup. From the Windows search bar, type anaconda navigator . You can close Anaconda Navigator after it opens. If environment variables were not created, you will see error \"conda is not recognized as internal or external command\" when you type conda into the command line. To solve this you will need to set the environment variable correctly. Type environment variables in the search bar (this can be reached by hitting the Windows key or the bottom left Windows button). You should see an option called Edit the system environment variables . From here, click the Environment Variables button. Double click \"Path\" under System variable to edit the \"Path\" variable, click New to add the following new paths. %UserProfile%\\Anaconda3\\Scripts %UserProfile%\\Anaconda3\\Scripts\\conda.exe %UserProfile%\\Anaconda3 %UserProfile%\\Anaconda3\\python.exe Step 2: Setup and Activate a New Conda Environment You will create a new Conda environment to be used with the ML-Agents Toolkit. This means that all the packages that you install are localized to just this environment. It will not affect any other installation of Python or other environments. Whenever you want to run ML-Agents, you will need activate this Conda environment. To create a new Conda environment, open a new Anaconda Prompt ( Anaconda Prompt in the search bar) and type in the following command: conda create -n ml-agents python=3.6 You may be asked to install new packages. Type y and press enter (make sure you are connected to the Internet) . You must install these required packages. The new Conda environment is called ml-agents and uses Python version 3.6. To use this environment, you must activate it. (To use this environment In the future, you can run the same command) . In the same Anaconda Prompt, type in the following command: activate ml-agents You should see (ml-agents) prepended on the last line. Next, install tensorflow . Install this package using pip - which is a package management system used to install Python packages. Latest versions of TensorFlow won't work, so you will need to make sure that you install version 1.7.1. In the same Anaconda Prompt, type in the following command (make sure you are connected to the Internet) : pip install tensorflow==1.7.1 Step 3: Install Required Python Packages The ML-Agents Toolkit depends on a number of Python packages. Use pip to install these Python dependencies. If you haven't already, clone the ML-Agents Toolkit Github repository to your local computer. You can do this using Git ( download here ) and running the following commands in an Anaconda Prompt (if you open a new prompt, be sure to activate the ml-agents Conda environment by typing activate ml-agents ) : git clone --branch release_19 https://github.com/Unity-Technologies/ml-agents.git The --branch release_19 option will switch to the tag of the latest stable release. Omitting that will get the main branch which is potentially unstable. If you don't want to use Git, you can find download links on the releases page . The com.unity.ml-agents subdirectory contains the core code to add to your projects. The Project subdirectory contains many example environments to help you get started. The ml-agents subdirectory contains a Python package which provides deep reinforcement learning trainers to use with Unity environments. The ml-agents-envs subdirectory contains a Python API to interface with Unity, which the ml-agents package depends on. The gym-unity subdirectory contains a package to interface with OpenAI Gym. Keep in mind where the files were downloaded, as you will need the trainer config files in this directory when running mlagents-learn . Make sure you are connected to the Internet and then type in the Anaconda Prompt: python -m pip install mlagents==0.28.0 This will complete the installation of all the required Python packages to run the ML-Agents Toolkit. Sometimes on Windows, when you use pip to install certain Python packages, the pip will get stuck when trying to read the cache of the package. If you see this, you can try: python -m pip install mlagents==0.28.0 --no-cache-dir This --no-cache-dir tells the pip to disable the cache. Installing for Development If you intend to make modifications to ml-agents or ml-agents-envs , you should install the packages from the cloned repo rather than from PyPi. To do this, you will need to install ml-agents and ml-agents-envs separately. In our example, the files are located in C:\\Downloads . After you have either cloned or downloaded the files, from the Anaconda Prompt, change to the ml-agents subdirectory inside the ml-agents directory: cd C:\\Downloads\\ml-agents From the repo's main directory, now run: cd ml-agents-envs pip install -e . cd .. cd ml-agents pip install -e . Running pip with the -e flag will let you make changes to the Python files directly and have those reflected when you run mlagents-learn . It is important to install these packages in this order as the mlagents package depends on mlagents_envs , and installing it in the other order will download mlagents_envs from PyPi. (Optional) Step 4: GPU Training using The ML-Agents Toolkit GPU is not required for the ML-Agents Toolkit and won't speed up the PPO algorithm a lot during training(but something in the future will benefit from GPU). This is a guide for advanced users who want to train using GPUs. Additionally, you will need to check if your GPU is CUDA compatible. Please check Nvidia's page here . Currently for the ML-Agents Toolkit, only CUDA v9.0 and cuDNN v7.0.5 is supported. Install Nvidia CUDA toolkit Download and install the CUDA toolkit 9.0 from Nvidia's archive. The toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ (Step Visual Studio 2017) compiler and a runtime library and is needed to run the ML-Agents Toolkit. In this guide, we are using version 9.0.176 ). Before installing, please make sure you close any running instances of Unity or Visual Studio . Run the installer and select the Express option. Note the directory where you installed the CUDA toolkit. In this guide, we installed in the directory C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0 Install Nvidia cuDNN library Download and install the cuDNN library from Nvidia. cuDNN is a GPU-accelerated library of primitives for deep neural networks. Before you can download, you will need to sign up for free to the Nvidia Developer Program. Once you've signed up, go back to the cuDNN downloads page . You may or may not be asked to fill out a short survey. When you get to the list cuDNN releases, make sure you are downloading the right version for the CUDA toolkit you installed in Step 1. In this guide, we are using version 7.0.5 for CUDA toolkit version 9.0 ( direct link ). After you have downloaded the cuDNN files, you will need to extract the files into the CUDA toolkit directory. In the cuDNN zip file, there are three folders called bin , include , and lib . Copy these three folders into the CUDA toolkit directory. The CUDA toolkit directory is located at C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0 Set Environment Variables You will need to add one environment variable and two path variables. To set the environment variable, type environment variables in the search bar (this can be reached by hitting the Windows key or the bottom left Windows button). You should see an option called Edit the system environment variables . From here, click the Environment Variables button. Click New to add a new system variable (make sure you do this under System variables and not User variables . For Variable Name , enter CUDA_HOME . For the variable value, put the directory location for the CUDA toolkit. In this guide, the directory location is C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0 . Press OK once. To set the two path variables, inside the same Environment Variables window and under the second box called System Variables , find a variable called Path and click Edit . You will add two directories to the list. For this guide, the two entries would look like: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\lib\\x64 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\extras\\CUPTI\\libx64 Make sure to replace the relevant directory location with the one you have installed. Please note that case sensitivity matters . Install TensorFlow GPU Next, install tensorflow-gpu using pip . You'll need version 1.7.1. In an Anaconda Prompt with the Conda environment ml-agents activated, type in the following command to uninstall TensorFlow for cpu and install TensorFlow for gpu (make sure you are connected to the Internet) : pip uninstall tensorflow pip install tensorflow-gpu==1.7.1 Lastly, you should test to see if everything installed properly and that TensorFlow can identify your GPU. In the same Anaconda Prompt, open Python in the Prompt by calling: python And then type the following commands: import tensorflow as tf sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)) You should see something similar to: Found device 0 with properties ... Acknowledgments We would like to thank Jason Weimann and Nitish S. Mutha for writing the original articles which were used to create this guide.","title":"Installing ML-Agents Toolkit for Windows (Deprecated)"},{"location":"Installation-Anaconda-Windows/#installing-ml-agents-toolkit-for-windows-deprecated","text":":warning: Note: We no longer use this guide ourselves and so it may not work correctly. We've decided to keep it up just in case it is helpful to you. The ML-Agents Toolkit supports Windows 10. While it might be possible to run the ML-Agents Toolkit using other versions of Windows, it has not been tested on other versions. Furthermore, the ML-Agents Toolkit has not been tested on a Windows VM such as Bootcamp or Parallels. To use the ML-Agents Toolkit, you install Python and the required Python packages as outlined below. This guide also covers how set up GPU-based training (for advanced users). GPU-based training is not currently required for the ML-Agents Toolkit. However, training on a GPU might be required by future versions and features.","title":"Installing ML-Agents Toolkit for Windows (Deprecated)"},{"location":"Installation-Anaconda-Windows/#step-1-install-python-via-anaconda","text":"Download and install Anaconda for Windows. By using Anaconda, you can manage separate environments for different distributions of Python. Python 3.6.1 or higher is required as we no longer support Python 2. In this guide, we are using Python version 3.6 and Anaconda version 5.1 ( 64-bit or 32-bit direct links). We recommend the default advanced installation options . However, select the options appropriate for your specific situation. After installation, you must open Anaconda Navigator to finish the setup. From the Windows search bar, type anaconda navigator . You can close Anaconda Navigator after it opens. If environment variables were not created, you will see error \"conda is not recognized as internal or external command\" when you type conda into the command line. To solve this you will need to set the environment variable correctly. Type environment variables in the search bar (this can be reached by hitting the Windows key or the bottom left Windows button). You should see an option called Edit the system environment variables . From here, click the Environment Variables button. Double click \"Path\" under System variable to edit the \"Path\" variable, click New to add the following new paths. %UserProfile%\\Anaconda3\\Scripts %UserProfile%\\Anaconda3\\Scripts\\conda.exe %UserProfile%\\Anaconda3 %UserProfile%\\Anaconda3\\python.exe","title":"Step 1: Install Python via Anaconda"},{"location":"Installation-Anaconda-Windows/#step-2-setup-and-activate-a-new-conda-environment","text":"You will create a new Conda environment to be used with the ML-Agents Toolkit. This means that all the packages that you install are localized to just this environment. It will not affect any other installation of Python or other environments. Whenever you want to run ML-Agents, you will need activate this Conda environment. To create a new Conda environment, open a new Anaconda Prompt ( Anaconda Prompt in the search bar) and type in the following command: conda create -n ml-agents python=3.6 You may be asked to install new packages. Type y and press enter (make sure you are connected to the Internet) . You must install these required packages. The new Conda environment is called ml-agents and uses Python version 3.6. To use this environment, you must activate it. (To use this environment In the future, you can run the same command) . In the same Anaconda Prompt, type in the following command: activate ml-agents You should see (ml-agents) prepended on the last line. Next, install tensorflow . Install this package using pip - which is a package management system used to install Python packages. Latest versions of TensorFlow won't work, so you will need to make sure that you install version 1.7.1. In the same Anaconda Prompt, type in the following command (make sure you are connected to the Internet) : pip install tensorflow==1.7.1","title":"Step 2: Setup and Activate a New Conda Environment"},{"location":"Installation-Anaconda-Windows/#step-3-install-required-python-packages","text":"The ML-Agents Toolkit depends on a number of Python packages. Use pip to install these Python dependencies. If you haven't already, clone the ML-Agents Toolkit Github repository to your local computer. You can do this using Git ( download here ) and running the following commands in an Anaconda Prompt (if you open a new prompt, be sure to activate the ml-agents Conda environment by typing activate ml-agents ) : git clone --branch release_19 https://github.com/Unity-Technologies/ml-agents.git The --branch release_19 option will switch to the tag of the latest stable release. Omitting that will get the main branch which is potentially unstable. If you don't want to use Git, you can find download links on the releases page . The com.unity.ml-agents subdirectory contains the core code to add to your projects. The Project subdirectory contains many example environments to help you get started. The ml-agents subdirectory contains a Python package which provides deep reinforcement learning trainers to use with Unity environments. The ml-agents-envs subdirectory contains a Python API to interface with Unity, which the ml-agents package depends on. The gym-unity subdirectory contains a package to interface with OpenAI Gym. Keep in mind where the files were downloaded, as you will need the trainer config files in this directory when running mlagents-learn . Make sure you are connected to the Internet and then type in the Anaconda Prompt: python -m pip install mlagents==0.28.0 This will complete the installation of all the required Python packages to run the ML-Agents Toolkit. Sometimes on Windows, when you use pip to install certain Python packages, the pip will get stuck when trying to read the cache of the package. If you see this, you can try: python -m pip install mlagents==0.28.0 --no-cache-dir This --no-cache-dir tells the pip to disable the cache.","title":"Step 3: Install Required Python Packages"},{"location":"Installation-Anaconda-Windows/#installing-for-development","text":"If you intend to make modifications to ml-agents or ml-agents-envs , you should install the packages from the cloned repo rather than from PyPi. To do this, you will need to install ml-agents and ml-agents-envs separately. In our example, the files are located in C:\\Downloads . After you have either cloned or downloaded the files, from the Anaconda Prompt, change to the ml-agents subdirectory inside the ml-agents directory: cd C:\\Downloads\\ml-agents From the repo's main directory, now run: cd ml-agents-envs pip install -e . cd .. cd ml-agents pip install -e . Running pip with the -e flag will let you make changes to the Python files directly and have those reflected when you run mlagents-learn . It is important to install these packages in this order as the mlagents package depends on mlagents_envs , and installing it in the other order will download mlagents_envs from PyPi.","title":"Installing for Development"},{"location":"Installation-Anaconda-Windows/#optional-step-4-gpu-training-using-the-ml-agents-toolkit","text":"GPU is not required for the ML-Agents Toolkit and won't speed up the PPO algorithm a lot during training(but something in the future will benefit from GPU). This is a guide for advanced users who want to train using GPUs. Additionally, you will need to check if your GPU is CUDA compatible. Please check Nvidia's page here . Currently for the ML-Agents Toolkit, only CUDA v9.0 and cuDNN v7.0.5 is supported.","title":"(Optional) Step 4: GPU Training using The ML-Agents Toolkit"},{"location":"Installation-Anaconda-Windows/#install-nvidia-cuda-toolkit","text":"Download and install the CUDA toolkit 9.0 from Nvidia's archive. The toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ (Step Visual Studio 2017) compiler and a runtime library and is needed to run the ML-Agents Toolkit. In this guide, we are using version 9.0.176 ). Before installing, please make sure you close any running instances of Unity or Visual Studio . Run the installer and select the Express option. Note the directory where you installed the CUDA toolkit. In this guide, we installed in the directory C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0","title":"Install Nvidia CUDA toolkit"},{"location":"Installation-Anaconda-Windows/#install-nvidia-cudnn-library","text":"Download and install the cuDNN library from Nvidia. cuDNN is a GPU-accelerated library of primitives for deep neural networks. Before you can download, you will need to sign up for free to the Nvidia Developer Program. Once you've signed up, go back to the cuDNN downloads page . You may or may not be asked to fill out a short survey. When you get to the list cuDNN releases, make sure you are downloading the right version for the CUDA toolkit you installed in Step 1. In this guide, we are using version 7.0.5 for CUDA toolkit version 9.0 ( direct link ). After you have downloaded the cuDNN files, you will need to extract the files into the CUDA toolkit directory. In the cuDNN zip file, there are three folders called bin , include , and lib . Copy these three folders into the CUDA toolkit directory. The CUDA toolkit directory is located at C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0","title":"Install Nvidia cuDNN library"},{"location":"Installation-Anaconda-Windows/#set-environment-variables","text":"You will need to add one environment variable and two path variables. To set the environment variable, type environment variables in the search bar (this can be reached by hitting the Windows key or the bottom left Windows button). You should see an option called Edit the system environment variables . From here, click the Environment Variables button. Click New to add a new system variable (make sure you do this under System variables and not User variables . For Variable Name , enter CUDA_HOME . For the variable value, put the directory location for the CUDA toolkit. In this guide, the directory location is C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0 . Press OK once. To set the two path variables, inside the same Environment Variables window and under the second box called System Variables , find a variable called Path and click Edit . You will add two directories to the list. For this guide, the two entries would look like: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\lib\\x64 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\extras\\CUPTI\\libx64 Make sure to replace the relevant directory location with the one you have installed. Please note that case sensitivity matters .","title":"Set Environment Variables"},{"location":"Installation-Anaconda-Windows/#install-tensorflow-gpu","text":"Next, install tensorflow-gpu using pip . You'll need version 1.7.1. In an Anaconda Prompt with the Conda environment ml-agents activated, type in the following command to uninstall TensorFlow for cpu and install TensorFlow for gpu (make sure you are connected to the Internet) : pip uninstall tensorflow pip install tensorflow-gpu==1.7.1 Lastly, you should test to see if everything installed properly and that TensorFlow can identify your GPU. In the same Anaconda Prompt, open Python in the Prompt by calling: python And then type the following commands: import tensorflow as tf sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)) You should see something similar to: Found device 0 with properties ...","title":"Install TensorFlow GPU"},{"location":"Installation-Anaconda-Windows/#acknowledgments","text":"We would like to thank Jason Weimann and Nitish S. Mutha for writing the original articles which were used to create this guide.","title":"Acknowledgments"},{"location":"Installation/","text":"Installation The ML-Agents Toolkit contains several components: Unity package ( com.unity.ml-agents ) contains the Unity C# SDK that will be integrated into your Unity project. This package contains a sample to help you get started with ML-Agents. Unity package ( com.unity.ml-agents.extensions ) contains experimental C#/Unity components that are not yet ready to be part of the base com.unity.ml-agents package. com.unity.ml-agents.extensions has a direct dependency on com.unity.ml-agents . Three Python packages: mlagents contains the machine learning algorithms that enables you to train behaviors in your Unity scene. Most users of ML-Agents will only need to directly install mlagents . mlagents_envs contains a Python API to interact with a Unity scene. It is a foundational layer that facilitates data messaging between Unity scene and the Python machine learning algorithms. Consequently, mlagents depends on mlagents_envs . gym_unity provides a Python-wrapper for your Unity scene that supports the OpenAI Gym interface. Unity Project that contains several example environments that highlight the various features of the toolkit to help you get started. Consequently, to install and use the ML-Agents Toolkit you will need to: Install Unity (2020.3 or later) Install Python (3.6.1 or higher) Clone this repository (Optional) Note: If you do not clone the repository, then you will not be able to access the example environments and training configurations or the com.unity.ml-agents.extensions package. Additionally, the Getting Started Guide assumes that you have cloned the repository. Install the com.unity.ml-agents Unity package Install the com.unity.ml-agents.extensions Unity package (Optional) Install the mlagents Python package Install Unity 2020.3 or Later Download and install Unity. We strongly recommend that you install Unity through the Unity Hub as it will enable you to manage multiple Unity versions. Install Python 3.6.1 or Higher We recommend installing Python 3.6 or 3.7. If you are using Windows, please install the x86-64 version and not x86. If your Python environment doesn't include pip3 , see these instructions on installing it. Clone the ML-Agents Toolkit Repository (Optional) Now that you have installed Unity and Python, you can now install the Unity and Python packages. You do not need to clone the repository to install those packages, but you may choose to clone the repository if you'd like download our example environments and training configurations to experiment with them (some of our tutorials / guides assume you have access to our example environments). NOTE: There are samples shipped with the Unity Package. You only need to clone the repository if you would like to explore more examples. git clone --branch release_19 https://github.com/Unity-Technologies/ml-agents.git The --branch release_19 option will switch to the tag of the latest stable release. Omitting that will get the main branch which is potentially unstable. Advanced: Local Installation for Development You will need to clone the repository if you plan to modify or extend the ML-Agents Toolkit for your purposes. If you plan to contribute those changes back, make sure to clone the main branch (by omitting --branch release_19 from the command above). See our Contributions Guidelines for more information on contributing to the ML-Agents Toolkit. Install the com.unity.ml-agents Unity package The Unity ML-Agents C# SDK is a Unity Package. You can install the com.unity.ml-agents package directly from the Package Manager registry . Please make sure you enable 'Preview Packages' in the 'Advanced' dropdown in order to find the latest Preview release of the package. NOTE: If you do not see the ML-Agents package listed in the Package Manager please follow the advanced installation instructions below. Advanced: Local Installation for Development You can add the local com.unity.ml-agents package (from the repository that you just cloned) to your project by: navigating to the menu Window -> Package Manager . In the package manager window click on the + button on the top left of the packages list). Select Add package from disk... Navigate into the com.unity.ml-agents folder. Select the package.json file. If you are going to follow the examples from our documentation, you can open the Project folder in Unity and start tinkering immediately. Install the com.unity.ml-agents.extensions Unity package (Optional) To install the com.unity.ml-agents.extensions package, you need to first clone the repo and then complete a local installation similar to what was outlined in the previous Advanced: Local Installation for Development section. Complete installation steps can be found in the package documentation . Install the mlagents Python package Installing the mlagents Python package involves installing other Python packages that mlagents depends on. So you may run into installation issues if your machine has older versions of any of those dependencies already installed. Consequently, our supported path for installing mlagents is to leverage Python Virtual Environments. Virtual Environments provide a mechanism for isolating the dependencies for each project and are supported on Mac / Windows / Linux. We offer a dedicated guide on Virtual Environments . (Windows) Installing PyTorch On Windows, you'll have to install the PyTorch package separately prior to installing ML-Agents. Activate your virtual environment and run from the command line: pip3 install torch~=1.7.1 -f https://download.pytorch.org/whl/torch_stable.html Note that on Windows, you may also need Microsoft's Visual C++ Redistributable if you don't have it already. See the PyTorch installation guide for more installation options and versions. Installing mlagents To install the mlagents Python package, activate your virtual environment and run from the command line: python -m pip install mlagents==0.28.0 Note that this will install mlagents from PyPi, not from the cloned repository. If you installed this correctly, you should be able to run mlagents-learn --help , after which you will see the command line parameters you can use with mlagents-learn . By installing the mlagents package, the dependencies listed in the setup.py file are also installed. These include PyTorch (Requires a CPU w/ AVX support). Advanced: Local Installation for Development If you intend to make modifications to mlagents or mlagents_envs , you should install the packages from the cloned repository rather than from PyPi. To do this, you will need to install mlagents and mlagents_envs separately. From the repository's root directory, run: pip3 install torch -f https://download.pytorch.org/whl/torch_stable.html pip3 install -e ./ml-agents-envs pip3 install -e ./ml-agents Running pip with the -e flag will let you make changes to the Python files directly and have those reflected when you run mlagents-learn . It is important to install these packages in this order as the mlagents package depends on mlagents_envs , and installing it in the other order will download mlagents_envs from PyPi. Next Steps The Getting Started guide contains several short tutorials on setting up the ML-Agents Toolkit within Unity, running a pre-trained model, in addition to building and training environments. Help If you run into any problems regarding ML-Agents, refer to our FAQ and our Limitations pages. If you can't find anything please submit an issue and make sure to cite relevant information on OS, Python version, and exact error message (whenever possible).","title":"Installation"},{"location":"Installation/#installation","text":"The ML-Agents Toolkit contains several components: Unity package ( com.unity.ml-agents ) contains the Unity C# SDK that will be integrated into your Unity project. This package contains a sample to help you get started with ML-Agents. Unity package ( com.unity.ml-agents.extensions ) contains experimental C#/Unity components that are not yet ready to be part of the base com.unity.ml-agents package. com.unity.ml-agents.extensions has a direct dependency on com.unity.ml-agents . Three Python packages: mlagents contains the machine learning algorithms that enables you to train behaviors in your Unity scene. Most users of ML-Agents will only need to directly install mlagents . mlagents_envs contains a Python API to interact with a Unity scene. It is a foundational layer that facilitates data messaging between Unity scene and the Python machine learning algorithms. Consequently, mlagents depends on mlagents_envs . gym_unity provides a Python-wrapper for your Unity scene that supports the OpenAI Gym interface. Unity Project that contains several example environments that highlight the various features of the toolkit to help you get started. Consequently, to install and use the ML-Agents Toolkit you will need to: Install Unity (2020.3 or later) Install Python (3.6.1 or higher) Clone this repository (Optional) Note: If you do not clone the repository, then you will not be able to access the example environments and training configurations or the com.unity.ml-agents.extensions package. Additionally, the Getting Started Guide assumes that you have cloned the repository. Install the com.unity.ml-agents Unity package Install the com.unity.ml-agents.extensions Unity package (Optional) Install the mlagents Python package","title":"Installation"},{"location":"Installation/#install-unity-20203-or-later","text":"Download and install Unity. We strongly recommend that you install Unity through the Unity Hub as it will enable you to manage multiple Unity versions.","title":"Install Unity 2020.3 or Later"},{"location":"Installation/#install-python-361-or-higher","text":"We recommend installing Python 3.6 or 3.7. If you are using Windows, please install the x86-64 version and not x86. If your Python environment doesn't include pip3 , see these instructions on installing it.","title":"Install Python 3.6.1 or Higher"},{"location":"Installation/#clone-the-ml-agents-toolkit-repository-optional","text":"Now that you have installed Unity and Python, you can now install the Unity and Python packages. You do not need to clone the repository to install those packages, but you may choose to clone the repository if you'd like download our example environments and training configurations to experiment with them (some of our tutorials / guides assume you have access to our example environments). NOTE: There are samples shipped with the Unity Package. You only need to clone the repository if you would like to explore more examples. git clone --branch release_19 https://github.com/Unity-Technologies/ml-agents.git The --branch release_19 option will switch to the tag of the latest stable release. Omitting that will get the main branch which is potentially unstable.","title":"Clone the ML-Agents Toolkit Repository (Optional)"},{"location":"Installation/#advanced-local-installation-for-development","text":"You will need to clone the repository if you plan to modify or extend the ML-Agents Toolkit for your purposes. If you plan to contribute those changes back, make sure to clone the main branch (by omitting --branch release_19 from the command above). See our Contributions Guidelines for more information on contributing to the ML-Agents Toolkit.","title":"Advanced: Local Installation for Development"},{"location":"Installation/#install-the-comunityml-agents-unity-package","text":"The Unity ML-Agents C# SDK is a Unity Package. You can install the com.unity.ml-agents package directly from the Package Manager registry . Please make sure you enable 'Preview Packages' in the 'Advanced' dropdown in order to find the latest Preview release of the package. NOTE: If you do not see the ML-Agents package listed in the Package Manager please follow the advanced installation instructions below.","title":"Install the com.unity.ml-agents Unity package"},{"location":"Installation/#advanced-local-installation-for-development_1","text":"You can add the local com.unity.ml-agents package (from the repository that you just cloned) to your project by: navigating to the menu Window -> Package Manager . In the package manager window click on the + button on the top left of the packages list). Select Add package from disk... Navigate into the com.unity.ml-agents folder. Select the package.json file. If you are going to follow the examples from our documentation, you can open the Project folder in Unity and start tinkering immediately.","title":"Advanced: Local Installation for Development"},{"location":"Installation/#install-the-comunityml-agentsextensions-unity-package-optional","text":"To install the com.unity.ml-agents.extensions package, you need to first clone the repo and then complete a local installation similar to what was outlined in the previous Advanced: Local Installation for Development section. Complete installation steps can be found in the package documentation .","title":"Install the com.unity.ml-agents.extensions Unity package (Optional)"},{"location":"Installation/#install-the-mlagents-python-package","text":"Installing the mlagents Python package involves installing other Python packages that mlagents depends on. So you may run into installation issues if your machine has older versions of any of those dependencies already installed. Consequently, our supported path for installing mlagents is to leverage Python Virtual Environments. Virtual Environments provide a mechanism for isolating the dependencies for each project and are supported on Mac / Windows / Linux. We offer a dedicated guide on Virtual Environments .","title":"Install the mlagents Python package"},{"location":"Installation/#windows-installing-pytorch","text":"On Windows, you'll have to install the PyTorch package separately prior to installing ML-Agents. Activate your virtual environment and run from the command line: pip3 install torch~=1.7.1 -f https://download.pytorch.org/whl/torch_stable.html Note that on Windows, you may also need Microsoft's Visual C++ Redistributable if you don't have it already. See the PyTorch installation guide for more installation options and versions.","title":"(Windows) Installing PyTorch"},{"location":"Installation/#installing-mlagents","text":"To install the mlagents Python package, activate your virtual environment and run from the command line: python -m pip install mlagents==0.28.0 Note that this will install mlagents from PyPi, not from the cloned repository. If you installed this correctly, you should be able to run mlagents-learn --help , after which you will see the command line parameters you can use with mlagents-learn . By installing the mlagents package, the dependencies listed in the setup.py file are also installed. These include PyTorch (Requires a CPU w/ AVX support).","title":"Installing mlagents"},{"location":"Installation/#advanced-local-installation-for-development_2","text":"If you intend to make modifications to mlagents or mlagents_envs , you should install the packages from the cloned repository rather than from PyPi. To do this, you will need to install mlagents and mlagents_envs separately. From the repository's root directory, run: pip3 install torch -f https://download.pytorch.org/whl/torch_stable.html pip3 install -e ./ml-agents-envs pip3 install -e ./ml-agents Running pip with the -e flag will let you make changes to the Python files directly and have those reflected when you run mlagents-learn . It is important to install these packages in this order as the mlagents package depends on mlagents_envs , and installing it in the other order will download mlagents_envs from PyPi.","title":"Advanced: Local Installation for Development"},{"location":"Installation/#next-steps","text":"The Getting Started guide contains several short tutorials on setting up the ML-Agents Toolkit within Unity, running a pre-trained model, in addition to building and training environments.","title":"Next Steps"},{"location":"Installation/#help","text":"If you run into any problems regarding ML-Agents, refer to our FAQ and our Limitations pages. If you can't find anything please submit an issue and make sure to cite relevant information on OS, Python version, and exact error message (whenever possible).","title":"Help"},{"location":"Integrations-Match3/","text":"Match-3 with ML-Agents Getting started The C# code for Match-3 exists inside of the Unity package ( com.unity.ml-agents ). The good first step would be to take a look at how we have implemented the C# code in the example Match-3 scene (located under /Project/Assets/ML-Agents/Examples/match3). Once you have some familiarity, then the next step would be to implement the C# code for Match-3 from the extensions package. Additionally, see below for additional technical specifications on the C# code for Match-3. Please note the Match-3 game isn't human playable as implemented and can be only played via training. Technical specifications for Match-3 with ML-Agents AbstractBoard class The AbstractBoard is the bridge between ML-Agents and your game. It allows ML-Agents to * ask your game what the current and maximum sizes (rows, columns, and potential piece types) of the board are * ask your game what the \"color\" of a cell is * ask whether the cell is a \"special\" piece type or not * ask your game whether a move is allowed * request that your game make a move These are handled by implementing the abstract methods of AbstractBoard . public abstract BoardSize GetMaxBoardSize() Returns the largest BoardSize that the game can use. This is used to determine the sizes of observations and sensors, so don't make it larger than necessary. public virtual BoardSize GetCurrentBoardSize() Returns the current size of the board. Each field on this BoardSize must be less than or equal to the corresponding field returned by GetMaxBoardSize() . This method is optional; if your always use the same size board, you don't need to override it. If the current board size is smaller than the maximum board size, GetCellType() and GetSpecialType() will not be called for cells outside the current board size, and IsValidMove won't be called for moves that would go outside of the current board size. public abstract int GetCellType(int row, int col) Returns the \"color\" of piece at the given row and column. This should be between 0 and BoardSize.NumCellTypes-1 (inclusive). The actual order of the values doesn't matter. public abstract int GetSpecialType(int row, int col) Returns the special type of the piece at the given row and column. This should be between 0 and BoardSize.NumSpecialTypes (inclusive). The actual order of the values doesn't matter. public abstract bool IsMoveValid(Move m) Check whether the particular Move is valid for the game. The actual results will depend on the rules of the game, but we provide the SimpleIsMoveValid() method that handles basic match3 rules with no special or immovable pieces. public abstract bool MakeMove(Move m) Instruct the game to make the given move. Returns true if the move was made. Note that during training, a move that was marked as invalid may occasionally still be requested. If this happens, it is safe to do nothing and request another move. Move struct The Move struct encapsulates a swap of two adjacent cells. You can get the number of potential moves for a board of a given size with. Move.NumPotentialMoves(maxBoardSize) . There are two helper functions to create a new Move : * public static Move FromMoveIndex(int moveIndex, BoardSize maxBoardSize) can be used to iterate over all potential moves for the board by looping from 0 to Move.NumPotentialMoves() * public static Move FromPositionAndDirection(int row, int col, Direction dir, BoardSize maxBoardSize) creates a Move from a row, column, and direction (and board size). BoardSize struct Describes the \"size\" of the board, including the number of potential piece types that the board can have. This is returned by the AbstractBoard.GetMaxBoardSize() and GetCurrentBoardSize() methods. Match3Sensor and Match3SensorComponent classes The Match3Sensor generates observations about the state using the AbstractBoard interface. You can choose whether to use vector or \"visual\" observations; in theory, visual observations should perform better because they are 2-dimensional like the board, but we need to experiment more on this. A Match3SensorComponent generates Match3Sensor s (the exact number of sensors depends on your configuration) at runtime, and should be added to the same GameObject as your Agent implementation. You do not need to write any additional code to use them. Match3Actuator and Match3ActuatorComponent classes The Match3Actuator converts actions from training or inference into a Move that is sent to AbstractBoard.MakeMove() It also checks AbstractBoard.IsMoveValid for each potential move and uses this to set the action mask for Agent. A Match3ActuatorComponent generates a Match3Actuator at runtime, and should be added to the same GameObject as your Agent implementation. You do not need to write any additional code to use them. Setting up Match-3 simulation Implement the AbstractBoard methods to integrate with your game. Give the Agent rewards when it does what you want it to (match multiple pieces in a row, clears pieces of a certain type, etc). Add the Agent , AbstractBoard implementation, Match3SensorComponent , and Match3ActuatorComponent to the same GameObject . Call Agent.RequestDecision() when you're ready for the Agent to make a move on the next Academy step. During the next Academy step, the MakeMove() method on the board will be called. Implementation Details Action Space The indexing for actions is the same as described in Human Like Playtesting with Deep Learning (for example, Figure 2b). The horizontal moves are enumerated first, then the vertical ones. Feedback If you are a Match-3 developer and are trying to leverage ML-Agents for this scenario, we want to hear from you . Additionally, we are also looking for interested Match-3 teams to speak with us for 45 minutes. If you are interested, please indicate that in the form . If selected, we will provide gift cards as a token of appreciation.","title":"Match-3 with ML-Agents"},{"location":"Integrations-Match3/#match-3-with-ml-agents","text":"","title":"Match-3 with ML-Agents"},{"location":"Integrations-Match3/#getting-started","text":"The C# code for Match-3 exists inside of the Unity package ( com.unity.ml-agents ). The good first step would be to take a look at how we have implemented the C# code in the example Match-3 scene (located under /Project/Assets/ML-Agents/Examples/match3). Once you have some familiarity, then the next step would be to implement the C# code for Match-3 from the extensions package. Additionally, see below for additional technical specifications on the C# code for Match-3. Please note the Match-3 game isn't human playable as implemented and can be only played via training.","title":"Getting started"},{"location":"Integrations-Match3/#technical-specifications-for-match-3-with-ml-agents","text":"","title":"Technical specifications for Match-3 with ML-Agents"},{"location":"Integrations-Match3/#abstractboard-class","text":"The AbstractBoard is the bridge between ML-Agents and your game. It allows ML-Agents to * ask your game what the current and maximum sizes (rows, columns, and potential piece types) of the board are * ask your game what the \"color\" of a cell is * ask whether the cell is a \"special\" piece type or not * ask your game whether a move is allowed * request that your game make a move These are handled by implementing the abstract methods of AbstractBoard .","title":"AbstractBoard class"},{"location":"Integrations-Match3/#public-abstract-boardsize-getmaxboardsize","text":"Returns the largest BoardSize that the game can use. This is used to determine the sizes of observations and sensors, so don't make it larger than necessary.","title":"public abstract BoardSize GetMaxBoardSize()"},{"location":"Integrations-Match3/#public-virtual-boardsize-getcurrentboardsize","text":"Returns the current size of the board. Each field on this BoardSize must be less than or equal to the corresponding field returned by GetMaxBoardSize() . This method is optional; if your always use the same size board, you don't need to override it. If the current board size is smaller than the maximum board size, GetCellType() and GetSpecialType() will not be called for cells outside the current board size, and IsValidMove won't be called for moves that would go outside of the current board size.","title":"public virtual BoardSize GetCurrentBoardSize()"},{"location":"Integrations-Match3/#public-abstract-int-getcelltypeint-row-int-col","text":"Returns the \"color\" of piece at the given row and column. This should be between 0 and BoardSize.NumCellTypes-1 (inclusive). The actual order of the values doesn't matter.","title":"public abstract int GetCellType(int row, int col)"},{"location":"Integrations-Match3/#public-abstract-int-getspecialtypeint-row-int-col","text":"Returns the special type of the piece at the given row and column. This should be between 0 and BoardSize.NumSpecialTypes (inclusive). The actual order of the values doesn't matter.","title":"public abstract int GetSpecialType(int row, int col)"},{"location":"Integrations-Match3/#public-abstract-bool-ismovevalidmove-m","text":"Check whether the particular Move is valid for the game. The actual results will depend on the rules of the game, but we provide the SimpleIsMoveValid() method that handles basic match3 rules with no special or immovable pieces.","title":"public abstract bool IsMoveValid(Move m)"},{"location":"Integrations-Match3/#public-abstract-bool-makemovemove-m","text":"Instruct the game to make the given move. Returns true if the move was made. Note that during training, a move that was marked as invalid may occasionally still be requested. If this happens, it is safe to do nothing and request another move.","title":"public abstract bool MakeMove(Move m)"},{"location":"Integrations-Match3/#move-struct","text":"The Move struct encapsulates a swap of two adjacent cells. You can get the number of potential moves for a board of a given size with. Move.NumPotentialMoves(maxBoardSize) . There are two helper functions to create a new Move : * public static Move FromMoveIndex(int moveIndex, BoardSize maxBoardSize) can be used to iterate over all potential moves for the board by looping from 0 to Move.NumPotentialMoves() * public static Move FromPositionAndDirection(int row, int col, Direction dir, BoardSize maxBoardSize) creates a Move from a row, column, and direction (and board size).","title":"Move struct"},{"location":"Integrations-Match3/#boardsize-struct","text":"Describes the \"size\" of the board, including the number of potential piece types that the board can have. This is returned by the AbstractBoard.GetMaxBoardSize() and GetCurrentBoardSize() methods.","title":"BoardSize struct"},{"location":"Integrations-Match3/#match3sensor-and-match3sensorcomponent-classes","text":"The Match3Sensor generates observations about the state using the AbstractBoard interface. You can choose whether to use vector or \"visual\" observations; in theory, visual observations should perform better because they are 2-dimensional like the board, but we need to experiment more on this. A Match3SensorComponent generates Match3Sensor s (the exact number of sensors depends on your configuration) at runtime, and should be added to the same GameObject as your Agent implementation. You do not need to write any additional code to use them.","title":"Match3Sensor and Match3SensorComponent classes"},{"location":"Integrations-Match3/#match3actuator-and-match3actuatorcomponent-classes","text":"The Match3Actuator converts actions from training or inference into a Move that is sent to AbstractBoard.MakeMove() It also checks AbstractBoard.IsMoveValid for each potential move and uses this to set the action mask for Agent. A Match3ActuatorComponent generates a Match3Actuator at runtime, and should be added to the same GameObject as your Agent implementation. You do not need to write any additional code to use them.","title":"Match3Actuator and Match3ActuatorComponent classes"},{"location":"Integrations-Match3/#setting-up-match-3-simulation","text":"Implement the AbstractBoard methods to integrate with your game. Give the Agent rewards when it does what you want it to (match multiple pieces in a row, clears pieces of a certain type, etc). Add the Agent , AbstractBoard implementation, Match3SensorComponent , and Match3ActuatorComponent to the same GameObject . Call Agent.RequestDecision() when you're ready for the Agent to make a move on the next Academy step. During the next Academy step, the MakeMove() method on the board will be called.","title":"Setting up Match-3 simulation"},{"location":"Integrations-Match3/#implementation-details","text":"","title":"Implementation Details"},{"location":"Integrations-Match3/#action-space","text":"The indexing for actions is the same as described in Human Like Playtesting with Deep Learning (for example, Figure 2b). The horizontal moves are enumerated first, then the vertical ones.","title":"Action Space"},{"location":"Integrations-Match3/#feedback","text":"If you are a Match-3 developer and are trying to leverage ML-Agents for this scenario, we want to hear from you . Additionally, we are also looking for interested Match-3 teams to speak with us for 45 minutes. If you are interested, please indicate that in the form . If selected, we will provide gift cards as a token of appreciation.","title":"Feedback"},{"location":"Integrations/","text":"Game Integrations ML-Agents provides some utilities to make it easier to integrate with some common genres of games. Match-3 The Match-3 integration provides an abstraction of a match-3 game board and moves, along with a sensor to observe the game state, and an actuator to translate the ML-Agent actions into game moves. Interested in more game templates? Do you have a type of game you are interested for ML-Agents? If so, please post a forum issue with [GAME TEMPLATE] in the title.","title":"Game Integrations"},{"location":"Integrations/#game-integrations","text":"ML-Agents provides some utilities to make it easier to integrate with some common genres of games.","title":"Game Integrations"},{"location":"Integrations/#match-3","text":"The Match-3 integration provides an abstraction of a match-3 game board and moves, along with a sensor to observe the game state, and an actuator to translate the ML-Agent actions into game moves.","title":"Match-3"},{"location":"Integrations/#interested-in-more-game-templates","text":"Do you have a type of game you are interested for ML-Agents? If so, please post a forum issue with [GAME TEMPLATE] in the title.","title":"Interested in more game templates?"},{"location":"Learning-Environment-Create-New/","text":"Making a New Learning Environment This tutorial walks through the process of creating a Unity Environment from scratch. We recommend first reading the Getting Started guide to understand the concepts presented here first in an already-built environment. In this example, we will create an agent capable of controlling a ball on a platform. We will then train the agent to roll the ball toward the cube while avoiding falling off the platform. Overview Using the ML-Agents Toolkit in a Unity project involves the following basic steps: Create an environment for your agents to live in. An environment can range from a simple physical simulation containing a few objects to an entire game or ecosystem. Implement your Agent subclasses. An Agent subclass defines the code an Agent uses to observe its environment, to carry out assigned actions, and to calculate the rewards used for reinforcement training. You can also implement optional methods to reset the Agent when it has finished or failed its task. Add your Agent subclasses to appropriate GameObjects, typically, the object in the scene that represents the Agent in the simulation. Note: If you are unfamiliar with Unity, refer to Learning the interface in the Unity Manual if an Editor task isn't explained sufficiently in this tutorial. If you haven't already, follow the installation instructions . Set Up the Unity Project The first task to accomplish is simply creating a new Unity project and importing the ML-Agents assets into it: Launch Unity Hub and create a new 3D project named \"RollerBall\". Add the ML-Agents Unity package to your project. Your Unity Project window should contain the following assets: Create the Environment Next, we will create a very simple scene to act as our learning environment. The \"physical\" components of the environment include a Plane to act as the floor for the Agent to move around on, a Cube to act as the goal or target for the agent to seek, and a Sphere to represent the Agent itself. Create the Floor Plane Right click in Hierarchy window, select 3D Object > Plane. Name the GameObject \"Floor\". Select the Floor Plane to view its properties in the Inspector window. Set Transform to Position = (0, 0, 0) , Rotation = (0, 0, 0) , Scale = (1, 1, 1) . Add the Target Cube Right click in Hierarchy window, select 3D Object > Cube. Name the GameObject \"Target\". Select the Target Cube to view its properties in the Inspector window. Set Transform to Position = (3, 0.5, 3) , Rotation = (0, 0, 0) , Scale = (1, 1, 1) . Add the Agent Sphere Right click in Hierarchy window, select 3D Object > Sphere. Name the GameObject \"RollerAgent\". Select the RollerAgent Sphere to view its properties in the Inspector window. Set Transform to Position = (0, 0.5, 0) , Rotation = (0, 0, 0) , Scale = (1, 1, 1) . Click Add Component . Add the Rigidbody component to the Sphere. Group into Training Area Group the floor, target and agent under a single, empty, GameObject. This will simplify some of our subsequent steps. To do so: Right-click on your Project Hierarchy and create a new empty GameObject. Name it TrainingArea. Reset the TrainingArea\u2019s Transform so that it is at (0,0,0) with Rotation (0,0,0) and Scale (1,1,1) . Drag the Floor, Target, and RollerAgent GameObjects in the Hierarchy into the TrainingArea GameObject. Implement an Agent To create the Agent Script: Select the RollerAgent GameObject to view it in the Inspector window. Click Add Component . Click New Script in the list of components (at the bottom). Name the script \"RollerAgent\". Click Create and Add . Then, edit the new RollerAgent script: In the Unity Project window, double-click the RollerAgent script to open it in your code editor. Import ML-Agent package by adding csharp using Unity.MLAgents; using Unity.MLAgents.Sensors; using Unity.MLAgents.Actuators; then change the base class from MonoBehaviour to Agent . 1. Delete Update() since we are not using it, but keep Start() . So far, these are the basic steps that you would use to add ML-Agents to any Unity project. Next, we will add the logic that will let our Agent learn to roll to the cube using reinforcement learning. More specifically, we will need to extend three methods from the Agent base class: OnEpisodeBegin() CollectObservations(VectorSensor sensor) OnActionReceived(ActionBuffers actionBuffers) We overview each of these in more detail in the dedicated subsections below. Initialization and Resetting the Agent The process of training in the ML-Agents Toolkit involves running episodes where the Agent (Sphere) attempts to solve the task. Each episode lasts until the Agents solves the task (i.e. reaches the cube), fails (rolls off the platform) or times out (takes too long to solve or fail at the task). At the start of each episode, OnEpisodeBegin() is called to set-up the environment for a new episode. Typically the scene is initialized in a random manner to enable the agent to learn to solve the task under a variety of conditions. In this example, each time the Agent (Sphere) reaches its target (Cube), the episode ends and the target (Cube) is moved to a new random location; and if the Agent rolls off the platform, it will be put back onto the floor. These are all handled in OnEpisodeBegin() . To move the target (Cube), we need a reference to its Transform (which stores a GameObject's position, orientation and scale in the 3D world). To get this reference, add a public field of type Transform to the RollerAgent class. Public fields of a component in Unity get displayed in the Inspector window, allowing you to choose which GameObject to use as the target in the Unity Editor. To reset the Agent's velocity (and later to apply force to move the agent) we need a reference to the Rigidbody component. A Rigidbody is Unity's primary element for physics simulation. (See Physics for full documentation of Unity physics.) Since the Rigidbody component is on the same GameObject as our Agent script, the best way to get this reference is using GameObject.GetComponent<T>() , which we can call in our script's Start() method. So far, our RollerAgent script looks like: using System.Collections.Generic; using UnityEngine; using Unity.MLAgents; using Unity.MLAgents.Sensors; public class RollerAgent : Agent { Rigidbody rBody; void Start () { rBody = GetComponent<Rigidbody>(); } public Transform Target; public override void OnEpisodeBegin() { // If the Agent fell, zero its momentum if (this.transform.localPosition.y < 0) { this.rBody.angularVelocity = Vector3.zero; this.rBody.velocity = Vector3.zero; this.transform.localPosition = new Vector3( 0, 0.5f, 0); } // Move the target to a new spot Target.localPosition = new Vector3(Random.value * 8 - 4, 0.5f, Random.value * 8 - 4); } } Next, let's implement the Agent.CollectObservations(VectorSensor sensor) method. Observing the Environment The Agent sends the information we collect to the Brain, which uses it to make a decision. When you train the Agent (or use a trained model), the data is fed into a neural network as a feature vector. For an Agent to successfully learn a task, we need to provide the correct information. A good rule of thumb for deciding what information to collect is to consider what you would need to calculate an analytical solution to the problem. In our case, the information our Agent collects includes the position of the target, the position of the agent itself, and the velocity of the agent. This helps the Agent learn to control its speed so it doesn't overshoot the target and roll off the platform. In total, the agent observation contains 8 values as implemented below: public override void CollectObservations(VectorSensor sensor) { // Target and Agent positions sensor.AddObservation(Target.localPosition); sensor.AddObservation(this.transform.localPosition); // Agent velocity sensor.AddObservation(rBody.velocity.x); sensor.AddObservation(rBody.velocity.z); } Taking Actions and Assigning Rewards The final part of the Agent code is the Agent.OnActionReceived() method, which receives actions and assigns the reward. Actions To solve the task of moving towards the target, the Agent (Sphere) needs to be able to move in the x and z directions. As such, the agent needs 2 actions: the first determines the force applied along the x-axis; and the second determines the force applied along the z-axis. (If we allowed the Agent to move in three dimensions, then we would need a third action.) The RollerAgent applies the values from the action[] array to its Rigidbody component rBody , using Rigidbody.AddForce() : Vector3 controlSignal = Vector3.zero; controlSignal.x = action[0]; controlSignal.z = action[1]; rBody.AddForce(controlSignal * forceMultiplier); Rewards Reinforcement learning requires rewards to signal which decisions are good and which are bad. The learning algorithm uses the rewards to determine whether it is giving the Agent the optimal actions. You want to reward an Agent for completing the assigned task. In this case, the Agent is given a reward of 1.0 for reaching the Target cube. Rewards are assigned in OnActionReceived() . The RollerAgent calculates the distance to detect when it reaches the target. When it does, the code calls Agent.SetReward() to assign a reward of 1.0 and marks the agent as finished by calling EndEpisode() on the Agent. float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition); // Reached target if (distanceToTarget < 1.42f) { SetReward(1.0f); EndEpisode(); } Finally, if the Agent falls off the platform, end the episode so that it can reset itself: // Fell off platform if (this.transform.localPosition.y < 0) { EndEpisode(); } OnActionReceived() With the action and reward logic outlined above, the final version of OnActionReceived() looks like: public float forceMultiplier = 10; public override void OnActionReceived(ActionBuffers actionBuffers) { // Actions, size = 2 Vector3 controlSignal = Vector3.zero; controlSignal.x = actionBuffers.ContinuousActions[0]; controlSignal.z = actionBuffers.ContinuousActions[1]; rBody.AddForce(controlSignal * forceMultiplier); // Rewards float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition); // Reached target if (distanceToTarget < 1.42f) { SetReward(1.0f); EndEpisode(); } // Fell off platform else if (this.transform.localPosition.y < 0) { EndEpisode(); } } Note the forceMultiplier class variable is defined before the method definition. Since forceMultiplier is public, you can set the value from the Inspector window. Final Agent Setup in Editor Now that all the GameObjects and ML-Agent components are in place, it is time to connect everything together in the Unity Editor. This involves adding and setting some of the Agent Component's properties so that they are compatible with our Agent script. Select the RollerAgent GameObject to show its properties in the Inspector window. Drag the Target GameObject in the Hierarchy into the Target field in RollerAgent Script. Add a Decision Requester script with the Add Component button. Set the Decision Period to 10 . For more information on decisions, see the Agent documentation Add a Behavior Parameters script with the Add Component button. Set the Behavior Parameters of the Agent to the following: Behavior Name : RollerBall Vector Observation > Space Size = 8 Actions > Continuous Actions = 2 In the inspector, the RollerAgent should look like this now: Now you are ready to test the environment before training. Testing the Environment It is always a good idea to first test your environment by controlling the Agent using the keyboard. To do so, you will need to extend the Heuristic() method in the RollerAgent class. For our example, the heuristic will generate an action corresponding to the values of the \"Horizontal\" and \"Vertical\" input axis (which correspond to the keyboard arrow keys): public override void Heuristic(in ActionBuffers actionsOut) { var continuousActionsOut = actionsOut.ContinuousActions; continuousActionsOut[0] = Input.GetAxis(\"Horizontal\"); continuousActionsOut[1] = Input.GetAxis(\"Vertical\"); } In order for the Agent to use the Heuristic, You will need to set the Behavior Type to Heuristic Only in the Behavior Parameters of the RollerAgent. Press Play to run the scene and use the arrows keys to move the Agent around the platform. Make sure that there are no errors displayed in the Unity Editor Console window and that the Agent resets when it reaches its target or falls from the platform. Training the Environment The process is the same as described in the Getting Started Guide . The hyperparameters for training are specified in a configuration file that you pass to the mlagents-learn program. Create a new rollerball_config.yaml file under config/ and include the following hyperparameter values: behaviors: RollerBall: trainer_type: ppo hyperparameters: batch_size: 10 buffer_size: 100 learning_rate: 3.0e-4 beta: 5.0e-4 epsilon: 0.2 lambd: 0.99 num_epoch: 3 learning_rate_schedule: linear beta_schedule: constant epsilon_schedule: linear network_settings: normalize: false hidden_units: 128 num_layers: 2 reward_signals: extrinsic: gamma: 0.99 strength: 1.0 max_steps: 500000 time_horizon: 64 summary_freq: 10000 Hyperparameters are explained in the training configuration file documentation Since this example creates a very simple training environment with only a few inputs and outputs, using small batch and buffer sizes speeds up the training considerably. However, if you add more complexity to the environment or change the reward or observation functions, you might also find that training performs better with different hyperparameter values. In addition to setting these hyperparameter values, the Agent DecisionFrequency parameter has a large effect on training time and success. A larger value reduces the number of decisions the training algorithm has to consider and, in this simple environment, speeds up training. To train your agent, run the following command before pressing Play in the Editor: mlagents-learn config/rollerball_config.yaml --run-id=RollerBall To monitor the statistics of Agent performance during training, use TensorBoard . In particular, the cumulative_reward and value_estimate statistics show how well the Agent is achieving the task. In this example, the maximum reward an Agent can earn is 1.0, so these statistics approach that value when the Agent has successfully solved the problem. Optional: Multiple Training Areas within the Same Scene In many of the example environments , many copies of the training area are instantiated in the scene. This generally speeds up training, allowing the environment to gather many experiences in parallel. This can be achieved simply by instantiating many Agents with the same Behavior Name . Note that we've already simplified our transition to using multiple areas by creating the TrainingArea GameObject and relying on local positions in RollerAgent.cs . Use the following steps to parallelize your RollerBall environment: Drag the TrainingArea GameObject, along with its attached GameObjects, into your Assets browser, turning it into a prefab. You can now instantiate copies of the TrainingArea prefab. Drag them into your scene, positioning them so that they do not overlap. Alternatively, you can use the TrainingAreaReplicator to replicate training areas. Use the following steps: Create a new empty Game Object in the scene. Click on the new object and add a TrainingAreaReplicator component to the empty Game Object through the inspector. Drag the training area to Base Area in the Training Area Replicator. Specify the number of areas to replicate and the separation between areas. Hit play and the areas will be replicated automatically! Optional: Training Using Concurrent Unity Instances Another level of parallelization comes by training using concurrent Unity instances . For example, mlagents-learn config/rollerball_config.yaml --run-id=RollerBall --num-envs=2 will start ML Agents training with two environment instances. Combining multiple training areas within the same scene, with concurrent Unity instances, effectively gives you two levels of parallelism to speed up training. The command line option --num-envs=<n> controls the number of concurrent Unity instances that are executed in parallel during training.","title":"Making a New Learning Environment"},{"location":"Learning-Environment-Create-New/#making-a-new-learning-environment","text":"This tutorial walks through the process of creating a Unity Environment from scratch. We recommend first reading the Getting Started guide to understand the concepts presented here first in an already-built environment. In this example, we will create an agent capable of controlling a ball on a platform. We will then train the agent to roll the ball toward the cube while avoiding falling off the platform.","title":"Making a New Learning Environment"},{"location":"Learning-Environment-Create-New/#overview","text":"Using the ML-Agents Toolkit in a Unity project involves the following basic steps: Create an environment for your agents to live in. An environment can range from a simple physical simulation containing a few objects to an entire game or ecosystem. Implement your Agent subclasses. An Agent subclass defines the code an Agent uses to observe its environment, to carry out assigned actions, and to calculate the rewards used for reinforcement training. You can also implement optional methods to reset the Agent when it has finished or failed its task. Add your Agent subclasses to appropriate GameObjects, typically, the object in the scene that represents the Agent in the simulation. Note: If you are unfamiliar with Unity, refer to Learning the interface in the Unity Manual if an Editor task isn't explained sufficiently in this tutorial. If you haven't already, follow the installation instructions .","title":"Overview"},{"location":"Learning-Environment-Create-New/#set-up-the-unity-project","text":"The first task to accomplish is simply creating a new Unity project and importing the ML-Agents assets into it: Launch Unity Hub and create a new 3D project named \"RollerBall\". Add the ML-Agents Unity package to your project. Your Unity Project window should contain the following assets:","title":"Set Up the Unity Project"},{"location":"Learning-Environment-Create-New/#create-the-environment","text":"Next, we will create a very simple scene to act as our learning environment. The \"physical\" components of the environment include a Plane to act as the floor for the Agent to move around on, a Cube to act as the goal or target for the agent to seek, and a Sphere to represent the Agent itself.","title":"Create the Environment"},{"location":"Learning-Environment-Create-New/#create-the-floor-plane","text":"Right click in Hierarchy window, select 3D Object > Plane. Name the GameObject \"Floor\". Select the Floor Plane to view its properties in the Inspector window. Set Transform to Position = (0, 0, 0) , Rotation = (0, 0, 0) , Scale = (1, 1, 1) .","title":"Create the Floor Plane"},{"location":"Learning-Environment-Create-New/#add-the-target-cube","text":"Right click in Hierarchy window, select 3D Object > Cube. Name the GameObject \"Target\". Select the Target Cube to view its properties in the Inspector window. Set Transform to Position = (3, 0.5, 3) , Rotation = (0, 0, 0) , Scale = (1, 1, 1) .","title":"Add the Target Cube"},{"location":"Learning-Environment-Create-New/#add-the-agent-sphere","text":"Right click in Hierarchy window, select 3D Object > Sphere. Name the GameObject \"RollerAgent\". Select the RollerAgent Sphere to view its properties in the Inspector window. Set Transform to Position = (0, 0.5, 0) , Rotation = (0, 0, 0) , Scale = (1, 1, 1) . Click Add Component . Add the Rigidbody component to the Sphere.","title":"Add the Agent Sphere"},{"location":"Learning-Environment-Create-New/#group-into-training-area","text":"Group the floor, target and agent under a single, empty, GameObject. This will simplify some of our subsequent steps. To do so: Right-click on your Project Hierarchy and create a new empty GameObject. Name it TrainingArea. Reset the TrainingArea\u2019s Transform so that it is at (0,0,0) with Rotation (0,0,0) and Scale (1,1,1) . Drag the Floor, Target, and RollerAgent GameObjects in the Hierarchy into the TrainingArea GameObject.","title":"Group into Training Area"},{"location":"Learning-Environment-Create-New/#implement-an-agent","text":"To create the Agent Script: Select the RollerAgent GameObject to view it in the Inspector window. Click Add Component . Click New Script in the list of components (at the bottom). Name the script \"RollerAgent\". Click Create and Add . Then, edit the new RollerAgent script: In the Unity Project window, double-click the RollerAgent script to open it in your code editor. Import ML-Agent package by adding csharp using Unity.MLAgents; using Unity.MLAgents.Sensors; using Unity.MLAgents.Actuators; then change the base class from MonoBehaviour to Agent . 1. Delete Update() since we are not using it, but keep Start() . So far, these are the basic steps that you would use to add ML-Agents to any Unity project. Next, we will add the logic that will let our Agent learn to roll to the cube using reinforcement learning. More specifically, we will need to extend three methods from the Agent base class: OnEpisodeBegin() CollectObservations(VectorSensor sensor) OnActionReceived(ActionBuffers actionBuffers) We overview each of these in more detail in the dedicated subsections below.","title":"Implement an Agent"},{"location":"Learning-Environment-Create-New/#initialization-and-resetting-the-agent","text":"The process of training in the ML-Agents Toolkit involves running episodes where the Agent (Sphere) attempts to solve the task. Each episode lasts until the Agents solves the task (i.e. reaches the cube), fails (rolls off the platform) or times out (takes too long to solve or fail at the task). At the start of each episode, OnEpisodeBegin() is called to set-up the environment for a new episode. Typically the scene is initialized in a random manner to enable the agent to learn to solve the task under a variety of conditions. In this example, each time the Agent (Sphere) reaches its target (Cube), the episode ends and the target (Cube) is moved to a new random location; and if the Agent rolls off the platform, it will be put back onto the floor. These are all handled in OnEpisodeBegin() . To move the target (Cube), we need a reference to its Transform (which stores a GameObject's position, orientation and scale in the 3D world). To get this reference, add a public field of type Transform to the RollerAgent class. Public fields of a component in Unity get displayed in the Inspector window, allowing you to choose which GameObject to use as the target in the Unity Editor. To reset the Agent's velocity (and later to apply force to move the agent) we need a reference to the Rigidbody component. A Rigidbody is Unity's primary element for physics simulation. (See Physics for full documentation of Unity physics.) Since the Rigidbody component is on the same GameObject as our Agent script, the best way to get this reference is using GameObject.GetComponent<T>() , which we can call in our script's Start() method. So far, our RollerAgent script looks like: using System.Collections.Generic; using UnityEngine; using Unity.MLAgents; using Unity.MLAgents.Sensors; public class RollerAgent : Agent { Rigidbody rBody; void Start () { rBody = GetComponent<Rigidbody>(); } public Transform Target; public override void OnEpisodeBegin() { // If the Agent fell, zero its momentum if (this.transform.localPosition.y < 0) { this.rBody.angularVelocity = Vector3.zero; this.rBody.velocity = Vector3.zero; this.transform.localPosition = new Vector3( 0, 0.5f, 0); } // Move the target to a new spot Target.localPosition = new Vector3(Random.value * 8 - 4, 0.5f, Random.value * 8 - 4); } } Next, let's implement the Agent.CollectObservations(VectorSensor sensor) method.","title":"Initialization and Resetting the Agent"},{"location":"Learning-Environment-Create-New/#observing-the-environment","text":"The Agent sends the information we collect to the Brain, which uses it to make a decision. When you train the Agent (or use a trained model), the data is fed into a neural network as a feature vector. For an Agent to successfully learn a task, we need to provide the correct information. A good rule of thumb for deciding what information to collect is to consider what you would need to calculate an analytical solution to the problem. In our case, the information our Agent collects includes the position of the target, the position of the agent itself, and the velocity of the agent. This helps the Agent learn to control its speed so it doesn't overshoot the target and roll off the platform. In total, the agent observation contains 8 values as implemented below: public override void CollectObservations(VectorSensor sensor) { // Target and Agent positions sensor.AddObservation(Target.localPosition); sensor.AddObservation(this.transform.localPosition); // Agent velocity sensor.AddObservation(rBody.velocity.x); sensor.AddObservation(rBody.velocity.z); }","title":"Observing the Environment"},{"location":"Learning-Environment-Create-New/#taking-actions-and-assigning-rewards","text":"The final part of the Agent code is the Agent.OnActionReceived() method, which receives actions and assigns the reward.","title":"Taking Actions and Assigning Rewards"},{"location":"Learning-Environment-Create-New/#actions","text":"To solve the task of moving towards the target, the Agent (Sphere) needs to be able to move in the x and z directions. As such, the agent needs 2 actions: the first determines the force applied along the x-axis; and the second determines the force applied along the z-axis. (If we allowed the Agent to move in three dimensions, then we would need a third action.) The RollerAgent applies the values from the action[] array to its Rigidbody component rBody , using Rigidbody.AddForce() : Vector3 controlSignal = Vector3.zero; controlSignal.x = action[0]; controlSignal.z = action[1]; rBody.AddForce(controlSignal * forceMultiplier);","title":"Actions"},{"location":"Learning-Environment-Create-New/#rewards","text":"Reinforcement learning requires rewards to signal which decisions are good and which are bad. The learning algorithm uses the rewards to determine whether it is giving the Agent the optimal actions. You want to reward an Agent for completing the assigned task. In this case, the Agent is given a reward of 1.0 for reaching the Target cube. Rewards are assigned in OnActionReceived() . The RollerAgent calculates the distance to detect when it reaches the target. When it does, the code calls Agent.SetReward() to assign a reward of 1.0 and marks the agent as finished by calling EndEpisode() on the Agent. float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition); // Reached target if (distanceToTarget < 1.42f) { SetReward(1.0f); EndEpisode(); } Finally, if the Agent falls off the platform, end the episode so that it can reset itself: // Fell off platform if (this.transform.localPosition.y < 0) { EndEpisode(); }","title":"Rewards"},{"location":"Learning-Environment-Create-New/#onactionreceived","text":"With the action and reward logic outlined above, the final version of OnActionReceived() looks like: public float forceMultiplier = 10; public override void OnActionReceived(ActionBuffers actionBuffers) { // Actions, size = 2 Vector3 controlSignal = Vector3.zero; controlSignal.x = actionBuffers.ContinuousActions[0]; controlSignal.z = actionBuffers.ContinuousActions[1]; rBody.AddForce(controlSignal * forceMultiplier); // Rewards float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition); // Reached target if (distanceToTarget < 1.42f) { SetReward(1.0f); EndEpisode(); } // Fell off platform else if (this.transform.localPosition.y < 0) { EndEpisode(); } } Note the forceMultiplier class variable is defined before the method definition. Since forceMultiplier is public, you can set the value from the Inspector window.","title":"OnActionReceived()"},{"location":"Learning-Environment-Create-New/#final-agent-setup-in-editor","text":"Now that all the GameObjects and ML-Agent components are in place, it is time to connect everything together in the Unity Editor. This involves adding and setting some of the Agent Component's properties so that they are compatible with our Agent script. Select the RollerAgent GameObject to show its properties in the Inspector window. Drag the Target GameObject in the Hierarchy into the Target field in RollerAgent Script. Add a Decision Requester script with the Add Component button. Set the Decision Period to 10 . For more information on decisions, see the Agent documentation Add a Behavior Parameters script with the Add Component button. Set the Behavior Parameters of the Agent to the following: Behavior Name : RollerBall Vector Observation > Space Size = 8 Actions > Continuous Actions = 2 In the inspector, the RollerAgent should look like this now: Now you are ready to test the environment before training.","title":"Final Agent Setup in Editor"},{"location":"Learning-Environment-Create-New/#testing-the-environment","text":"It is always a good idea to first test your environment by controlling the Agent using the keyboard. To do so, you will need to extend the Heuristic() method in the RollerAgent class. For our example, the heuristic will generate an action corresponding to the values of the \"Horizontal\" and \"Vertical\" input axis (which correspond to the keyboard arrow keys): public override void Heuristic(in ActionBuffers actionsOut) { var continuousActionsOut = actionsOut.ContinuousActions; continuousActionsOut[0] = Input.GetAxis(\"Horizontal\"); continuousActionsOut[1] = Input.GetAxis(\"Vertical\"); } In order for the Agent to use the Heuristic, You will need to set the Behavior Type to Heuristic Only in the Behavior Parameters of the RollerAgent. Press Play to run the scene and use the arrows keys to move the Agent around the platform. Make sure that there are no errors displayed in the Unity Editor Console window and that the Agent resets when it reaches its target or falls from the platform.","title":"Testing the Environment"},{"location":"Learning-Environment-Create-New/#training-the-environment","text":"The process is the same as described in the Getting Started Guide . The hyperparameters for training are specified in a configuration file that you pass to the mlagents-learn program. Create a new rollerball_config.yaml file under config/ and include the following hyperparameter values: behaviors: RollerBall: trainer_type: ppo hyperparameters: batch_size: 10 buffer_size: 100 learning_rate: 3.0e-4 beta: 5.0e-4 epsilon: 0.2 lambd: 0.99 num_epoch: 3 learning_rate_schedule: linear beta_schedule: constant epsilon_schedule: linear network_settings: normalize: false hidden_units: 128 num_layers: 2 reward_signals: extrinsic: gamma: 0.99 strength: 1.0 max_steps: 500000 time_horizon: 64 summary_freq: 10000 Hyperparameters are explained in the training configuration file documentation Since this example creates a very simple training environment with only a few inputs and outputs, using small batch and buffer sizes speeds up the training considerably. However, if you add more complexity to the environment or change the reward or observation functions, you might also find that training performs better with different hyperparameter values. In addition to setting these hyperparameter values, the Agent DecisionFrequency parameter has a large effect on training time and success. A larger value reduces the number of decisions the training algorithm has to consider and, in this simple environment, speeds up training. To train your agent, run the following command before pressing Play in the Editor: mlagents-learn config/rollerball_config.yaml --run-id=RollerBall To monitor the statistics of Agent performance during training, use TensorBoard . In particular, the cumulative_reward and value_estimate statistics show how well the Agent is achieving the task. In this example, the maximum reward an Agent can earn is 1.0, so these statistics approach that value when the Agent has successfully solved the problem.","title":"Training the Environment"},{"location":"Learning-Environment-Create-New/#optional-multiple-training-areas-within-the-same-scene","text":"In many of the example environments , many copies of the training area are instantiated in the scene. This generally speeds up training, allowing the environment to gather many experiences in parallel. This can be achieved simply by instantiating many Agents with the same Behavior Name . Note that we've already simplified our transition to using multiple areas by creating the TrainingArea GameObject and relying on local positions in RollerAgent.cs . Use the following steps to parallelize your RollerBall environment: Drag the TrainingArea GameObject, along with its attached GameObjects, into your Assets browser, turning it into a prefab. You can now instantiate copies of the TrainingArea prefab. Drag them into your scene, positioning them so that they do not overlap. Alternatively, you can use the TrainingAreaReplicator to replicate training areas. Use the following steps: Create a new empty Game Object in the scene. Click on the new object and add a TrainingAreaReplicator component to the empty Game Object through the inspector. Drag the training area to Base Area in the Training Area Replicator. Specify the number of areas to replicate and the separation between areas. Hit play and the areas will be replicated automatically!","title":"Optional: Multiple Training Areas within the Same Scene"},{"location":"Learning-Environment-Create-New/#optional-training-using-concurrent-unity-instances","text":"Another level of parallelization comes by training using concurrent Unity instances . For example, mlagents-learn config/rollerball_config.yaml --run-id=RollerBall --num-envs=2 will start ML Agents training with two environment instances. Combining multiple training areas within the same scene, with concurrent Unity instances, effectively gives you two levels of parallelism to speed up training. The command line option --num-envs=<n> controls the number of concurrent Unity instances that are executed in parallel during training.","title":"Optional: Training Using Concurrent Unity Instances"},{"location":"Learning-Environment-Design-Agents/","text":"Agents Table of Contents: Decisions Observations and Sensors Generating Observations Agent.CollectObservations() Observable Fields and Properties ISensor interface and SensorComponents Vector Observations One-hot encoding categorical information Normalization Stacking Vector Observation Summary & Best Practices Visual Observations Visual Observation Summary & Best Practices Raycast Observations RayCast Observation Summary & Best Practices Variable Length Observations Variable Length Observation Summary & Best Practices Goal Signal Goal Signal Summary & Best Practices Actions and Actuators Continuous Actions Discrete Actions Masking Discrete Actions Actions Summary & Best Practices Rewards Examples Rewards Summary & Best Practices Agent Properties Destroying an Agent Defining Multi-agent Scenarios Teams for Adversarial Scenarios Groups for Cooperative Scenarios Recording Demonstrations An agent is an entity that can observe its environment, decide on the best course of action using those observations, and execute those actions within its environment. Agents can be created in Unity by extending the Agent class. The most important aspects of creating agents that can successfully learn are the observations the agent collects, and the reward you assign to estimate the value of the agent's current state toward accomplishing its tasks. An Agent passes its observations to its Policy. The Policy then makes a decision and passes the chosen action back to the agent. Your agent code must execute the action, for example, move the agent in one direction or another. In order to train an agent using reinforcement learning , your agent must calculate a reward value at each action. The reward is used to discover the optimal decision-making policy. The Policy class abstracts out the decision making logic from the Agent itself so that you can use the same Policy in multiple Agents. How a Policy makes its decisions depends on the Behavior Parameters associated with the agent. If you set Behavior Type to Heuristic Only , the Agent will use its Heuristic() method to make decisions which can allow you to control the Agent manually or write your own Policy. If the Agent has a Model file, its Policy will use the neural network Model to take decisions. When you create an Agent, you should usually extend the base Agent class. This includes implementing the following methods: Agent.OnEpisodeBegin() \u2014 Called at the beginning of an Agent's episode, including at the beginning of the simulation. Agent.CollectObservations(VectorSensor sensor) \u2014 Called every step that the Agent requests a decision. This is one possible way for collecting the Agent's observations of the environment; see Generating Observations below for more options. Agent.OnActionReceived() \u2014 Called every time the Agent receives an action to take. Receives the action chosen by the Agent. It is also common to assign a reward in this method. Agent.Heuristic() - When the Behavior Type is set to Heuristic Only in the Behavior Parameters of the Agent, the Agent will use the Heuristic() method to generate the actions of the Agent. As such, the Heuristic() method writes to the array of floats provided to the Heuristic method as argument. Note : Do not create a new float array of action in the Heuristic() method, as this will prevent writing floats to the original action array. As a concrete example, here is how the Ball3DAgent class implements these methods: Agent.OnEpisodeBegin() \u2014 Resets the agent cube and ball to their starting positions. The function randomizes the reset values so that the training generalizes to more than a specific starting position and agent cube orientation. Agent.CollectObservations(VectorSensor sensor) \u2014 Adds information about the orientation of the agent cube, the ball velocity, and the relative position between the ball and the cube. Since the CollectObservations() method calls VectorSensor.AddObservation() such that vector size adds up to 8, the Behavior Parameters of the Agent are set with vector observation space with a state size of 8. Agent.OnActionReceived() \u2014 The action results in a small change in the agent cube's rotation at each step. In this example, an Agent receives a small positive reward for each step it keeps the ball on the agent cube's head and a larger, negative reward for dropping the ball. An Agent's episode is also ended when it drops the ball so that it will reset with a new ball for the next simulation step. Agent.Heuristic() - Converts the keyboard inputs into actions. Decisions The observation-decision-action-reward cycle repeats each time the Agent request a decision. Agents will request a decision when Agent.RequestDecision() is called. If you need the Agent to request decisions on its own at regular intervals, add a Decision Requester component to the Agent's GameObject. Making decisions at regular step intervals is generally most appropriate for physics-based simulations. For example, an agent in a robotic simulator that must provide fine-control of joint torques should make its decisions every step of the simulation. On the other hand, an agent that only needs to make decisions when certain game or simulation events occur, such as in a turn-based game, should call Agent.RequestDecision() manually. Observations and Sensors In order for an agent to learn, the observations should include all the information an agent needs to accomplish its task. Without sufficient and relevant information, an agent may learn poorly or may not learn at all. A reasonable approach for determining what information should be included is to consider what you would need to calculate an analytical solution to the problem, or what you would expect a human to be able to use to solve the problem. Generating Observations ML-Agents provides multiple ways for an Agent to make observations: 1. Overriding the Agent.CollectObservations() method and passing the observations to the provided VectorSensor . 1. Adding the [Observable] attribute to fields and properties on the Agent. 1. Implementing the ISensor interface, using a SensorComponent attached to the Agent to create the ISensor . Agent.CollectObservations() Agent.CollectObservations() is best used for aspects of the environment which are numerical and non-visual. The Policy class calls the CollectObservations(VectorSensor sensor) method of each Agent. Your implementation of this function must call VectorSensor.AddObservation to add vector observations. The VectorSensor.AddObservation method provides a number of overloads for adding common types of data to your observation vector. You can add Integers and booleans directly to the observation vector, as well as some common Unity data types such as Vector2 , Vector3 , and Quaternion . For examples of various state observation functions, you can look at the example environments included in the ML-Agents SDK. For instance, the 3DBall example uses the rotation of the platform, the relative position of the ball, and the velocity of the ball as its state observation. public GameObject ball; public override void CollectObservations(VectorSensor sensor) { // Orientation of the cube (2 floats) sensor.AddObservation(gameObject.transform.rotation.z); sensor.AddObservation(gameObject.transform.rotation.x); // Relative position of the ball to the cube (3 floats) sensor.AddObservation(ball.transform.position - gameObject.transform.position); // Velocity of the ball (3 floats) sensor.AddObservation(m_BallRb.velocity); // 8 floats total } As an experiment, you can remove the velocity components from the observation and retrain the 3DBall agent. While it will learn to balance the ball reasonably well, the performance of the agent without using velocity is noticeably worse. The observations passed to VectorSensor.AddObservation() must always contain the same number of elements must always be in the same order. If the number of observed entities in an environment can vary, you can pad the calls with zeros for any missing entities in a specific observation, or you can limit an agent's observations to a fixed subset. For example, instead of observing every enemy in an environment, you could only observe the closest five. Additionally, when you set up an Agent's Behavior Parameters in the Unity Editor, you must set the Vector Observations > Space Size to equal the number of floats that are written by CollectObservations() . Observable Fields and Properties Another approach is to define the relevant observations as fields or properties on your Agent class, and annotate them with an ObservableAttribute . For example, in the Ball3DHardAgent, the difference between positions could be observed by adding a property to the Agent: using Unity.MLAgents.Sensors.Reflection; public class Ball3DHardAgent : Agent { [Observable(numStackedObservations: 9)] Vector3 PositionDelta { get { return ball.transform.position - gameObject.transform.position; } } } ObservableAttribute currently supports most basic types (e.g. floats, ints, bools), as well as Vector2 , Vector3 , Vector4 , Quaternion , and enums. The behavior of ObservableAttribute s are controlled by the \"Observable Attribute Handling\" in the Agent's Behavior Parameters . The possible values for this are: * Ignore (default) - All ObservableAttributes on the Agent will be ignored. If there are no ObservableAttributes on the Agent, this will result in the fastest initialization time. * Exclude Inherited - Only members on the declared class will be examined; members that are inherited are ignored. This is a reasonable tradeoff between performance and flexibility. * Examine All All members on the class will be examined. This can lead to slower startup times. \"Exclude Inherited\" is generally sufficient, but if your Agent inherits from another Agent implementation that has Observable members, you will need to use \"Examine All\". Internally, ObservableAttribute uses reflection to determine which members of the Agent have ObservableAttributes, and also uses reflection to access the fields or invoke the properties at runtime. This may be slower than using CollectObservations or an ISensor, although this might not be enough to noticeably affect performance. NOTE : you do not need to adjust the Space Size in the Agent's Behavior Parameters when you add [Observable] fields or properties to an Agent, since their size can be computed before they are used. ISensor interface and SensorComponents The ISensor interface is generally intended for advanced users. The Write() method is used to actually generate the observation, but some other methods such as returning the shape of the observations must also be implemented. The SensorComponent abstract class is used to create the actual ISensor at runtime. It must be attached to the same GameObject as the Agent , or to a child GameObject . There are several SensorComponents provided in the API, including: - CameraSensorComponent - Uses images from a Camera as observations. - RenderTextureSensorComponent - Uses the content of a RenderTexture as observations. - RayPerceptionSensorComponent - Uses the information from set of ray casts as observations. - Match3SensorComponent - Uses the board of a Match-3 game as observations. - GridSensorComponent - Uses a set of box queries in a grid shape as observations. NOTE : you do not need to adjust the Space Size in the Agent's Behavior Parameters when using SensorComponents s. Internally, both Agent.CollectObservations and [Observable] attribute use an ISensors to write observations, although this is mostly abstracted from the user. Vector Observations Both Agent.CollectObservations() and ObservableAttribute s produce vector observations, which are represented at lists of float s. ISensor s can produce both vector observations and visual observations, which are multi-dimensional arrays of floats. Below are some additional considerations when dealing with vector observations: One-hot encoding categorical information Type enumerations should be encoded in the one-hot style. That is, add an element to the feature vector for each element of enumeration, setting the element representing the observed member to one and set the rest to zero. For example, if your enumeration contains [Sword, Shield, Bow] and the agent observes that the current item is a Bow, you would add the elements: 0, 0, 1 to the feature vector. The following code example illustrates how to add. enum ItemType { Sword, Shield, Bow, LastItem } public override void CollectObservations(VectorSensor sensor) { for (int ci = 0; ci < (int)ItemType.LastItem; ci++) { sensor.AddObservation((int)currentItem == ci ? 1.0f : 0.0f); } } VectorSensor also provides a two-argument function AddOneHotObservation() as a shortcut for one-hot style observations. The following example is identical to the previous one. enum ItemType { Sword, Shield, Bow, LastItem } const int NUM_ITEM_TYPES = (int)ItemType.LastItem; public override void CollectObservations(VectorSensor sensor) { // The first argument is the selection index; the second is the // number of possibilities sensor.AddOneHotObservation((int)currentItem, NUM_ITEM_TYPES); } ObservableAttribute has built-in support for enums. Note that you don't need the LastItem placeholder in this case: enum ItemType { Sword, Shield, Bow } public class HeroAgent : Agent { [Observable] ItemType m_CurrentItem; } Normalization For the best results when training, you should normalize the components of your feature vector to the range [-1, +1] or [0, 1]. When you normalize the values, the PPO neural network can often converge to a solution faster. Note that it isn't always necessary to normalize to these recommended ranges, but it is considered a best practice when using neural networks. The greater the variation in ranges between the components of your observation, the more likely that training will be affected. To normalize a value to [0, 1], you can use the following formula: normalizedValue = (currentValue - minValue)/(maxValue - minValue) :warning: For vectors, you should apply the above formula to each component (x, y, and z). Note that this is not the same as using the Vector3.normalized property or Vector3.Normalize() method in Unity (and similar for Vector2 ). Rotations and angles should also be normalized. For angles between 0 and 360 degrees, you can use the following formulas: Quaternion rotation = transform.rotation; Vector3 normalized = rotation.eulerAngles / 180.0f - Vector3.one; // [-1,1] Vector3 normalized = rotation.eulerAngles / 360.0f; // [0,1] For angles that can be outside the range [0,360], you can either reduce the angle, or, if the number of turns is significant, increase the maximum value used in your normalization formula. Stacking Stacking refers to repeating observations from previous steps as part of a larger observation. For example, consider an Agent that generates these observations in four steps step 1: [0.1] step 2: [0.2] step 3: [0.3] step 4: [0.4] If we use a stack size of 3, the observations would instead be: step 1: [0.1, 0.0, 0.0] step 2: [0.2, 0.1, 0.0] step 3: [0.3, 0.2, 0.1] step 4: [0.4, 0.3, 0.2] (The observations are padded with zeroes for the first stackSize-1 steps). This is a simple way to give an Agent limited \"memory\" without the complexity of adding a recurrent neural network (RNN). The steps for enabling stacking depends on how you generate observations: * For Agent.CollectObservations(), set \"Stacked Vectors\" on the Agent's Behavior Parameters to a value greater than 1. * For ObservableAttribute, set the numStackedObservations parameter in the constructor, e.g. [Observable(numStackedObservations: 2)] . * For ISensor s, wrap them in a StackingSensor (which is also an ISensor ). Generally, this should happen in the CreateSensor() method of your SensorComponent . Vector Observation Summary & Best Practices Vector Observations should include all variables relevant for allowing the agent to take the optimally informed decision, and ideally no extraneous information. In cases where Vector Observations need to be remembered or compared over time, either an RNN should be used in the model, or the Stacked Vectors value in the agent GameObject's Behavior Parameters should be changed. Categorical variables such as type of object (Sword, Shield, Bow) should be encoded in one-hot fashion (i.e. 3 -> 0, 0, 1 ). This can be done automatically using the AddOneHotObservation() method of the VectorSensor , or using [Observable] on an enum field or property of the Agent. In general, all inputs should be normalized to be in the range 0 to +1 (or -1 to 1). For example, the x position information of an agent where the maximum possible value is maxValue should be recorded as VectorSensor.AddObservation(transform.position.x / maxValue); rather than VectorSensor.AddObservation(transform.position.x); . Positional information of relevant GameObjects should be encoded in relative coordinates wherever possible. This is often relative to the agent position. Visual Observations Visual observations are generally provided to agent via either a CameraSensor or RenderTextureSensor . These collect image information and transforms it into a 3D Tensor which can be fed into the convolutional neural network (CNN) of the agent policy. For more information on CNNs, see this guide . This allows agents to learn from spatial regularities in the observation images. It is possible to use visual and vector observations with the same agent. Agents using visual observations can capture state of arbitrary complexity and are useful when the state is difficult to describe numerically. However, they are also typically less efficient and slower to train, and sometimes don't succeed at all as compared to vector observations. As such, they should only be used when it is not possible to properly define the problem using vector or ray-cast observations. Visual observations can be derived from Cameras or RenderTextures within your scene. To add a visual observation to an Agent, add either a Camera Sensor Component or RenderTextures Sensor Component to the Agent. Then drag the camera or render texture you want to add to the Camera or RenderTexture field. You can have more than one camera or render texture and even use a combination of both attached to an Agent. For each visual observation, set the width and height of the image (in pixels) and whether or not the observation is color or grayscale. or Each Agent that uses the same Policy must have the same number of visual observations, and they must all have the same resolutions (including whether or not they are grayscale). Additionally, each Sensor Component on an Agent must have a unique name so that they can be sorted deterministically (the name must be unique for that Agent, but multiple Agents can have a Sensor Component with the same name). Visual observations also support stacking, by specifying Observation Stacks to a value greater than 1. The visual observations from the last stackSize steps will be stacked on the last dimension (channel dimension). When using RenderTexture visual observations, a handy feature for debugging is adding a Canvas , then adding a Raw Image with it's texture set to the Agent's RenderTexture . This will render the agent observation on the game screen. The GridWorld environment is an example on how to use a RenderTexture for both debugging and observation. Note that in this example, a Camera is rendered to a RenderTexture, which is then used for observations and debugging. To update the RenderTexture, the Camera must be asked to render every time a decision is requested within the game code. When using Cameras as observations directly, this is done automatically by the Agent. Visual Observation Summary & Best Practices To collect visual observations, attach CameraSensor or RenderTextureSensor components to the agent GameObject. Visual observations should generally be used unless vector observations are not sufficient. Image size should be kept as small as possible, without the loss of needed details for decision making. Images should be made grayscale in situations where color information is not needed for making informed decisions. Raycast Observations Raycasts are another possible method for providing observations to an agent. This can be easily implemented by adding a RayPerceptionSensorComponent3D (or RayPerceptionSensorComponent2D ) to the Agent GameObject. During observations, several rays (or spheres, depending on settings) are cast into the physics world, and the objects that are hit determine the observation vector that is produced. Both sensor components have several settings: Detectable Tags A list of strings corresponding to the types of objects that the Agent should be able to distinguish between. For example, in the WallJump example, we use \"wall\", \"goal\", and \"block\" as the list of objects to detect. Rays Per Direction Determines the number of rays that are cast. One ray is always cast forward, and this many rays are cast to the left and right. Max Ray Degrees The angle (in degrees) for the outermost rays. 90 degrees corresponds to the left and right of the agent. Sphere Cast Radius The size of the sphere used for sphere casting. If set to 0, rays will be used instead of spheres. Rays may be more efficient, especially in complex scenes. Ray Length The length of the casts Ray Layer Mask The LayerMask passed to the raycast or spherecast. This can be used to ignore certain types of objects when casting. Observation Stacks The number of previous results to \"stack\" with the cast results. Note that this can be independent of the \"Stacked Vectors\" setting in Behavior Parameters . Start Vertical Offset (3D only) The vertical offset of the ray start point. End Vertical Offset (3D only) The vertical offset of the ray end point. In the example image above, the Agent has two RayPerceptionSensorComponent3D s. Both use 3 Rays Per Direction and 90 Max Ray Degrees. One of the components had a vertical offset, so the Agent can tell whether it's clear to jump over the wall. The total size of the created observations is (Observation Stacks) * (1 + 2 * Rays Per Direction) * (Num Detectable Tags + 2) so the number of rays and tags should be kept as small as possible to reduce the amount of data used. Note that this is separate from the State Size defined in Behavior Parameters , so you don't need to worry about the formula above when setting the State Size. RayCast Observation Summary & Best Practices Attach RayPerceptionSensorComponent3D or RayPerceptionSensorComponent2D to use. This observation type is best used when there is relevant spatial information for the agent that doesn't require a fully rendered image to convey. Use as few rays and tags as necessary to solve the problem in order to improve learning stability and agent performance. Grid Observations Grid-base observations combine the advantages of 2D spatial representation in visual observations, and the flexibility of defining detectable objects in RayCast observations. The sensor uses a set of box queries in a grid shape and gives a top-down 2D view around the agent. This can be implemented by adding a GridSensorComponent to the Agent GameObject. During observations, the sensor detects the presence of detectable objects in each cell and encode that into one-hot representation. The collected information from each cell forms a 3D tensor observation and will be fed into the convolutional neural network (CNN) of the agent policy just like visual observations. The sensor component has the following settings: - Cell Scale The scale of each cell in the grid. - Grid Size Number of cells on each side of the grid. - Agent Game Object The Agent that holds the grid sensor. This is used to disambiguate objects with the same tag as the agent so that the agent doesn't detect itself. - Rotate With Agent Whether the grid rotates with the Agent. - Detectable Tags A list of strings corresponding to the types of objects that the Agent should be able to distinguish between. - Collider Mask The LayerMask passed to the collider detection. This can be used to ignore certain types of objects. - Initial Collider Buffer Size The initial size of the Collider buffer used in the non-allocating Physics calls for each cell. - Max Collider Buffer Size The max size of the Collider buffer used in the non-allocating Physics calls for each cell. The observation for each grid cell is a one-hot encoding of the detected object. The total size of the created observations is GridSize.x * GridSize.z * Num Detectable Tags so the number of detectable tags and size of the grid should be kept as small as possible to reduce the amount of data used. This makes a trade-off between the granularity of the observation and training speed. To allow more variety of observations that grid sensor can capture, the GridSensorComponent and the underlying GridSensorBase also provides interfaces that can be overridden to collect customized observation from detected objects. See the doc on extending grid Sensors for more details on custom grid sensors. Note : The GridSensor only works in 3D environments and will not behave properly in 2D environments. Grid Observation Summary & Best Practices Attach GridSensorComponent to use. This observation type is best used when there is relevant non-visual spatial information that can be best captured in 2D representations. Use as small grid size and as few tags as necessary to solve the problem in order to improve learning stability and agent performance. Do not use GridSensor in a 2D game. Variable Length Observations It is possible for agents to collect observations from a varying number of GameObjects by using a BufferSensor . You can add a BufferSensor to your Agent by adding a BufferSensorComponent to its GameObject. The BufferSensor can be useful in situations in which the Agent must pay attention to a varying number of entities (for example, a varying number of enemies or projectiles). On the trainer side, the BufferSensor is processed using an attention module. More information about attention mechanisms can be found here . Training or doing inference with variable length observations can be slower than using a flat vector observation. However, attention mechanisms enable solving problems that require comparative reasoning between entities in a scene such as our Sorter environment . Note that even though the BufferSensor can process a variable number of entities, you still need to define a maximum number of entities. This is because our network architecture requires to know what the shape of the observations will be. If fewer entities are observed than the maximum, the observation will be padded with zeros and the trainer will ignore the padded observations. Note that attention layers are invariant to the order of the entities, so there is no need to properly \"order\" the entities before feeding them into the BufferSensor . The BufferSensorComponent Editor inspector has two arguments: - Observation Size : This is how many floats each entities will be represented with. This number is fixed and all entities must have the same representation. For example, if the entities you want to put into the BufferSensor have for relevant information position and speed, then the Observation Size should be 6 floats. - Maximum Number of Entities : This is the maximum number of entities the BufferSensor will be able to collect. To add an entity's observations to a BufferSensorComponent , you need to call BufferSensorComponent.AppendObservation() in the Agent.CollectObservations() method with a float array of size Observation Size as argument. Note : Currently, the observations put into the BufferSensor are not normalized, you will need to normalize your observations manually between -1 and 1. Variable Length Observation Summary & Best Practices Attach BufferSensorComponent to use. Call BufferSensorComponent.AppendObservation() in the Agent.CollectObservations() methodto add the observations of an entity to the BufferSensor . Normalize the entities observations before feeding them into the BufferSensor . Goal Signal It is possible for agents to collect observations that will be treated as \"goal signal\". A goal signal is used to condition the policy of the agent, meaning that if the goal changes, the policy (i.e. the mapping from observations to actions) will change as well. Note that this is true for any observation since all observations influence the policy of the Agent to some degree. But by specifying a goal signal explicitly, we can make this conditioning more important to the agent. This feature can be used in settings where an agent must learn to solve different tasks that are similar by some aspects because the agent will learn to reuse learnings from different tasks to generalize better. In Unity, you can specify that a VectorSensor or a CameraSensor is a goal by attaching a VectorSensorComponent or a CameraSensorComponent to the Agent and selecting Goal Signal as Observation Type . On the trainer side, there are two different ways to condition the policy. This setting is determined by the conditioning_type parameter . If set to hyper (default) a HyperNetwork will be used to generate some of the weights of the policy using the goal observations as input. Note that using a HyperNetwork requires a lot of computations, it is recommended to use a smaller number of hidden units in the policy to alleviate this. If set to none the goal signal will be considered as regular observations. For an example on how to use a goal signal, see the GridWorld example . Goal Signal Summary & Best Practices Attach a VectorSensorComponent or CameraSensorComponent to an agent and set the observation type to goal to use the feature. Set the conditioning_type parameter in the training configuration. Reduce the number of hidden units in the network when using the HyperNetwork conditioning type. Actions and Actuators An action is an instruction from the Policy that the agent carries out. The action is passed to the an IActionReceiver (either an Agent or an IActuator ) as the ActionBuffers parameter when the Academy invokes the IActionReciever.OnActionReceived() function. There are two types of actions supported: Continuous and Discrete . Neither the Policy nor the training algorithm know anything about what the action values themselves mean. The training algorithm simply tries different values for the action list and observes the affect on the accumulated rewards over time and many training episodes. Thus, the only place actions are defined for an Agent is in the OnActionReceived() function. For example, if you designed an agent to move in two dimensions, you could use either continuous or the discrete actions. In the continuous case, you would set the action size to two (one for each dimension), and the agent's Policy would output an action with two floating point values. In the discrete case, you would use one Branch with a size of four (one for each direction), and the Policy would create an action array containing a single element with a value ranging from zero to three. Alternatively, you could create two branches of size two (one for horizontal movement and one for vertical movement), and the Policy would output an action array containing two elements with values ranging from zero to one. You could alternatively use a combination of continuous and discrete actions e.g., using one continuous action for horizontal movement and a discrete branch of size two for the vertical movement. Note that when you are programming actions for an agent, it is often helpful to test your action logic using the Heuristic() method of the Agent, which lets you map keyboard commands to actions. Continuous Actions When an Agent's Policy has Continuous actions, the ActionBuffers.ContinuousActions passed to the Agent's OnActionReceived() function is an array with length equal to the Continuous Action Size property value. The individual values in the array have whatever meanings that you ascribe to them. If you assign an element in the array as the speed of an Agent, for example, the training process learns to control the speed of the Agent through this parameter. The 3DBall example uses continuous actions with two control values. These control values are applied as rotation to the cube: public override void OnActionReceived(ActionBuffers actionBuffers) { var actionZ = 2f * Mathf.Clamp(actionBuffers.ContinuousActions[0], -1f, 1f); var actionX = 2f * Mathf.Clamp(actionBuffers.ContinuousActions[1], -1f, 1f); gameObject.transform.Rotate(new Vector3(0, 0, 1), actionZ); gameObject.transform.Rotate(new Vector3(1, 0, 0), actionX); } By default the output from our provided PPO algorithm pre-clamps the values of ActionBuffers.ContinuousActions into the [-1, 1] range. It is a best practice to manually clip these as well, if you plan to use a 3rd party algorithm with your environment. As shown above, you can scale the control values as needed after clamping them. Discrete Actions When an Agent's Policy uses discrete actions, the ActionBuffers.DiscreteActions passed to the Agent's OnActionReceived() function is an array of integers with length equal to Discrete Branch Size . When defining the discrete actions, Branches is an array of integers, each value corresponds to the number of possibilities for each branch. For example, if we wanted an Agent that can move in a plane and jump, we could define two branches (one for motion and one for jumping) because we want our agent be able to move and jump concurrently. We define the first branch to have 5 possible actions (don't move, go left, go right, go backward, go forward) and the second one to have 2 possible actions (don't jump, jump). The OnActionReceived() method would look something like: // Get the action index for movement int movement = actionBuffers.DiscreteActions[0]; // Get the action index for jumping int jump = actionBuffers.DiscreteActions[1]; // Look up the index in the movement action list: if (movement == 1) { directionX = -1; } if (movement == 2) { directionX = 1; } if (movement == 3) { directionZ = -1; } if (movement == 4) { directionZ = 1; } // Look up the index in the jump action list: if (jump == 1 && IsGrounded()) { directionY = 1; } // Apply the action results to move the Agent gameObject.GetComponent<Rigidbody>().AddForce( new Vector3( directionX * 40f, directionY * 300f, directionZ * 40f)); Masking Discrete Actions When using Discrete Actions, it is possible to specify that some actions are impossible for the next decision. When the Agent is controlled by a neural network, the Agent will be unable to perform the specified action. Note that when the Agent is controlled by its Heuristic, the Agent will still be able to decide to perform the masked action. In order to disallow an action, override the Agent.WriteDiscreteActionMask() virtual method, and call SetActionEnabled() on the provided IDiscreteActionMask : public override void WriteDiscreteActionMask(IDiscreteActionMask actionMask) { actionMask.SetActionEnabled(branch, actionIndex, isEnabled); } Where: branch is the index (starting at 0) of the branch on which you want to allow or disallow the action actionIndex is the index of the action that you want to allow or disallow. isEnabled is a bool indicating whether the action should be allowed or now. For example, if you have an Agent with 2 branches and on the first branch (branch 0) there are 4 possible actions : \"do nothing\" , \"jump\" , \"shoot\" and \"change weapon\" . Then with the code bellow, the Agent will either \"do nothing\" or \"change weapon\" for their next decision (since action index 1 and 2 are masked) actionMask.SetActionEnabled(0, 1, false); actionMask.SetActionEnabled(0, 2, false); Notes: You can call SetActionEnabled multiple times if you want to put masks on multiple branches. At each step, the state of an action is reset and enabled by default. You cannot mask all the actions of a branch. You cannot mask actions in continuous control. IActuator interface and ActuatorComponents The Actuator API allows users to abstract behavior out of Agents and in to components (similar to the ISensor API). The IActuator interface and Agent class both implement the IActionReceiver interface to allow for backward compatibility with the current Agent.OnActionReceived . This means you will not have to change your code until you decide to use the IActuator API. Like the ISensor interface, the IActuator interface is intended for advanced users. The ActuatorComponent abstract class is used to create the actual IActuator at runtime. It must be attached to the same GameObject as the Agent , or to a child GameObject . Actuators and all of their data structures are initialized during Agent.Initialize . This was done to prevent an unexpected allocations at runtime. You can find an example of an IActuator implementation in the Basic example scene. NOTE : you do not need to adjust the Actions in the Agent's Behavior Parameters when using an IActuator and ActuatorComponents . Internally, Agent.OnActionReceived uses an IActuator to send actions to the Agent, although this is mostly abstracted from the user. Actions Summary & Best Practices Agents can use Discrete and/or Continuous actions. Discrete actions can have multiple action branches, and it's possible to mask certain actions so that they won't be taken. In general, fewer actions will make for easier learning. Be sure to set the Continuous Action Size and Discrete Branch Size to the desired number for each type of action, and not greater, as doing the latter can interfere with the efficiency of the training process. Continuous action values should be clipped to an appropriate range. The provided PPO model automatically clips these values between -1 and 1, but third party training systems may not do so. Rewards In reinforcement learning, the reward is a signal that the agent has done something right. The PPO reinforcement learning algorithm works by optimizing the choices an agent makes such that the agent earns the highest cumulative reward over time. The better your reward mechanism, the better your agent will learn. Note: Rewards are not used during inference by an Agent using a trained model and is also not used during imitation learning. Perhaps the best advice is to start simple and only add complexity as needed. In general, you should reward results rather than actions you think will lead to the desired results. You can even use the Agent's Heuristic to control the Agent while watching how it accumulates rewards. Allocate rewards to an Agent by calling the AddReward() or SetReward() methods on the agent. The reward assigned between each decision should be in the range [-1,1]. Values outside this range can lead to unstable training. The reward value is reset to zero when the agent receives a new decision. If there are multiple calls to AddReward() for a single agent decision, the rewards will be summed together to evaluate how good the previous decision was. The SetReward() will override all previous rewards given to an agent since the previous decision. Examples You can examine the OnActionReceived() functions defined in the example environments to see how those projects allocate rewards. The GridAgent class in the GridWorld example uses a very simple reward system: Collider[] hitObjects = Physics.OverlapBox(trueAgent.transform.position, new Vector3(0.3f, 0.3f, 0.3f)); if (hitObjects.Where(col => col.gameObject.tag == \"goal\").ToArray().Length == 1) { AddReward(1.0f); EndEpisode(); } else if (hitObjects.Where(col => col.gameObject.tag == \"pit\").ToArray().Length == 1) { AddReward(-1f); EndEpisode(); } The agent receives a positive reward when it reaches the goal and a negative reward when it falls into the pit. Otherwise, it gets no rewards. This is an example of a sparse reward system. The agent must explore a lot to find the infrequent reward. In contrast, the AreaAgent in the Area example gets a small negative reward every step. In order to get the maximum reward, the agent must finish its task of reaching the goal square as quickly as possible: AddReward( -0.005f); MoveAgent(act); if (gameObject.transform.position.y < 0.0f || Mathf.Abs(gameObject.transform.position.x - area.transform.position.x) > 8f || Mathf.Abs(gameObject.transform.position.z + 5 - area.transform.position.z) > 8) { AddReward(-1f); EndEpisode(); } The agent also gets a larger negative penalty if it falls off the playing surface. The Ball3DAgent in the 3DBall takes a similar approach, but allocates a small positive reward as long as the agent balances the ball. The agent can maximize its rewards by keeping the ball on the platform: SetReward(0.1f); // When ball falls mark Agent as finished and give a negative penalty if ((ball.transform.position.y - gameObject.transform.position.y) < -2f || Mathf.Abs(ball.transform.position.x - gameObject.transform.position.x) > 3f || Mathf.Abs(ball.transform.position.z - gameObject.transform.position.z) > 3f) { SetReward(-1f); EndEpisode(); } The Ball3DAgent also assigns a negative penalty when the ball falls off the platform. Note that all of these environments make use of the EndEpisode() method, which manually terminates an episode when a termination condition is reached. This can be called independently of the Max Step property. Rewards Summary & Best Practices Use AddReward() to accumulate rewards between decisions. Use SetReward() to overwrite any previous rewards accumulate between decisions. The magnitude of any given reward should typically not be greater than 1.0 in order to ensure a more stable learning process. Positive rewards are often more helpful to shaping the desired behavior of an agent than negative rewards. Excessive negative rewards can result in the agent failing to learn any meaningful behavior. For locomotion tasks, a small positive reward (+0.1) for forward velocity is typically used. If you want the agent to finish a task quickly, it is often helpful to provide a small penalty every step (-0.05) that the agent does not complete the task. In this case completion of the task should also coincide with the end of the episode by calling EndEpisode() on the agent when it has accomplished its goal. Agent Properties Behavior Parameters - The parameters dictating what Policy the Agent will receive. Behavior Name - The identifier for the behavior. Agents with the same behavior name will learn the same policy. Vector Observation Space Size - Length of vector observation for the Agent. Stacked Vectors - The number of previous vector observations that will be stacked and used collectively for decision making. This results in the effective size of the vector observation being passed to the Policy being: Space Size x Stacked Vectors . Actions Continuous Actions - The number of concurrent continuous actions that the Agent can take. Discrete Branches - An array of integers, defines multiple concurrent discrete actions. The values in the Discrete Branches array correspond to the number of possible discrete values for each action branch. Model - The neural network model used for inference (obtained after training) Inference Device - Whether to use CPU or GPU to run the model during inference Behavior Type - Determines whether the Agent will do training, inference, or use its Heuristic() method: Default - the Agent will train if they connect to a python trainer, otherwise they will perform inference. Heuristic Only - the Agent will always use the Heuristic() method. Inference Only - the Agent will always perform inference. Team ID - Used to define the team for self-play Use Child Sensors - Whether to use all Sensor components attached to child GameObjects of this Agent. Max Step - The per-agent maximum number of steps. Once this number is reached, the Agent will be reset. Destroying an Agent You can destroy an Agent GameObject during the simulation. Make sure that there is always at least one Agent training at all times by either spawning a new Agent every time one is destroyed or by re-spawning new Agents when the whole environment resets. Defining Multi-agent Scenarios Teams for Adversarial Scenarios Self-play is triggered by including the self-play hyperparameter hierarchy in the trainer configuration . To distinguish opposing agents, set the team ID to different integer values in the behavior parameters script on the agent prefab. Team ID must be 0 or an integer greater than 0. In symmetric games, since all agents (even on opposing teams) will share the same policy, they should have the same 'Behavior Name' in their Behavior Parameters Script. In asymmetric games, they should have a different Behavior Name in their Behavior Parameters script. Note, in asymmetric games, the agents must have both different Behavior Names and different team IDs! For examples of how to use this feature, you can see the trainer configurations and agent prefabs for our Tennis and Soccer environments. Tennis and Soccer provide examples of symmetric games. To train an asymmetric game, specify trainer configurations for each of your behavior names and include the self-play hyperparameter hierarchy in both. Groups for Cooperative Scenarios Cooperative behavior in ML-Agents can be enabled by instantiating a SimpleMultiAgentGroup , typically in an environment controller or similar script, and adding agents to it using the RegisterAgent(Agent agent) method. Note that all agents added to the same SimpleMultiAgentGroup must have the same behavior name and Behavior Parameters. Using SimpleMultiAgentGroup enables the agents within a group to learn how to work together to achieve a common goal (i.e., maximize a group-given reward), even if one or more of the group members are removed before the episode ends. You can then use this group to add/set rewards, end or interrupt episodes at a group level using the AddGroupReward() , SetGroupReward() , EndGroupEpisode() , and GroupEpisodeInterrupted() methods. For example: // Create a Multi Agent Group in Start() or Initialize() m_AgentGroup = new SimpleMultiAgentGroup(); // Register agents in group at the beginning of an episode for (var agent in AgentList) { m_AgentGroup.RegisterAgent(agent); } // if the team scores a goal m_AgentGroup.AddGroupReward(rewardForGoal); // If the goal is reached and the episode is over m_AgentGroup.EndGroupEpisode(); ResetScene(); // If time ran out and we need to interrupt the episode m_AgentGroup.GroupEpisodeInterrupted(); ResetScene(); Multi Agent Groups should be used with the MA-POCA trainer, which is explicitly designed to train cooperative environments. This can be enabled by using the poca trainer - see the training configurations doc for more information on configuring MA-POCA. When using MA-POCA, agents which are deactivated or removed from the Scene during the episode will still learn to contribute to the group's long term rewards, even if they are not active in the scene to experience them. See the Cooperative Push Block environment for an example of how to use Multi Agent Groups, and the Dungeon Escape environment for an example of how the Multi Agent Group can be used with agents that are removed from the scene mid-episode. NOTE : Groups differ from Teams (for competitive settings) in the following way - Agents working together should be added to the same Group, while agents playing against each other should be given different Team Ids. If in the Scene there is one playing field and two teams, there should be two Groups, one for each team, and each team should be assigned a different Team Id. If this playing field is duplicated many times in the Scene (e.g. for training speedup), there should be two Groups per playing field , and two unique Team Ids for the entire Scene . In environments with both Groups and Team Ids configured, MA-POCA and self-play can be used together for training. In the diagram below, there are two agents on each team, and two playing fields where teams are pitted against each other. All the blue agents should share a Team Id (and the orange ones a different ID), and there should be four group managers, one per pair of agents. Please see the SoccerTwos environment for an example. Cooperative Behaviors Notes and Best Practices An agent can only be registered to one MultiAgentGroup at a time. If you want to re-assign an agent from one group to another, you have to unregister it from the current group first. Agents with different behavior names in the same group are not supported. Agents within groups should always set the Max Steps parameter in the Agent script to 0. Instead, handle Max Steps using the MultiAgentGroup by ending the episode for the entire Group using GroupEpisodeInterrupted() . EndGroupEpisode and GroupEpisodeInterrupted do the same job in the game, but has slightly different effect on the training. If the episode is completed, you would want to call EndGroupEpisode . But if the episode is not over but it has been running for enough steps, i.e. reaching max step, you would call GroupEpisodeInterrupted . If an agent finished earlier, e.g. completed tasks/be removed/be killed in the game, do not call EndEpisode() on the Agent. Instead, disable the agent and re-enable it when the next episode starts, or destroy the agent entirely. This is because calling EndEpisode() will call OnEpisodeBegin() , which will reset the agent immediately. While it is possible to call EndEpisode() in this way, it is usually not the desired behavior when training groups of agents. If an agent that was disabled in a scene needs to be re-enabled, it must be re-registered to the MultiAgentGroup. Group rewards are meant to reinforce agents to act in the group's best interest instead of individual ones, and are treated differently than individual agent rewards during training. So calling AddGroupReward() is not equivalent to calling agent.AddReward() on each agent in the group. You can still add incremental rewards to agents using Agent.AddReward() if they are in a Group. These rewards will only be given to those agents and are received when the Agent is active. Environments which use Multi Agent Groups can be trained using PPO or SAC, but agents will not be able to learn from group rewards after deactivation/removal, nor will they behave as cooperatively. Recording Demonstrations In order to record demonstrations from an agent, add the Demonstration Recorder component to a GameObject in the scene which contains an Agent component. Once added, it is possible to name the demonstration that will be recorded from the agent. When Record is checked, a demonstration will be created whenever the scene is played from the Editor. Depending on the complexity of the task, anywhere from a few minutes or a few hours of demonstration data may be necessary to be useful for imitation learning. To specify an exact number of steps you want to record use the Num Steps To Record field and the editor will end your play session automatically once that many steps are recorded. If you set Num Steps To Record to 0 then recording will continue until you manually end the play session. Once the play session ends a .demo file will be created in the Assets/Demonstrations folder (by default). This file contains the demonstrations. Clicking on the file will provide metadata about the demonstration in the inspector. You can then specify the path to this file in your training configurations .","title":"Agents"},{"location":"Learning-Environment-Design-Agents/#agents","text":"Table of Contents: Decisions Observations and Sensors Generating Observations Agent.CollectObservations() Observable Fields and Properties ISensor interface and SensorComponents Vector Observations One-hot encoding categorical information Normalization Stacking Vector Observation Summary & Best Practices Visual Observations Visual Observation Summary & Best Practices Raycast Observations RayCast Observation Summary & Best Practices Variable Length Observations Variable Length Observation Summary & Best Practices Goal Signal Goal Signal Summary & Best Practices Actions and Actuators Continuous Actions Discrete Actions Masking Discrete Actions Actions Summary & Best Practices Rewards Examples Rewards Summary & Best Practices Agent Properties Destroying an Agent Defining Multi-agent Scenarios Teams for Adversarial Scenarios Groups for Cooperative Scenarios Recording Demonstrations An agent is an entity that can observe its environment, decide on the best course of action using those observations, and execute those actions within its environment. Agents can be created in Unity by extending the Agent class. The most important aspects of creating agents that can successfully learn are the observations the agent collects, and the reward you assign to estimate the value of the agent's current state toward accomplishing its tasks. An Agent passes its observations to its Policy. The Policy then makes a decision and passes the chosen action back to the agent. Your agent code must execute the action, for example, move the agent in one direction or another. In order to train an agent using reinforcement learning , your agent must calculate a reward value at each action. The reward is used to discover the optimal decision-making policy. The Policy class abstracts out the decision making logic from the Agent itself so that you can use the same Policy in multiple Agents. How a Policy makes its decisions depends on the Behavior Parameters associated with the agent. If you set Behavior Type to Heuristic Only , the Agent will use its Heuristic() method to make decisions which can allow you to control the Agent manually or write your own Policy. If the Agent has a Model file, its Policy will use the neural network Model to take decisions. When you create an Agent, you should usually extend the base Agent class. This includes implementing the following methods: Agent.OnEpisodeBegin() \u2014 Called at the beginning of an Agent's episode, including at the beginning of the simulation. Agent.CollectObservations(VectorSensor sensor) \u2014 Called every step that the Agent requests a decision. This is one possible way for collecting the Agent's observations of the environment; see Generating Observations below for more options. Agent.OnActionReceived() \u2014 Called every time the Agent receives an action to take. Receives the action chosen by the Agent. It is also common to assign a reward in this method. Agent.Heuristic() - When the Behavior Type is set to Heuristic Only in the Behavior Parameters of the Agent, the Agent will use the Heuristic() method to generate the actions of the Agent. As such, the Heuristic() method writes to the array of floats provided to the Heuristic method as argument. Note : Do not create a new float array of action in the Heuristic() method, as this will prevent writing floats to the original action array. As a concrete example, here is how the Ball3DAgent class implements these methods: Agent.OnEpisodeBegin() \u2014 Resets the agent cube and ball to their starting positions. The function randomizes the reset values so that the training generalizes to more than a specific starting position and agent cube orientation. Agent.CollectObservations(VectorSensor sensor) \u2014 Adds information about the orientation of the agent cube, the ball velocity, and the relative position between the ball and the cube. Since the CollectObservations() method calls VectorSensor.AddObservation() such that vector size adds up to 8, the Behavior Parameters of the Agent are set with vector observation space with a state size of 8. Agent.OnActionReceived() \u2014 The action results in a small change in the agent cube's rotation at each step. In this example, an Agent receives a small positive reward for each step it keeps the ball on the agent cube's head and a larger, negative reward for dropping the ball. An Agent's episode is also ended when it drops the ball so that it will reset with a new ball for the next simulation step. Agent.Heuristic() - Converts the keyboard inputs into actions.","title":"Agents"},{"location":"Learning-Environment-Design-Agents/#decisions","text":"The observation-decision-action-reward cycle repeats each time the Agent request a decision. Agents will request a decision when Agent.RequestDecision() is called. If you need the Agent to request decisions on its own at regular intervals, add a Decision Requester component to the Agent's GameObject. Making decisions at regular step intervals is generally most appropriate for physics-based simulations. For example, an agent in a robotic simulator that must provide fine-control of joint torques should make its decisions every step of the simulation. On the other hand, an agent that only needs to make decisions when certain game or simulation events occur, such as in a turn-based game, should call Agent.RequestDecision() manually.","title":"Decisions"},{"location":"Learning-Environment-Design-Agents/#observations-and-sensors","text":"In order for an agent to learn, the observations should include all the information an agent needs to accomplish its task. Without sufficient and relevant information, an agent may learn poorly or may not learn at all. A reasonable approach for determining what information should be included is to consider what you would need to calculate an analytical solution to the problem, or what you would expect a human to be able to use to solve the problem.","title":"Observations and Sensors"},{"location":"Learning-Environment-Design-Agents/#generating-observations","text":"ML-Agents provides multiple ways for an Agent to make observations: 1. Overriding the Agent.CollectObservations() method and passing the observations to the provided VectorSensor . 1. Adding the [Observable] attribute to fields and properties on the Agent. 1. Implementing the ISensor interface, using a SensorComponent attached to the Agent to create the ISensor .","title":"Generating Observations"},{"location":"Learning-Environment-Design-Agents/#agentcollectobservations","text":"Agent.CollectObservations() is best used for aspects of the environment which are numerical and non-visual. The Policy class calls the CollectObservations(VectorSensor sensor) method of each Agent. Your implementation of this function must call VectorSensor.AddObservation to add vector observations. The VectorSensor.AddObservation method provides a number of overloads for adding common types of data to your observation vector. You can add Integers and booleans directly to the observation vector, as well as some common Unity data types such as Vector2 , Vector3 , and Quaternion . For examples of various state observation functions, you can look at the example environments included in the ML-Agents SDK. For instance, the 3DBall example uses the rotation of the platform, the relative position of the ball, and the velocity of the ball as its state observation. public GameObject ball; public override void CollectObservations(VectorSensor sensor) { // Orientation of the cube (2 floats) sensor.AddObservation(gameObject.transform.rotation.z); sensor.AddObservation(gameObject.transform.rotation.x); // Relative position of the ball to the cube (3 floats) sensor.AddObservation(ball.transform.position - gameObject.transform.position); // Velocity of the ball (3 floats) sensor.AddObservation(m_BallRb.velocity); // 8 floats total } As an experiment, you can remove the velocity components from the observation and retrain the 3DBall agent. While it will learn to balance the ball reasonably well, the performance of the agent without using velocity is noticeably worse. The observations passed to VectorSensor.AddObservation() must always contain the same number of elements must always be in the same order. If the number of observed entities in an environment can vary, you can pad the calls with zeros for any missing entities in a specific observation, or you can limit an agent's observations to a fixed subset. For example, instead of observing every enemy in an environment, you could only observe the closest five. Additionally, when you set up an Agent's Behavior Parameters in the Unity Editor, you must set the Vector Observations > Space Size to equal the number of floats that are written by CollectObservations() .","title":"Agent.CollectObservations()"},{"location":"Learning-Environment-Design-Agents/#observable-fields-and-properties","text":"Another approach is to define the relevant observations as fields or properties on your Agent class, and annotate them with an ObservableAttribute . For example, in the Ball3DHardAgent, the difference between positions could be observed by adding a property to the Agent: using Unity.MLAgents.Sensors.Reflection; public class Ball3DHardAgent : Agent { [Observable(numStackedObservations: 9)] Vector3 PositionDelta { get { return ball.transform.position - gameObject.transform.position; } } } ObservableAttribute currently supports most basic types (e.g. floats, ints, bools), as well as Vector2 , Vector3 , Vector4 , Quaternion , and enums. The behavior of ObservableAttribute s are controlled by the \"Observable Attribute Handling\" in the Agent's Behavior Parameters . The possible values for this are: * Ignore (default) - All ObservableAttributes on the Agent will be ignored. If there are no ObservableAttributes on the Agent, this will result in the fastest initialization time. * Exclude Inherited - Only members on the declared class will be examined; members that are inherited are ignored. This is a reasonable tradeoff between performance and flexibility. * Examine All All members on the class will be examined. This can lead to slower startup times. \"Exclude Inherited\" is generally sufficient, but if your Agent inherits from another Agent implementation that has Observable members, you will need to use \"Examine All\". Internally, ObservableAttribute uses reflection to determine which members of the Agent have ObservableAttributes, and also uses reflection to access the fields or invoke the properties at runtime. This may be slower than using CollectObservations or an ISensor, although this might not be enough to noticeably affect performance. NOTE : you do not need to adjust the Space Size in the Agent's Behavior Parameters when you add [Observable] fields or properties to an Agent, since their size can be computed before they are used.","title":"Observable Fields and Properties"},{"location":"Learning-Environment-Design-Agents/#isensor-interface-and-sensorcomponents","text":"The ISensor interface is generally intended for advanced users. The Write() method is used to actually generate the observation, but some other methods such as returning the shape of the observations must also be implemented. The SensorComponent abstract class is used to create the actual ISensor at runtime. It must be attached to the same GameObject as the Agent , or to a child GameObject . There are several SensorComponents provided in the API, including: - CameraSensorComponent - Uses images from a Camera as observations. - RenderTextureSensorComponent - Uses the content of a RenderTexture as observations. - RayPerceptionSensorComponent - Uses the information from set of ray casts as observations. - Match3SensorComponent - Uses the board of a Match-3 game as observations. - GridSensorComponent - Uses a set of box queries in a grid shape as observations. NOTE : you do not need to adjust the Space Size in the Agent's Behavior Parameters when using SensorComponents s. Internally, both Agent.CollectObservations and [Observable] attribute use an ISensors to write observations, although this is mostly abstracted from the user.","title":"ISensor interface and SensorComponents"},{"location":"Learning-Environment-Design-Agents/#vector-observations","text":"Both Agent.CollectObservations() and ObservableAttribute s produce vector observations, which are represented at lists of float s. ISensor s can produce both vector observations and visual observations, which are multi-dimensional arrays of floats. Below are some additional considerations when dealing with vector observations:","title":"Vector Observations"},{"location":"Learning-Environment-Design-Agents/#one-hot-encoding-categorical-information","text":"Type enumerations should be encoded in the one-hot style. That is, add an element to the feature vector for each element of enumeration, setting the element representing the observed member to one and set the rest to zero. For example, if your enumeration contains [Sword, Shield, Bow] and the agent observes that the current item is a Bow, you would add the elements: 0, 0, 1 to the feature vector. The following code example illustrates how to add. enum ItemType { Sword, Shield, Bow, LastItem } public override void CollectObservations(VectorSensor sensor) { for (int ci = 0; ci < (int)ItemType.LastItem; ci++) { sensor.AddObservation((int)currentItem == ci ? 1.0f : 0.0f); } } VectorSensor also provides a two-argument function AddOneHotObservation() as a shortcut for one-hot style observations. The following example is identical to the previous one. enum ItemType { Sword, Shield, Bow, LastItem } const int NUM_ITEM_TYPES = (int)ItemType.LastItem; public override void CollectObservations(VectorSensor sensor) { // The first argument is the selection index; the second is the // number of possibilities sensor.AddOneHotObservation((int)currentItem, NUM_ITEM_TYPES); } ObservableAttribute has built-in support for enums. Note that you don't need the LastItem placeholder in this case: enum ItemType { Sword, Shield, Bow } public class HeroAgent : Agent { [Observable] ItemType m_CurrentItem; }","title":"One-hot encoding categorical information"},{"location":"Learning-Environment-Design-Agents/#normalization","text":"For the best results when training, you should normalize the components of your feature vector to the range [-1, +1] or [0, 1]. When you normalize the values, the PPO neural network can often converge to a solution faster. Note that it isn't always necessary to normalize to these recommended ranges, but it is considered a best practice when using neural networks. The greater the variation in ranges between the components of your observation, the more likely that training will be affected. To normalize a value to [0, 1], you can use the following formula: normalizedValue = (currentValue - minValue)/(maxValue - minValue) :warning: For vectors, you should apply the above formula to each component (x, y, and z). Note that this is not the same as using the Vector3.normalized property or Vector3.Normalize() method in Unity (and similar for Vector2 ). Rotations and angles should also be normalized. For angles between 0 and 360 degrees, you can use the following formulas: Quaternion rotation = transform.rotation; Vector3 normalized = rotation.eulerAngles / 180.0f - Vector3.one; // [-1,1] Vector3 normalized = rotation.eulerAngles / 360.0f; // [0,1] For angles that can be outside the range [0,360], you can either reduce the angle, or, if the number of turns is significant, increase the maximum value used in your normalization formula.","title":"Normalization"},{"location":"Learning-Environment-Design-Agents/#stacking","text":"Stacking refers to repeating observations from previous steps as part of a larger observation. For example, consider an Agent that generates these observations in four steps step 1: [0.1] step 2: [0.2] step 3: [0.3] step 4: [0.4] If we use a stack size of 3, the observations would instead be: step 1: [0.1, 0.0, 0.0] step 2: [0.2, 0.1, 0.0] step 3: [0.3, 0.2, 0.1] step 4: [0.4, 0.3, 0.2] (The observations are padded with zeroes for the first stackSize-1 steps). This is a simple way to give an Agent limited \"memory\" without the complexity of adding a recurrent neural network (RNN). The steps for enabling stacking depends on how you generate observations: * For Agent.CollectObservations(), set \"Stacked Vectors\" on the Agent's Behavior Parameters to a value greater than 1. * For ObservableAttribute, set the numStackedObservations parameter in the constructor, e.g. [Observable(numStackedObservations: 2)] . * For ISensor s, wrap them in a StackingSensor (which is also an ISensor ). Generally, this should happen in the CreateSensor() method of your SensorComponent .","title":"Stacking"},{"location":"Learning-Environment-Design-Agents/#vector-observation-summary-best-practices","text":"Vector Observations should include all variables relevant for allowing the agent to take the optimally informed decision, and ideally no extraneous information. In cases where Vector Observations need to be remembered or compared over time, either an RNN should be used in the model, or the Stacked Vectors value in the agent GameObject's Behavior Parameters should be changed. Categorical variables such as type of object (Sword, Shield, Bow) should be encoded in one-hot fashion (i.e. 3 -> 0, 0, 1 ). This can be done automatically using the AddOneHotObservation() method of the VectorSensor , or using [Observable] on an enum field or property of the Agent. In general, all inputs should be normalized to be in the range 0 to +1 (or -1 to 1). For example, the x position information of an agent where the maximum possible value is maxValue should be recorded as VectorSensor.AddObservation(transform.position.x / maxValue); rather than VectorSensor.AddObservation(transform.position.x); . Positional information of relevant GameObjects should be encoded in relative coordinates wherever possible. This is often relative to the agent position.","title":"Vector Observation Summary &amp; Best Practices"},{"location":"Learning-Environment-Design-Agents/#visual-observations","text":"Visual observations are generally provided to agent via either a CameraSensor or RenderTextureSensor . These collect image information and transforms it into a 3D Tensor which can be fed into the convolutional neural network (CNN) of the agent policy. For more information on CNNs, see this guide . This allows agents to learn from spatial regularities in the observation images. It is possible to use visual and vector observations with the same agent. Agents using visual observations can capture state of arbitrary complexity and are useful when the state is difficult to describe numerically. However, they are also typically less efficient and slower to train, and sometimes don't succeed at all as compared to vector observations. As such, they should only be used when it is not possible to properly define the problem using vector or ray-cast observations. Visual observations can be derived from Cameras or RenderTextures within your scene. To add a visual observation to an Agent, add either a Camera Sensor Component or RenderTextures Sensor Component to the Agent. Then drag the camera or render texture you want to add to the Camera or RenderTexture field. You can have more than one camera or render texture and even use a combination of both attached to an Agent. For each visual observation, set the width and height of the image (in pixels) and whether or not the observation is color or grayscale. or Each Agent that uses the same Policy must have the same number of visual observations, and they must all have the same resolutions (including whether or not they are grayscale). Additionally, each Sensor Component on an Agent must have a unique name so that they can be sorted deterministically (the name must be unique for that Agent, but multiple Agents can have a Sensor Component with the same name). Visual observations also support stacking, by specifying Observation Stacks to a value greater than 1. The visual observations from the last stackSize steps will be stacked on the last dimension (channel dimension). When using RenderTexture visual observations, a handy feature for debugging is adding a Canvas , then adding a Raw Image with it's texture set to the Agent's RenderTexture . This will render the agent observation on the game screen. The GridWorld environment is an example on how to use a RenderTexture for both debugging and observation. Note that in this example, a Camera is rendered to a RenderTexture, which is then used for observations and debugging. To update the RenderTexture, the Camera must be asked to render every time a decision is requested within the game code. When using Cameras as observations directly, this is done automatically by the Agent.","title":"Visual Observations"},{"location":"Learning-Environment-Design-Agents/#visual-observation-summary-best-practices","text":"To collect visual observations, attach CameraSensor or RenderTextureSensor components to the agent GameObject. Visual observations should generally be used unless vector observations are not sufficient. Image size should be kept as small as possible, without the loss of needed details for decision making. Images should be made grayscale in situations where color information is not needed for making informed decisions.","title":"Visual Observation Summary &amp; Best Practices"},{"location":"Learning-Environment-Design-Agents/#raycast-observations","text":"Raycasts are another possible method for providing observations to an agent. This can be easily implemented by adding a RayPerceptionSensorComponent3D (or RayPerceptionSensorComponent2D ) to the Agent GameObject. During observations, several rays (or spheres, depending on settings) are cast into the physics world, and the objects that are hit determine the observation vector that is produced. Both sensor components have several settings: Detectable Tags A list of strings corresponding to the types of objects that the Agent should be able to distinguish between. For example, in the WallJump example, we use \"wall\", \"goal\", and \"block\" as the list of objects to detect. Rays Per Direction Determines the number of rays that are cast. One ray is always cast forward, and this many rays are cast to the left and right. Max Ray Degrees The angle (in degrees) for the outermost rays. 90 degrees corresponds to the left and right of the agent. Sphere Cast Radius The size of the sphere used for sphere casting. If set to 0, rays will be used instead of spheres. Rays may be more efficient, especially in complex scenes. Ray Length The length of the casts Ray Layer Mask The LayerMask passed to the raycast or spherecast. This can be used to ignore certain types of objects when casting. Observation Stacks The number of previous results to \"stack\" with the cast results. Note that this can be independent of the \"Stacked Vectors\" setting in Behavior Parameters . Start Vertical Offset (3D only) The vertical offset of the ray start point. End Vertical Offset (3D only) The vertical offset of the ray end point. In the example image above, the Agent has two RayPerceptionSensorComponent3D s. Both use 3 Rays Per Direction and 90 Max Ray Degrees. One of the components had a vertical offset, so the Agent can tell whether it's clear to jump over the wall. The total size of the created observations is (Observation Stacks) * (1 + 2 * Rays Per Direction) * (Num Detectable Tags + 2) so the number of rays and tags should be kept as small as possible to reduce the amount of data used. Note that this is separate from the State Size defined in Behavior Parameters , so you don't need to worry about the formula above when setting the State Size.","title":"Raycast Observations"},{"location":"Learning-Environment-Design-Agents/#raycast-observation-summary-best-practices","text":"Attach RayPerceptionSensorComponent3D or RayPerceptionSensorComponent2D to use. This observation type is best used when there is relevant spatial information for the agent that doesn't require a fully rendered image to convey. Use as few rays and tags as necessary to solve the problem in order to improve learning stability and agent performance.","title":"RayCast Observation Summary &amp; Best Practices"},{"location":"Learning-Environment-Design-Agents/#grid-observations","text":"Grid-base observations combine the advantages of 2D spatial representation in visual observations, and the flexibility of defining detectable objects in RayCast observations. The sensor uses a set of box queries in a grid shape and gives a top-down 2D view around the agent. This can be implemented by adding a GridSensorComponent to the Agent GameObject. During observations, the sensor detects the presence of detectable objects in each cell and encode that into one-hot representation. The collected information from each cell forms a 3D tensor observation and will be fed into the convolutional neural network (CNN) of the agent policy just like visual observations. The sensor component has the following settings: - Cell Scale The scale of each cell in the grid. - Grid Size Number of cells on each side of the grid. - Agent Game Object The Agent that holds the grid sensor. This is used to disambiguate objects with the same tag as the agent so that the agent doesn't detect itself. - Rotate With Agent Whether the grid rotates with the Agent. - Detectable Tags A list of strings corresponding to the types of objects that the Agent should be able to distinguish between. - Collider Mask The LayerMask passed to the collider detection. This can be used to ignore certain types of objects. - Initial Collider Buffer Size The initial size of the Collider buffer used in the non-allocating Physics calls for each cell. - Max Collider Buffer Size The max size of the Collider buffer used in the non-allocating Physics calls for each cell. The observation for each grid cell is a one-hot encoding of the detected object. The total size of the created observations is GridSize.x * GridSize.z * Num Detectable Tags so the number of detectable tags and size of the grid should be kept as small as possible to reduce the amount of data used. This makes a trade-off between the granularity of the observation and training speed. To allow more variety of observations that grid sensor can capture, the GridSensorComponent and the underlying GridSensorBase also provides interfaces that can be overridden to collect customized observation from detected objects. See the doc on extending grid Sensors for more details on custom grid sensors. Note : The GridSensor only works in 3D environments and will not behave properly in 2D environments.","title":"Grid Observations"},{"location":"Learning-Environment-Design-Agents/#grid-observation-summary-best-practices","text":"Attach GridSensorComponent to use. This observation type is best used when there is relevant non-visual spatial information that can be best captured in 2D representations. Use as small grid size and as few tags as necessary to solve the problem in order to improve learning stability and agent performance. Do not use GridSensor in a 2D game.","title":"Grid Observation Summary &amp; Best Practices"},{"location":"Learning-Environment-Design-Agents/#variable-length-observations","text":"It is possible for agents to collect observations from a varying number of GameObjects by using a BufferSensor . You can add a BufferSensor to your Agent by adding a BufferSensorComponent to its GameObject. The BufferSensor can be useful in situations in which the Agent must pay attention to a varying number of entities (for example, a varying number of enemies or projectiles). On the trainer side, the BufferSensor is processed using an attention module. More information about attention mechanisms can be found here . Training or doing inference with variable length observations can be slower than using a flat vector observation. However, attention mechanisms enable solving problems that require comparative reasoning between entities in a scene such as our Sorter environment . Note that even though the BufferSensor can process a variable number of entities, you still need to define a maximum number of entities. This is because our network architecture requires to know what the shape of the observations will be. If fewer entities are observed than the maximum, the observation will be padded with zeros and the trainer will ignore the padded observations. Note that attention layers are invariant to the order of the entities, so there is no need to properly \"order\" the entities before feeding them into the BufferSensor . The BufferSensorComponent Editor inspector has two arguments: - Observation Size : This is how many floats each entities will be represented with. This number is fixed and all entities must have the same representation. For example, if the entities you want to put into the BufferSensor have for relevant information position and speed, then the Observation Size should be 6 floats. - Maximum Number of Entities : This is the maximum number of entities the BufferSensor will be able to collect. To add an entity's observations to a BufferSensorComponent , you need to call BufferSensorComponent.AppendObservation() in the Agent.CollectObservations() method with a float array of size Observation Size as argument. Note : Currently, the observations put into the BufferSensor are not normalized, you will need to normalize your observations manually between -1 and 1.","title":"Variable Length Observations"},{"location":"Learning-Environment-Design-Agents/#variable-length-observation-summary-best-practices","text":"Attach BufferSensorComponent to use. Call BufferSensorComponent.AppendObservation() in the Agent.CollectObservations() methodto add the observations of an entity to the BufferSensor . Normalize the entities observations before feeding them into the BufferSensor .","title":"Variable Length Observation Summary &amp; Best Practices"},{"location":"Learning-Environment-Design-Agents/#goal-signal","text":"It is possible for agents to collect observations that will be treated as \"goal signal\". A goal signal is used to condition the policy of the agent, meaning that if the goal changes, the policy (i.e. the mapping from observations to actions) will change as well. Note that this is true for any observation since all observations influence the policy of the Agent to some degree. But by specifying a goal signal explicitly, we can make this conditioning more important to the agent. This feature can be used in settings where an agent must learn to solve different tasks that are similar by some aspects because the agent will learn to reuse learnings from different tasks to generalize better. In Unity, you can specify that a VectorSensor or a CameraSensor is a goal by attaching a VectorSensorComponent or a CameraSensorComponent to the Agent and selecting Goal Signal as Observation Type . On the trainer side, there are two different ways to condition the policy. This setting is determined by the conditioning_type parameter . If set to hyper (default) a HyperNetwork will be used to generate some of the weights of the policy using the goal observations as input. Note that using a HyperNetwork requires a lot of computations, it is recommended to use a smaller number of hidden units in the policy to alleviate this. If set to none the goal signal will be considered as regular observations. For an example on how to use a goal signal, see the GridWorld example .","title":"Goal Signal"},{"location":"Learning-Environment-Design-Agents/#goal-signal-summary-best-practices","text":"Attach a VectorSensorComponent or CameraSensorComponent to an agent and set the observation type to goal to use the feature. Set the conditioning_type parameter in the training configuration. Reduce the number of hidden units in the network when using the HyperNetwork conditioning type.","title":"Goal Signal Summary &amp; Best Practices"},{"location":"Learning-Environment-Design-Agents/#actions-and-actuators","text":"An action is an instruction from the Policy that the agent carries out. The action is passed to the an IActionReceiver (either an Agent or an IActuator ) as the ActionBuffers parameter when the Academy invokes the IActionReciever.OnActionReceived() function. There are two types of actions supported: Continuous and Discrete . Neither the Policy nor the training algorithm know anything about what the action values themselves mean. The training algorithm simply tries different values for the action list and observes the affect on the accumulated rewards over time and many training episodes. Thus, the only place actions are defined for an Agent is in the OnActionReceived() function. For example, if you designed an agent to move in two dimensions, you could use either continuous or the discrete actions. In the continuous case, you would set the action size to two (one for each dimension), and the agent's Policy would output an action with two floating point values. In the discrete case, you would use one Branch with a size of four (one for each direction), and the Policy would create an action array containing a single element with a value ranging from zero to three. Alternatively, you could create two branches of size two (one for horizontal movement and one for vertical movement), and the Policy would output an action array containing two elements with values ranging from zero to one. You could alternatively use a combination of continuous and discrete actions e.g., using one continuous action for horizontal movement and a discrete branch of size two for the vertical movement. Note that when you are programming actions for an agent, it is often helpful to test your action logic using the Heuristic() method of the Agent, which lets you map keyboard commands to actions.","title":"Actions and Actuators"},{"location":"Learning-Environment-Design-Agents/#continuous-actions","text":"When an Agent's Policy has Continuous actions, the ActionBuffers.ContinuousActions passed to the Agent's OnActionReceived() function is an array with length equal to the Continuous Action Size property value. The individual values in the array have whatever meanings that you ascribe to them. If you assign an element in the array as the speed of an Agent, for example, the training process learns to control the speed of the Agent through this parameter. The 3DBall example uses continuous actions with two control values. These control values are applied as rotation to the cube: public override void OnActionReceived(ActionBuffers actionBuffers) { var actionZ = 2f * Mathf.Clamp(actionBuffers.ContinuousActions[0], -1f, 1f); var actionX = 2f * Mathf.Clamp(actionBuffers.ContinuousActions[1], -1f, 1f); gameObject.transform.Rotate(new Vector3(0, 0, 1), actionZ); gameObject.transform.Rotate(new Vector3(1, 0, 0), actionX); } By default the output from our provided PPO algorithm pre-clamps the values of ActionBuffers.ContinuousActions into the [-1, 1] range. It is a best practice to manually clip these as well, if you plan to use a 3rd party algorithm with your environment. As shown above, you can scale the control values as needed after clamping them.","title":"Continuous Actions"},{"location":"Learning-Environment-Design-Agents/#discrete-actions","text":"When an Agent's Policy uses discrete actions, the ActionBuffers.DiscreteActions passed to the Agent's OnActionReceived() function is an array of integers with length equal to Discrete Branch Size . When defining the discrete actions, Branches is an array of integers, each value corresponds to the number of possibilities for each branch. For example, if we wanted an Agent that can move in a plane and jump, we could define two branches (one for motion and one for jumping) because we want our agent be able to move and jump concurrently. We define the first branch to have 5 possible actions (don't move, go left, go right, go backward, go forward) and the second one to have 2 possible actions (don't jump, jump). The OnActionReceived() method would look something like: // Get the action index for movement int movement = actionBuffers.DiscreteActions[0]; // Get the action index for jumping int jump = actionBuffers.DiscreteActions[1]; // Look up the index in the movement action list: if (movement == 1) { directionX = -1; } if (movement == 2) { directionX = 1; } if (movement == 3) { directionZ = -1; } if (movement == 4) { directionZ = 1; } // Look up the index in the jump action list: if (jump == 1 && IsGrounded()) { directionY = 1; } // Apply the action results to move the Agent gameObject.GetComponent<Rigidbody>().AddForce( new Vector3( directionX * 40f, directionY * 300f, directionZ * 40f));","title":"Discrete Actions"},{"location":"Learning-Environment-Design-Agents/#masking-discrete-actions","text":"When using Discrete Actions, it is possible to specify that some actions are impossible for the next decision. When the Agent is controlled by a neural network, the Agent will be unable to perform the specified action. Note that when the Agent is controlled by its Heuristic, the Agent will still be able to decide to perform the masked action. In order to disallow an action, override the Agent.WriteDiscreteActionMask() virtual method, and call SetActionEnabled() on the provided IDiscreteActionMask : public override void WriteDiscreteActionMask(IDiscreteActionMask actionMask) { actionMask.SetActionEnabled(branch, actionIndex, isEnabled); } Where: branch is the index (starting at 0) of the branch on which you want to allow or disallow the action actionIndex is the index of the action that you want to allow or disallow. isEnabled is a bool indicating whether the action should be allowed or now. For example, if you have an Agent with 2 branches and on the first branch (branch 0) there are 4 possible actions : \"do nothing\" , \"jump\" , \"shoot\" and \"change weapon\" . Then with the code bellow, the Agent will either \"do nothing\" or \"change weapon\" for their next decision (since action index 1 and 2 are masked) actionMask.SetActionEnabled(0, 1, false); actionMask.SetActionEnabled(0, 2, false); Notes: You can call SetActionEnabled multiple times if you want to put masks on multiple branches. At each step, the state of an action is reset and enabled by default. You cannot mask all the actions of a branch. You cannot mask actions in continuous control.","title":"Masking Discrete Actions"},{"location":"Learning-Environment-Design-Agents/#iactuator-interface-and-actuatorcomponents","text":"The Actuator API allows users to abstract behavior out of Agents and in to components (similar to the ISensor API). The IActuator interface and Agent class both implement the IActionReceiver interface to allow for backward compatibility with the current Agent.OnActionReceived . This means you will not have to change your code until you decide to use the IActuator API. Like the ISensor interface, the IActuator interface is intended for advanced users. The ActuatorComponent abstract class is used to create the actual IActuator at runtime. It must be attached to the same GameObject as the Agent , or to a child GameObject . Actuators and all of their data structures are initialized during Agent.Initialize . This was done to prevent an unexpected allocations at runtime. You can find an example of an IActuator implementation in the Basic example scene. NOTE : you do not need to adjust the Actions in the Agent's Behavior Parameters when using an IActuator and ActuatorComponents . Internally, Agent.OnActionReceived uses an IActuator to send actions to the Agent, although this is mostly abstracted from the user.","title":"IActuator interface and ActuatorComponents"},{"location":"Learning-Environment-Design-Agents/#actions-summary-best-practices","text":"Agents can use Discrete and/or Continuous actions. Discrete actions can have multiple action branches, and it's possible to mask certain actions so that they won't be taken. In general, fewer actions will make for easier learning. Be sure to set the Continuous Action Size and Discrete Branch Size to the desired number for each type of action, and not greater, as doing the latter can interfere with the efficiency of the training process. Continuous action values should be clipped to an appropriate range. The provided PPO model automatically clips these values between -1 and 1, but third party training systems may not do so.","title":"Actions Summary &amp; Best Practices"},{"location":"Learning-Environment-Design-Agents/#rewards","text":"In reinforcement learning, the reward is a signal that the agent has done something right. The PPO reinforcement learning algorithm works by optimizing the choices an agent makes such that the agent earns the highest cumulative reward over time. The better your reward mechanism, the better your agent will learn. Note: Rewards are not used during inference by an Agent using a trained model and is also not used during imitation learning. Perhaps the best advice is to start simple and only add complexity as needed. In general, you should reward results rather than actions you think will lead to the desired results. You can even use the Agent's Heuristic to control the Agent while watching how it accumulates rewards. Allocate rewards to an Agent by calling the AddReward() or SetReward() methods on the agent. The reward assigned between each decision should be in the range [-1,1]. Values outside this range can lead to unstable training. The reward value is reset to zero when the agent receives a new decision. If there are multiple calls to AddReward() for a single agent decision, the rewards will be summed together to evaluate how good the previous decision was. The SetReward() will override all previous rewards given to an agent since the previous decision.","title":"Rewards"},{"location":"Learning-Environment-Design-Agents/#examples","text":"You can examine the OnActionReceived() functions defined in the example environments to see how those projects allocate rewards. The GridAgent class in the GridWorld example uses a very simple reward system: Collider[] hitObjects = Physics.OverlapBox(trueAgent.transform.position, new Vector3(0.3f, 0.3f, 0.3f)); if (hitObjects.Where(col => col.gameObject.tag == \"goal\").ToArray().Length == 1) { AddReward(1.0f); EndEpisode(); } else if (hitObjects.Where(col => col.gameObject.tag == \"pit\").ToArray().Length == 1) { AddReward(-1f); EndEpisode(); } The agent receives a positive reward when it reaches the goal and a negative reward when it falls into the pit. Otherwise, it gets no rewards. This is an example of a sparse reward system. The agent must explore a lot to find the infrequent reward. In contrast, the AreaAgent in the Area example gets a small negative reward every step. In order to get the maximum reward, the agent must finish its task of reaching the goal square as quickly as possible: AddReward( -0.005f); MoveAgent(act); if (gameObject.transform.position.y < 0.0f || Mathf.Abs(gameObject.transform.position.x - area.transform.position.x) > 8f || Mathf.Abs(gameObject.transform.position.z + 5 - area.transform.position.z) > 8) { AddReward(-1f); EndEpisode(); } The agent also gets a larger negative penalty if it falls off the playing surface. The Ball3DAgent in the 3DBall takes a similar approach, but allocates a small positive reward as long as the agent balances the ball. The agent can maximize its rewards by keeping the ball on the platform: SetReward(0.1f); // When ball falls mark Agent as finished and give a negative penalty if ((ball.transform.position.y - gameObject.transform.position.y) < -2f || Mathf.Abs(ball.transform.position.x - gameObject.transform.position.x) > 3f || Mathf.Abs(ball.transform.position.z - gameObject.transform.position.z) > 3f) { SetReward(-1f); EndEpisode(); } The Ball3DAgent also assigns a negative penalty when the ball falls off the platform. Note that all of these environments make use of the EndEpisode() method, which manually terminates an episode when a termination condition is reached. This can be called independently of the Max Step property.","title":"Examples"},{"location":"Learning-Environment-Design-Agents/#rewards-summary-best-practices","text":"Use AddReward() to accumulate rewards between decisions. Use SetReward() to overwrite any previous rewards accumulate between decisions. The magnitude of any given reward should typically not be greater than 1.0 in order to ensure a more stable learning process. Positive rewards are often more helpful to shaping the desired behavior of an agent than negative rewards. Excessive negative rewards can result in the agent failing to learn any meaningful behavior. For locomotion tasks, a small positive reward (+0.1) for forward velocity is typically used. If you want the agent to finish a task quickly, it is often helpful to provide a small penalty every step (-0.05) that the agent does not complete the task. In this case completion of the task should also coincide with the end of the episode by calling EndEpisode() on the agent when it has accomplished its goal.","title":"Rewards Summary &amp; Best Practices"},{"location":"Learning-Environment-Design-Agents/#agent-properties","text":"Behavior Parameters - The parameters dictating what Policy the Agent will receive. Behavior Name - The identifier for the behavior. Agents with the same behavior name will learn the same policy. Vector Observation Space Size - Length of vector observation for the Agent. Stacked Vectors - The number of previous vector observations that will be stacked and used collectively for decision making. This results in the effective size of the vector observation being passed to the Policy being: Space Size x Stacked Vectors . Actions Continuous Actions - The number of concurrent continuous actions that the Agent can take. Discrete Branches - An array of integers, defines multiple concurrent discrete actions. The values in the Discrete Branches array correspond to the number of possible discrete values for each action branch. Model - The neural network model used for inference (obtained after training) Inference Device - Whether to use CPU or GPU to run the model during inference Behavior Type - Determines whether the Agent will do training, inference, or use its Heuristic() method: Default - the Agent will train if they connect to a python trainer, otherwise they will perform inference. Heuristic Only - the Agent will always use the Heuristic() method. Inference Only - the Agent will always perform inference. Team ID - Used to define the team for self-play Use Child Sensors - Whether to use all Sensor components attached to child GameObjects of this Agent. Max Step - The per-agent maximum number of steps. Once this number is reached, the Agent will be reset.","title":"Agent Properties"},{"location":"Learning-Environment-Design-Agents/#destroying-an-agent","text":"You can destroy an Agent GameObject during the simulation. Make sure that there is always at least one Agent training at all times by either spawning a new Agent every time one is destroyed or by re-spawning new Agents when the whole environment resets.","title":"Destroying an Agent"},{"location":"Learning-Environment-Design-Agents/#defining-multi-agent-scenarios","text":"","title":"Defining Multi-agent Scenarios"},{"location":"Learning-Environment-Design-Agents/#teams-for-adversarial-scenarios","text":"Self-play is triggered by including the self-play hyperparameter hierarchy in the trainer configuration . To distinguish opposing agents, set the team ID to different integer values in the behavior parameters script on the agent prefab. Team ID must be 0 or an integer greater than 0. In symmetric games, since all agents (even on opposing teams) will share the same policy, they should have the same 'Behavior Name' in their Behavior Parameters Script. In asymmetric games, they should have a different Behavior Name in their Behavior Parameters script. Note, in asymmetric games, the agents must have both different Behavior Names and different team IDs! For examples of how to use this feature, you can see the trainer configurations and agent prefabs for our Tennis and Soccer environments. Tennis and Soccer provide examples of symmetric games. To train an asymmetric game, specify trainer configurations for each of your behavior names and include the self-play hyperparameter hierarchy in both.","title":"Teams for Adversarial Scenarios"},{"location":"Learning-Environment-Design-Agents/#groups-for-cooperative-scenarios","text":"Cooperative behavior in ML-Agents can be enabled by instantiating a SimpleMultiAgentGroup , typically in an environment controller or similar script, and adding agents to it using the RegisterAgent(Agent agent) method. Note that all agents added to the same SimpleMultiAgentGroup must have the same behavior name and Behavior Parameters. Using SimpleMultiAgentGroup enables the agents within a group to learn how to work together to achieve a common goal (i.e., maximize a group-given reward), even if one or more of the group members are removed before the episode ends. You can then use this group to add/set rewards, end or interrupt episodes at a group level using the AddGroupReward() , SetGroupReward() , EndGroupEpisode() , and GroupEpisodeInterrupted() methods. For example: // Create a Multi Agent Group in Start() or Initialize() m_AgentGroup = new SimpleMultiAgentGroup(); // Register agents in group at the beginning of an episode for (var agent in AgentList) { m_AgentGroup.RegisterAgent(agent); } // if the team scores a goal m_AgentGroup.AddGroupReward(rewardForGoal); // If the goal is reached and the episode is over m_AgentGroup.EndGroupEpisode(); ResetScene(); // If time ran out and we need to interrupt the episode m_AgentGroup.GroupEpisodeInterrupted(); ResetScene(); Multi Agent Groups should be used with the MA-POCA trainer, which is explicitly designed to train cooperative environments. This can be enabled by using the poca trainer - see the training configurations doc for more information on configuring MA-POCA. When using MA-POCA, agents which are deactivated or removed from the Scene during the episode will still learn to contribute to the group's long term rewards, even if they are not active in the scene to experience them. See the Cooperative Push Block environment for an example of how to use Multi Agent Groups, and the Dungeon Escape environment for an example of how the Multi Agent Group can be used with agents that are removed from the scene mid-episode. NOTE : Groups differ from Teams (for competitive settings) in the following way - Agents working together should be added to the same Group, while agents playing against each other should be given different Team Ids. If in the Scene there is one playing field and two teams, there should be two Groups, one for each team, and each team should be assigned a different Team Id. If this playing field is duplicated many times in the Scene (e.g. for training speedup), there should be two Groups per playing field , and two unique Team Ids for the entire Scene . In environments with both Groups and Team Ids configured, MA-POCA and self-play can be used together for training. In the diagram below, there are two agents on each team, and two playing fields where teams are pitted against each other. All the blue agents should share a Team Id (and the orange ones a different ID), and there should be four group managers, one per pair of agents. Please see the SoccerTwos environment for an example.","title":"Groups for Cooperative Scenarios"},{"location":"Learning-Environment-Design-Agents/#cooperative-behaviors-notes-and-best-practices","text":"An agent can only be registered to one MultiAgentGroup at a time. If you want to re-assign an agent from one group to another, you have to unregister it from the current group first. Agents with different behavior names in the same group are not supported. Agents within groups should always set the Max Steps parameter in the Agent script to 0. Instead, handle Max Steps using the MultiAgentGroup by ending the episode for the entire Group using GroupEpisodeInterrupted() . EndGroupEpisode and GroupEpisodeInterrupted do the same job in the game, but has slightly different effect on the training. If the episode is completed, you would want to call EndGroupEpisode . But if the episode is not over but it has been running for enough steps, i.e. reaching max step, you would call GroupEpisodeInterrupted . If an agent finished earlier, e.g. completed tasks/be removed/be killed in the game, do not call EndEpisode() on the Agent. Instead, disable the agent and re-enable it when the next episode starts, or destroy the agent entirely. This is because calling EndEpisode() will call OnEpisodeBegin() , which will reset the agent immediately. While it is possible to call EndEpisode() in this way, it is usually not the desired behavior when training groups of agents. If an agent that was disabled in a scene needs to be re-enabled, it must be re-registered to the MultiAgentGroup. Group rewards are meant to reinforce agents to act in the group's best interest instead of individual ones, and are treated differently than individual agent rewards during training. So calling AddGroupReward() is not equivalent to calling agent.AddReward() on each agent in the group. You can still add incremental rewards to agents using Agent.AddReward() if they are in a Group. These rewards will only be given to those agents and are received when the Agent is active. Environments which use Multi Agent Groups can be trained using PPO or SAC, but agents will not be able to learn from group rewards after deactivation/removal, nor will they behave as cooperatively.","title":"Cooperative Behaviors Notes and Best Practices"},{"location":"Learning-Environment-Design-Agents/#recording-demonstrations","text":"In order to record demonstrations from an agent, add the Demonstration Recorder component to a GameObject in the scene which contains an Agent component. Once added, it is possible to name the demonstration that will be recorded from the agent. When Record is checked, a demonstration will be created whenever the scene is played from the Editor. Depending on the complexity of the task, anywhere from a few minutes or a few hours of demonstration data may be necessary to be useful for imitation learning. To specify an exact number of steps you want to record use the Num Steps To Record field and the editor will end your play session automatically once that many steps are recorded. If you set Num Steps To Record to 0 then recording will continue until you manually end the play session. Once the play session ends a .demo file will be created in the Assets/Demonstrations folder (by default). This file contains the demonstrations. Clicking on the file will provide metadata about the demonstration in the inspector. You can then specify the path to this file in your training configurations .","title":"Recording Demonstrations"},{"location":"Learning-Environment-Design/","text":"Designing a Learning Environment This page contains general advice on how to design your learning environment, in addition to overviewing aspects of the ML-Agents Unity SDK that pertain to setting up your scene and simulation as opposed to designing your agents within the scene. We have a dedicated page on Designing Agents which includes how to instrument observations, actions and rewards, define teams for multi-agent scenarios and record agent demonstrations for imitation learning. To help on-board to the entire set of functionality provided by the ML-Agents Toolkit, we recommend exploring our API documentation . Additionally, our example environments are a great resource as they provide sample usage of almost all of our features. The Simulation and Training Process Training and simulation proceed in steps orchestrated by the ML-Agents Academy class. The Academy works with Agent objects in the scene to step through the simulation. During training, the external Python training process communicates with the Academy to run a series of episodes while it collects data and optimizes its neural network model. When training is completed successfully, you can add the trained model file to your Unity project for later use. The ML-Agents Academy class orchestrates the agent simulation loop as follows: Calls your Academy's OnEnvironmentReset delegate. Calls the OnEpisodeBegin() function for each Agent in the scene. Gathers information about the scene. This is done by calling the CollectObservations(VectorSensor sensor) function for each Agent in the scene, as well as updating their sensor and collecting the resulting observations. Uses each Agent's Policy to decide on the Agent's next action. Calls the OnActionReceived() function for each Agent in the scene, passing in the action chosen by the Agent's Policy. Calls the Agent's OnEpisodeBegin() function if the Agent has reached its Max Step count or has otherwise marked itself as EndEpisode() . To create a training environment, extend the Agent class to implement the above methods whether you need to implement them or not depends on your specific scenario. Organizing the Unity Scene To train and use the ML-Agents Toolkit in a Unity scene, the scene as many Agent subclasses as you need. Agent instances should be attached to the GameObject representing that Agent. Academy The Academy is a singleton which orchestrates Agents and their decision making processes. Only a single Academy exists at a time. Academy resetting To alter the environment at the start of each episode, add your method to the Academy's OnEnvironmentReset action. public class MySceneBehavior : MonoBehaviour { public void Awake() { Academy.Instance.OnEnvironmentReset += EnvironmentReset; } void EnvironmentReset() { // Reset the scene here } } For example, you might want to reset an Agent to its starting position or move a goal to a random position. An environment resets when the reset() method is called on the Python UnityEnvironment . When you reset an environment, consider the factors that should change so that training is generalizable to different conditions. For example, if you were training a maze-solving agent, you would probably want to change the maze itself for each training episode. Otherwise, the agent would probably on learn to solve one, particular maze, not mazes in general. Multiple Areas In many of the example environments, many copies of the training area are instantiated in the scene. This generally speeds up training, allowing the environment to gather many experiences in parallel. This can be achieved simply by instantiating many Agents with the same Behavior Name. If possible, consider designing your scene to support multiple areas. Check out our example environments to see examples of multiple areas. Additionally, the Making a New Learning Environment guide demonstrates this option. Environments When you create a training environment in Unity, you must set up the scene so that it can be controlled by the external training process. Considerations include: The training scene must start automatically when your Unity application is launched by the training process. The Academy must reset the scene to a valid starting point for each episode of training. A training episode must have a definite end \u2014 either using Max Steps or by each Agent ending its episode manually with EndEpisode() . Environment Parameters Curriculum learning and environment parameter randomization are two training methods that control specific parameters in your environment. As such, it is important to ensure that your environment parameters are updated at each step to the correct values. To enable this, we expose a EnvironmentParameters C# class that you can use to retrieve the values of the parameters defined in the training configurations for both of those features. Please see our documentation for curriculum learning and environment parameter randomization for details. We recommend modifying the environment from the Agent's OnEpisodeBegin() function by leveraging Academy.Instance.EnvironmentParameters . See the WallJump example environment for a sample usage (specifically, WallJumpAgent.cs ). Agent The Agent class represents an actor in the scene that collects observations and carries out actions. The Agent class is typically attached to the GameObject in the scene that otherwise represents the actor \u2014 for example, to a player object in a football game or a car object in a vehicle simulation. Every Agent must have appropriate Behavior Parameters . Generally, when creating an Agent, you should extend the Agent class and implement the CollectObservations(VectorSensor sensor) and OnActionReceived() methods: CollectObservations(VectorSensor sensor) \u2014 Collects the Agent's observation of its environment. OnActionReceived() \u2014 Carries out the action chosen by the Agent's Policy and assigns a reward to the current state. Your implementations of these functions determine how the Behavior Parameters assigned to this Agent must be set. You must also determine how an Agent finishes its task or times out. You can manually terminate an Agent episode in your OnActionReceived() function when the Agent has finished (or irrevocably failed) its task by calling the EndEpisode() function. You can also set the Agent's Max Steps property to a positive value and the Agent will consider the episode over after it has taken that many steps. You can use the Agent.OnEpisodeBegin() function to prepare the Agent to start again. See Agents for detailed information about programming your own Agents. Recording Statistics We offer developers a mechanism to record statistics from within their Unity environments. These statistics are aggregated and generated during the training process. To record statistics, see the StatsRecorder C# class. See the FoodCollector example environment for a sample usage (specifically, FoodCollectorSettings.cs ).","title":"Designing a Learning Environment"},{"location":"Learning-Environment-Design/#designing-a-learning-environment","text":"This page contains general advice on how to design your learning environment, in addition to overviewing aspects of the ML-Agents Unity SDK that pertain to setting up your scene and simulation as opposed to designing your agents within the scene. We have a dedicated page on Designing Agents which includes how to instrument observations, actions and rewards, define teams for multi-agent scenarios and record agent demonstrations for imitation learning. To help on-board to the entire set of functionality provided by the ML-Agents Toolkit, we recommend exploring our API documentation . Additionally, our example environments are a great resource as they provide sample usage of almost all of our features.","title":"Designing a Learning Environment"},{"location":"Learning-Environment-Design/#the-simulation-and-training-process","text":"Training and simulation proceed in steps orchestrated by the ML-Agents Academy class. The Academy works with Agent objects in the scene to step through the simulation. During training, the external Python training process communicates with the Academy to run a series of episodes while it collects data and optimizes its neural network model. When training is completed successfully, you can add the trained model file to your Unity project for later use. The ML-Agents Academy class orchestrates the agent simulation loop as follows: Calls your Academy's OnEnvironmentReset delegate. Calls the OnEpisodeBegin() function for each Agent in the scene. Gathers information about the scene. This is done by calling the CollectObservations(VectorSensor sensor) function for each Agent in the scene, as well as updating their sensor and collecting the resulting observations. Uses each Agent's Policy to decide on the Agent's next action. Calls the OnActionReceived() function for each Agent in the scene, passing in the action chosen by the Agent's Policy. Calls the Agent's OnEpisodeBegin() function if the Agent has reached its Max Step count or has otherwise marked itself as EndEpisode() . To create a training environment, extend the Agent class to implement the above methods whether you need to implement them or not depends on your specific scenario.","title":"The Simulation and Training Process"},{"location":"Learning-Environment-Design/#organizing-the-unity-scene","text":"To train and use the ML-Agents Toolkit in a Unity scene, the scene as many Agent subclasses as you need. Agent instances should be attached to the GameObject representing that Agent.","title":"Organizing the Unity Scene"},{"location":"Learning-Environment-Design/#academy","text":"The Academy is a singleton which orchestrates Agents and their decision making processes. Only a single Academy exists at a time.","title":"Academy"},{"location":"Learning-Environment-Design/#academy-resetting","text":"To alter the environment at the start of each episode, add your method to the Academy's OnEnvironmentReset action. public class MySceneBehavior : MonoBehaviour { public void Awake() { Academy.Instance.OnEnvironmentReset += EnvironmentReset; } void EnvironmentReset() { // Reset the scene here } } For example, you might want to reset an Agent to its starting position or move a goal to a random position. An environment resets when the reset() method is called on the Python UnityEnvironment . When you reset an environment, consider the factors that should change so that training is generalizable to different conditions. For example, if you were training a maze-solving agent, you would probably want to change the maze itself for each training episode. Otherwise, the agent would probably on learn to solve one, particular maze, not mazes in general.","title":"Academy resetting"},{"location":"Learning-Environment-Design/#multiple-areas","text":"In many of the example environments, many copies of the training area are instantiated in the scene. This generally speeds up training, allowing the environment to gather many experiences in parallel. This can be achieved simply by instantiating many Agents with the same Behavior Name. If possible, consider designing your scene to support multiple areas. Check out our example environments to see examples of multiple areas. Additionally, the Making a New Learning Environment guide demonstrates this option.","title":"Multiple Areas"},{"location":"Learning-Environment-Design/#environments","text":"When you create a training environment in Unity, you must set up the scene so that it can be controlled by the external training process. Considerations include: The training scene must start automatically when your Unity application is launched by the training process. The Academy must reset the scene to a valid starting point for each episode of training. A training episode must have a definite end \u2014 either using Max Steps or by each Agent ending its episode manually with EndEpisode() .","title":"Environments"},{"location":"Learning-Environment-Design/#environment-parameters","text":"Curriculum learning and environment parameter randomization are two training methods that control specific parameters in your environment. As such, it is important to ensure that your environment parameters are updated at each step to the correct values. To enable this, we expose a EnvironmentParameters C# class that you can use to retrieve the values of the parameters defined in the training configurations for both of those features. Please see our documentation for curriculum learning and environment parameter randomization for details. We recommend modifying the environment from the Agent's OnEpisodeBegin() function by leveraging Academy.Instance.EnvironmentParameters . See the WallJump example environment for a sample usage (specifically, WallJumpAgent.cs ).","title":"Environment Parameters"},{"location":"Learning-Environment-Design/#agent","text":"The Agent class represents an actor in the scene that collects observations and carries out actions. The Agent class is typically attached to the GameObject in the scene that otherwise represents the actor \u2014 for example, to a player object in a football game or a car object in a vehicle simulation. Every Agent must have appropriate Behavior Parameters . Generally, when creating an Agent, you should extend the Agent class and implement the CollectObservations(VectorSensor sensor) and OnActionReceived() methods: CollectObservations(VectorSensor sensor) \u2014 Collects the Agent's observation of its environment. OnActionReceived() \u2014 Carries out the action chosen by the Agent's Policy and assigns a reward to the current state. Your implementations of these functions determine how the Behavior Parameters assigned to this Agent must be set. You must also determine how an Agent finishes its task or times out. You can manually terminate an Agent episode in your OnActionReceived() function when the Agent has finished (or irrevocably failed) its task by calling the EndEpisode() function. You can also set the Agent's Max Steps property to a positive value and the Agent will consider the episode over after it has taken that many steps. You can use the Agent.OnEpisodeBegin() function to prepare the Agent to start again. See Agents for detailed information about programming your own Agents.","title":"Agent"},{"location":"Learning-Environment-Design/#recording-statistics","text":"We offer developers a mechanism to record statistics from within their Unity environments. These statistics are aggregated and generated during the training process. To record statistics, see the StatsRecorder C# class. See the FoodCollector example environment for a sample usage (specifically, FoodCollectorSettings.cs ).","title":"Recording Statistics"},{"location":"Learning-Environment-Examples/","text":"Example Learning Environments The Unity ML-Agents Toolkit includes an expanding set of example environments that highlight the various features of the toolkit. These environments can also serve as templates for new environments or as ways to test new ML algorithms. Environments are located in Project/Assets/ML-Agents/Examples and summarized below. For the environments that highlight specific features of the toolkit, we provide the pre-trained model files and the training config file that enables you to train the scene yourself. The environments that are designed to serve as challenges for researchers do not have accompanying pre-trained model files or training configs and are marked as Optional below. This page only overviews the example environments we provide. To learn more on how to design and build your own environments see our Making a New Learning Environment page. If you would like to contribute environments, please see our contribution guidelines page. Basic Set-up: A linear movement task where the agent must move left or right to rewarding states. Goal: Move to the most reward state. Agents: The environment contains one agent. Agent Reward Function: -0.01 at each step +0.1 for arriving at suboptimal state. +1.0 for arriving at optimal state. Behavior Parameters: Vector Observation space: One variable corresponding to current state. Actions: 1 discrete action branch with 3 actions (Move left, do nothing, move right). Visual Observations: None Float Properties: None Benchmark Mean Reward: 0.93 3DBall: 3D Balance Ball Set-up: A balance-ball task, where the agent balances the ball on it's head. Goal: The agent must balance the ball on it's head for as long as possible. Agents: The environment contains 12 agents of the same kind, all using the same Behavior Parameters. Agent Reward Function: +0.1 for every step the ball remains on it's head. -1.0 if the ball falls off. Behavior Parameters: Vector Observation space: 8 variables corresponding to rotation of the agent cube, and position and velocity of ball. Vector Observation space (Hard Version): 5 variables corresponding to rotation of the agent cube and position of ball. Actions: 2 continuous actions, with one value corresponding to X-rotation, and the other to Z-rotation. Visual Observations: Third-person view from the upper-front of the agent. Use Visual3DBall scene. Float Properties: Three scale: Specifies the scale of the ball in the 3 dimensions (equal across the three dimensions) Default: 1 Recommended Minimum: 0.2 Recommended Maximum: 5 gravity: Magnitude of gravity Default: 9.81 Recommended Minimum: 4 Recommended Maximum: 105 mass: Specifies mass of the ball Default: 1 Recommended Minimum: 0.1 Recommended Maximum: 20 Benchmark Mean Reward: 100 GridWorld Set-up: A multi-goal version of the grid-world task. Scene contains agent, goal, and obstacles. Goal: The agent must navigate the grid to the appropriate goal while avoiding the obstacles. Agents: The environment contains nine agents with the same Behavior Parameters. Agent Reward Function: -0.01 for every step. +1.0 if the agent navigates to the correct goal (episode ends). -1.0 if the agent navigates to an incorrect goal (episode ends). Behavior Parameters: Vector Observation space: None Actions: 1 discrete action branch with 5 actions, corresponding to movement in cardinal directions or not moving. Note that for this environment, action masking is turned on by default (this option can be toggled using the Mask Actions checkbox within the trueAgent GameObject). The trained model file provided was generated with action masking turned on. Visual Observations: One corresponding to top-down view of GridWorld. Goal Signal : A one hot vector corresponding to which color is the correct goal for the Agent Float Properties: Three, corresponding to grid size, number of green goals, and number of red goals. Benchmark Mean Reward: 0.8 Push Block Set-up: A platforming environment where the agent can push a block around. Goal: The agent must push the block to the goal. Agents: The environment contains one agent. Agent Reward Function: -0.0025 for every step. +1.0 if the block touches the goal. Behavior Parameters: Vector Observation space: (Continuous) 70 variables corresponding to 14 ray-casts each detecting one of three possible objects (wall, goal, or block). Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise and counterclockwise, move along four different face directions, or do nothing. Float Properties: Four block_scale: Scale of the block along the x and z dimensions Default: 2 Recommended Minimum: 0.5 Recommended Maximum: 4 dynamic_friction: Coefficient of friction for the ground material acting on moving objects Default: 0 Recommended Minimum: 0 Recommended Maximum: 1 static_friction: Coefficient of friction for the ground material acting on stationary objects Default: 0 Recommended Minimum: 0 Recommended Maximum: 1 block_drag: Effect of air resistance on block Default: 0.5 Recommended Minimum: 0 Recommended Maximum: 2000 Benchmark Mean Reward: 4.5 Wall Jump Set-up: A platforming environment where the agent can jump over a wall. Goal: The agent must use the block to scale the wall and reach the goal. Agents: The environment contains one agent linked to two different Models. The Policy the agent is linked to changes depending on the height of the wall. The change of Policy is done in the WallJumpAgent class. Agent Reward Function: -0.0005 for every step. +1.0 if the agent touches the goal. -1.0 if the agent falls off the platform. Behavior Parameters: Vector Observation space: Size of 74, corresponding to 14 ray casts each detecting 4 possible objects. plus the global position of the agent and whether or not the agent is grounded. Actions: 4 discrete action branches: Forward Motion (3 possible actions: Forward, Backwards, No Action) Rotation (3 possible actions: Rotate Left, Rotate Right, No Action) Side Motion (3 possible actions: Left, Right, No Action) Jump (2 possible actions: Jump, No Action) Visual Observations: None Float Properties: Four Benchmark Mean Reward (Big & Small Wall): 0.8 Crawler Set-up: A creature with 4 arms and 4 forearms. Goal: The agents must move its body toward the goal direction without falling. Agents: The environment contains 10 agents with same Behavior Parameters. Agent Reward Function (independent): The reward function is now geometric meaning the reward each step is a product of all the rewards instead of a sum, this helps the agent try to maximize all rewards instead of the easiest rewards. Body velocity matches goal velocity. (normalized between (0,1)) Head direction alignment with goal direction. (normalized between (0,1)) Behavior Parameters: Vector Observation space: 172 variables corresponding to position, rotation, velocity, and angular velocities of each limb plus the acceleration and angular acceleration of the body. Actions: 20 continuous actions, corresponding to target rotations for joints. Visual Observations: None Float Properties: None Benchmark Mean Reward: 3000 Worm Set-up: A worm with a head and 3 body segments. Goal: The agents must move its body toward the goal direction. Agents: The environment contains 10 agents with same Behavior Parameters. Agent Reward Function (independent): The reward function is now geometric meaning the reward each step is a product of all the rewards instead of a sum, this helps the agent try to maximize all rewards instead of the easiest rewards. Body velocity matches goal velocity. (normalized between (0,1)) Body direction alignment with goal direction. (normalized between (0,1)) Behavior Parameters: Vector Observation space: 64 variables corresponding to position, rotation, velocity, and angular velocities of each limb plus the acceleration and angular acceleration of the body. Actions: 9 continuous actions, corresponding to target rotations for joints. Visual Observations: None Float Properties: None Benchmark Mean Reward: 800 Food Collector Set-up: A multi-agent environment where agents compete to collect food. Goal: The agents must learn to collect as many green food spheres as possible while avoiding red spheres. Agents: The environment contains 5 agents with same Behavior Parameters. Agent Reward Function (independent): +1 for interaction with green spheres -1 for interaction with red spheres Behavior Parameters: Vector Observation space: 53 corresponding to velocity of agent (2), whether agent is frozen and/or shot its laser (2), plus grid based perception of objects around agent's forward direction (40 by 40 with 6 different categories). Actions: 3 continuous actions correspond to Forward Motion, Side Motion and Rotation 1 discrete acion branch for Laser with 2 possible actions corresponding to Shoot Laser or No Action Visual Observations (Optional): First-person camera per-agent, plus one vector flag representing the frozen state of the agent. This scene uses a combination of vector and visual observations and the training will not succeed without the frozen vector flag. Use VisualFoodCollector scene. Float Properties: Two laser_length: Length of the laser used by the agent Default: 1 Recommended Minimum: 0.2 Recommended Maximum: 7 agent_scale: Specifies the scale of the agent in the 3 dimensions (equal across the three dimensions) Default: 1 Recommended Minimum: 0.5 Recommended Maximum: 5 Benchmark Mean Reward: 10 Hallway Set-up: Environment where the agent needs to find information in a room, remember it, and use it to move to the correct goal. Goal: Move to the goal which corresponds to the color of the block in the room. Agents: The environment contains one agent. Agent Reward Function (independent): +1 For moving to correct goal. -0.1 For moving to incorrect goal. -0.0003 Existential penalty. Behavior Parameters: Vector Observation space: 30 corresponding to local ray-casts detecting objects, goals, and walls. Actions: 1 discrete action Branch, with 4 actions corresponding to agent rotation and forward/backward movement. Float Properties: None Benchmark Mean Reward: 0.7 To train this environment, you can enable curiosity by adding the curiosity reward signal in config/ppo/Hallway.yaml Soccer Twos Set-up: Environment where four agents compete in a 2 vs 2 toy soccer game. Goal: Get the ball into the opponent's goal while preventing the ball from entering own goal. Agents: The environment contains two different Multi Agent Groups with two agents in each. Parameters : SoccerTwos. Agent Reward Function (dependent): (1 - accumulated time penalty ) When ball enters opponent's goal accumulated time penalty is incremented by (1 / MaxStep ) every fixed update and is reset to 0 at the beginning of an episode. -1 When ball enters team's goal. Behavior Parameters: Vector Observation space: 336 corresponding to 11 ray-casts forward distributed over 120 degrees and 3 ray-casts backward distributed over 90 degrees each detecting 6 possible object types, along with the object's distance. The forward ray-casts contribute 264 state dimensions and backward 72 state dimensions over three observation stacks. Actions: 3 discrete branched actions corresponding to forward, backward, sideways movement, as well as rotation. Visual Observations: None Float Properties: Two ball_scale: Specifies the scale of the ball in the 3 dimensions (equal across the three dimensions) Default: 7.5 Recommended minimum: 4 Recommended maximum: 10 gravity: Magnitude of the gravity Default: 9.81 Recommended minimum: 6 Recommended maximum: 20 Strikers Vs. Goalie Set-up: Environment where two agents compete in a 2 vs 1 soccer variant. Goal: Striker: Get the ball into the opponent's goal. Goalie: Keep the ball out of the goal. Agents: The environment contains two different Multi Agent Groups. One with two Strikers and the other one Goalie. Behavior Parameters : Striker, Goalie. Striker Agent Reward Function (dependent): +1 When ball enters opponent's goal. -0.001 Existential penalty. Goalie Agent Reward Function (dependent): -1 When ball enters goal. 0.001 Existential bonus. Behavior Parameters: Striker Vector Observation space: 294 corresponding to 11 ray-casts forward distributed over 120 degrees and 3 ray-casts backward distributed over 90 degrees each detecting 5 possible object types, along with the object's distance. The forward ray-casts contribute 231 state dimensions and backward 63 state dimensions over three observation stacks. Striker Actions: 3 discrete branched actions corresponding to forward, backward, sideways movement, as well as rotation. Goalie Vector Observation space: 738 corresponding to 41 ray-casts distributed over 360 degrees each detecting 4 possible object types, along with the object's distance and 3 observation stacks. Goalie Actions: 3 discrete branched actions corresponding to forward, backward, sideways movement, as well as rotation. Visual Observations: None Float Properties: Two ball_scale: Specifies the scale of the ball in the 3 dimensions (equal across the three dimensions) Default: 7.5 Recommended minimum: 4 Recommended maximum: 10 gravity: Magnitude of the gravity Default: 9.81 Recommended minimum: 6 Recommended maximum: 20 Walker Set-up: Physics-based Humanoid agents with 26 degrees of freedom. These DOFs correspond to articulation of the following body-parts: hips, chest, spine, head, thighs, shins, feet, arms, forearms and hands. Goal: The agents must move its body toward the goal direction without falling. Agents: The environment contains 10 independent agents with same Behavior Parameters. Agent Reward Function (independent): The reward function is now geometric meaning the reward each step is a product of all the rewards instead of a sum, this helps the agent try to maximize all rewards instead of the easiest rewards. Body velocity matches goal velocity. (normalized between (0,1)) Head direction alignment with goal direction. (normalized between (0,1)) Behavior Parameters: Vector Observation space: 243 variables corresponding to position, rotation, velocity, and angular velocities of each limb, along with goal direction. Actions: 39 continuous actions, corresponding to target rotations and strength applicable to the joints. Visual Observations: None Float Properties: Four gravity: Magnitude of gravity Default: 9.81 Recommended Minimum: Recommended Maximum: hip_mass: Mass of the hip component of the walker Default: 8 Recommended Minimum: 7 Recommended Maximum: 28 chest_mass: Mass of the chest component of the walker Default: 8 Recommended Minimum: 3 Recommended Maximum: 20 spine_mass: Mass of the spine component of the walker Default: 8 Recommended Minimum: 3 Recommended Maximum: 20 Benchmark Mean Reward : 2500 Pyramids Set-up: Environment where the agent needs to press a button to spawn a pyramid, then navigate to the pyramid, knock it over, and move to the gold brick at the top. Goal: Move to the golden brick on top of the spawned pyramid. Agents: The environment contains one agent. Agent Reward Function (independent): +2 For moving to golden brick (minus 0.001 per step). Behavior Parameters: Vector Observation space: 148 corresponding to local ray-casts detecting switch, bricks, golden brick, and walls, plus variable indicating switch state. Actions: 1 discrete action branch, with 4 actions corresponding to agent rotation and forward/backward movement. Float Properties: None Benchmark Mean Reward: 1.75 Match 3 Set-up: Simple match-3 game. Matched pieces are removed, and remaining pieces drop down. New pieces are spawned randomly at the top, with a chance of being \"special\". Goal: Maximize score from matching pieces. Agents: The environment contains several independent Agents. Agent Reward Function (independent): .01 for each normal piece cleared. Special pieces are worth 2x or 3x. Behavior Parameters: None Observations and actions are defined with a sensor and actuator respectively. Float Properties: None Benchmark Mean Reward: 39.5 for visual observations 38.5 for vector observations 34.2 for simple heuristic (pick a random valid move) 37.0 for greedy heuristic (pick the highest-scoring valid move) Sorter Set-up: The Agent is in a circular room with numbered tiles. The values of the tiles are random between 1 and 20. The tiles present in the room are randomized at each episode. When the Agent visits a tile, it turns green. Goal: Visit all the tiles in ascending order. Agents: The environment contains a single Agent Agent Reward Function: -.0002 Existential penalty. +1 For visiting the right tile -1 For visiting the wrong tile BehaviorParameters: Vector Observations : 4 : 2 floats for Position and 2 floats for orientation Variable Length Observations : Between 1 and 20 entities (one for each tile) each with 22 observations, the first 20 are one hot encoding of the value of the tile, the 21st and 22nd represent the position of the tile relative to the Agent and the 23rd is 1 if the tile was visited and 0 otherwise. Actions: 3 discrete branched actions corresponding to forward, backward, sideways movement, as well as rotation. Float Properties: One num_tiles: The maximum number of tiles to sample. Default: 2 Recommended Minimum: 1 Recommended Maximum: 20 Benchmark Mean Reward: Depends on the number of tiles. Cooperative Push Block Set-up: Similar to Push Block, the agents are in an area with blocks that need to be pushed into a goal. Small blocks can be pushed by one agents and are worth +1 value, medium blocks require two agents to push in and are worth +2, and large blocks require all 3 agents to push and are worth +3. Goal: Push all blocks into the goal. Agents: The environment contains three Agents in a Multi Agent Group. Agent Reward Function: -0.0001 Existential penalty, as a group reward. +1, +2, or +3 for pushing in a block, added as a group reward. Behavior Parameters: Observation space: A single Grid Sensor with separate tags for each block size, the goal, the walls, and other agents. Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise and counterclockwise, move along four different face directions, or do nothing. Float Properties: None Benchmark Mean Reward: 11 (Group Reward) Dungeon Escape Set-up: Agents are trapped in a dungeon with a dragon, and must work together to escape. To retrieve the key, one of the agents must find and slay the dragon, sacrificing itself to do so. The dragon will drop a key for the others to use. The other agents can then pick up this key and unlock the dungeon door. If the agents take too long, the dragon will escape through a portal and the environment resets. Goal: Unlock the dungeon door and leave. Agents: The environment contains three Agents in a Multi Agent Group and one Dragon, which moves in a predetermined pattern. Agent Reward Function: +1 group reward if any agent successfully unlocks the door and leaves the dungeon. Behavior Parameters: Observation space: A Ray Perception Sensor with separate tags for the walls, other agents, the door, key, the dragon, and the dragon's portal. A single Vector Observation which indicates whether the agent is holding a key. Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise and counterclockwise, move along four different face directions, or do nothing. Float Properties: None Benchmark Mean Reward: 1.0 (Group Reward)","title":"Example Learning Environments"},{"location":"Learning-Environment-Examples/#example-learning-environments","text":"The Unity ML-Agents Toolkit includes an expanding set of example environments that highlight the various features of the toolkit. These environments can also serve as templates for new environments or as ways to test new ML algorithms. Environments are located in Project/Assets/ML-Agents/Examples and summarized below. For the environments that highlight specific features of the toolkit, we provide the pre-trained model files and the training config file that enables you to train the scene yourself. The environments that are designed to serve as challenges for researchers do not have accompanying pre-trained model files or training configs and are marked as Optional below. This page only overviews the example environments we provide. To learn more on how to design and build your own environments see our Making a New Learning Environment page. If you would like to contribute environments, please see our contribution guidelines page.","title":"Example Learning Environments"},{"location":"Learning-Environment-Examples/#basic","text":"Set-up: A linear movement task where the agent must move left or right to rewarding states. Goal: Move to the most reward state. Agents: The environment contains one agent. Agent Reward Function: -0.01 at each step +0.1 for arriving at suboptimal state. +1.0 for arriving at optimal state. Behavior Parameters: Vector Observation space: One variable corresponding to current state. Actions: 1 discrete action branch with 3 actions (Move left, do nothing, move right). Visual Observations: None Float Properties: None Benchmark Mean Reward: 0.93","title":"Basic"},{"location":"Learning-Environment-Examples/#3dball-3d-balance-ball","text":"Set-up: A balance-ball task, where the agent balances the ball on it's head. Goal: The agent must balance the ball on it's head for as long as possible. Agents: The environment contains 12 agents of the same kind, all using the same Behavior Parameters. Agent Reward Function: +0.1 for every step the ball remains on it's head. -1.0 if the ball falls off. Behavior Parameters: Vector Observation space: 8 variables corresponding to rotation of the agent cube, and position and velocity of ball. Vector Observation space (Hard Version): 5 variables corresponding to rotation of the agent cube and position of ball. Actions: 2 continuous actions, with one value corresponding to X-rotation, and the other to Z-rotation. Visual Observations: Third-person view from the upper-front of the agent. Use Visual3DBall scene. Float Properties: Three scale: Specifies the scale of the ball in the 3 dimensions (equal across the three dimensions) Default: 1 Recommended Minimum: 0.2 Recommended Maximum: 5 gravity: Magnitude of gravity Default: 9.81 Recommended Minimum: 4 Recommended Maximum: 105 mass: Specifies mass of the ball Default: 1 Recommended Minimum: 0.1 Recommended Maximum: 20 Benchmark Mean Reward: 100","title":"3DBall: 3D Balance Ball"},{"location":"Learning-Environment-Examples/#gridworld","text":"Set-up: A multi-goal version of the grid-world task. Scene contains agent, goal, and obstacles. Goal: The agent must navigate the grid to the appropriate goal while avoiding the obstacles. Agents: The environment contains nine agents with the same Behavior Parameters. Agent Reward Function: -0.01 for every step. +1.0 if the agent navigates to the correct goal (episode ends). -1.0 if the agent navigates to an incorrect goal (episode ends). Behavior Parameters: Vector Observation space: None Actions: 1 discrete action branch with 5 actions, corresponding to movement in cardinal directions or not moving. Note that for this environment, action masking is turned on by default (this option can be toggled using the Mask Actions checkbox within the trueAgent GameObject). The trained model file provided was generated with action masking turned on. Visual Observations: One corresponding to top-down view of GridWorld. Goal Signal : A one hot vector corresponding to which color is the correct goal for the Agent Float Properties: Three, corresponding to grid size, number of green goals, and number of red goals. Benchmark Mean Reward: 0.8","title":"GridWorld"},{"location":"Learning-Environment-Examples/#push-block","text":"Set-up: A platforming environment where the agent can push a block around. Goal: The agent must push the block to the goal. Agents: The environment contains one agent. Agent Reward Function: -0.0025 for every step. +1.0 if the block touches the goal. Behavior Parameters: Vector Observation space: (Continuous) 70 variables corresponding to 14 ray-casts each detecting one of three possible objects (wall, goal, or block). Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise and counterclockwise, move along four different face directions, or do nothing. Float Properties: Four block_scale: Scale of the block along the x and z dimensions Default: 2 Recommended Minimum: 0.5 Recommended Maximum: 4 dynamic_friction: Coefficient of friction for the ground material acting on moving objects Default: 0 Recommended Minimum: 0 Recommended Maximum: 1 static_friction: Coefficient of friction for the ground material acting on stationary objects Default: 0 Recommended Minimum: 0 Recommended Maximum: 1 block_drag: Effect of air resistance on block Default: 0.5 Recommended Minimum: 0 Recommended Maximum: 2000 Benchmark Mean Reward: 4.5","title":"Push Block"},{"location":"Learning-Environment-Examples/#wall-jump","text":"Set-up: A platforming environment where the agent can jump over a wall. Goal: The agent must use the block to scale the wall and reach the goal. Agents: The environment contains one agent linked to two different Models. The Policy the agent is linked to changes depending on the height of the wall. The change of Policy is done in the WallJumpAgent class. Agent Reward Function: -0.0005 for every step. +1.0 if the agent touches the goal. -1.0 if the agent falls off the platform. Behavior Parameters: Vector Observation space: Size of 74, corresponding to 14 ray casts each detecting 4 possible objects. plus the global position of the agent and whether or not the agent is grounded. Actions: 4 discrete action branches: Forward Motion (3 possible actions: Forward, Backwards, No Action) Rotation (3 possible actions: Rotate Left, Rotate Right, No Action) Side Motion (3 possible actions: Left, Right, No Action) Jump (2 possible actions: Jump, No Action) Visual Observations: None Float Properties: Four Benchmark Mean Reward (Big & Small Wall): 0.8","title":"Wall Jump"},{"location":"Learning-Environment-Examples/#crawler","text":"Set-up: A creature with 4 arms and 4 forearms. Goal: The agents must move its body toward the goal direction without falling. Agents: The environment contains 10 agents with same Behavior Parameters. Agent Reward Function (independent): The reward function is now geometric meaning the reward each step is a product of all the rewards instead of a sum, this helps the agent try to maximize all rewards instead of the easiest rewards. Body velocity matches goal velocity. (normalized between (0,1)) Head direction alignment with goal direction. (normalized between (0,1)) Behavior Parameters: Vector Observation space: 172 variables corresponding to position, rotation, velocity, and angular velocities of each limb plus the acceleration and angular acceleration of the body. Actions: 20 continuous actions, corresponding to target rotations for joints. Visual Observations: None Float Properties: None Benchmark Mean Reward: 3000","title":"Crawler"},{"location":"Learning-Environment-Examples/#worm","text":"Set-up: A worm with a head and 3 body segments. Goal: The agents must move its body toward the goal direction. Agents: The environment contains 10 agents with same Behavior Parameters. Agent Reward Function (independent): The reward function is now geometric meaning the reward each step is a product of all the rewards instead of a sum, this helps the agent try to maximize all rewards instead of the easiest rewards. Body velocity matches goal velocity. (normalized between (0,1)) Body direction alignment with goal direction. (normalized between (0,1)) Behavior Parameters: Vector Observation space: 64 variables corresponding to position, rotation, velocity, and angular velocities of each limb plus the acceleration and angular acceleration of the body. Actions: 9 continuous actions, corresponding to target rotations for joints. Visual Observations: None Float Properties: None Benchmark Mean Reward: 800","title":"Worm"},{"location":"Learning-Environment-Examples/#food-collector","text":"Set-up: A multi-agent environment where agents compete to collect food. Goal: The agents must learn to collect as many green food spheres as possible while avoiding red spheres. Agents: The environment contains 5 agents with same Behavior Parameters. Agent Reward Function (independent): +1 for interaction with green spheres -1 for interaction with red spheres Behavior Parameters: Vector Observation space: 53 corresponding to velocity of agent (2), whether agent is frozen and/or shot its laser (2), plus grid based perception of objects around agent's forward direction (40 by 40 with 6 different categories). Actions: 3 continuous actions correspond to Forward Motion, Side Motion and Rotation 1 discrete acion branch for Laser with 2 possible actions corresponding to Shoot Laser or No Action Visual Observations (Optional): First-person camera per-agent, plus one vector flag representing the frozen state of the agent. This scene uses a combination of vector and visual observations and the training will not succeed without the frozen vector flag. Use VisualFoodCollector scene. Float Properties: Two laser_length: Length of the laser used by the agent Default: 1 Recommended Minimum: 0.2 Recommended Maximum: 7 agent_scale: Specifies the scale of the agent in the 3 dimensions (equal across the three dimensions) Default: 1 Recommended Minimum: 0.5 Recommended Maximum: 5 Benchmark Mean Reward: 10","title":"Food Collector"},{"location":"Learning-Environment-Examples/#hallway","text":"Set-up: Environment where the agent needs to find information in a room, remember it, and use it to move to the correct goal. Goal: Move to the goal which corresponds to the color of the block in the room. Agents: The environment contains one agent. Agent Reward Function (independent): +1 For moving to correct goal. -0.1 For moving to incorrect goal. -0.0003 Existential penalty. Behavior Parameters: Vector Observation space: 30 corresponding to local ray-casts detecting objects, goals, and walls. Actions: 1 discrete action Branch, with 4 actions corresponding to agent rotation and forward/backward movement. Float Properties: None Benchmark Mean Reward: 0.7 To train this environment, you can enable curiosity by adding the curiosity reward signal in config/ppo/Hallway.yaml","title":"Hallway"},{"location":"Learning-Environment-Examples/#soccer-twos","text":"Set-up: Environment where four agents compete in a 2 vs 2 toy soccer game. Goal: Get the ball into the opponent's goal while preventing the ball from entering own goal. Agents: The environment contains two different Multi Agent Groups with two agents in each. Parameters : SoccerTwos. Agent Reward Function (dependent): (1 - accumulated time penalty ) When ball enters opponent's goal accumulated time penalty is incremented by (1 / MaxStep ) every fixed update and is reset to 0 at the beginning of an episode. -1 When ball enters team's goal. Behavior Parameters: Vector Observation space: 336 corresponding to 11 ray-casts forward distributed over 120 degrees and 3 ray-casts backward distributed over 90 degrees each detecting 6 possible object types, along with the object's distance. The forward ray-casts contribute 264 state dimensions and backward 72 state dimensions over three observation stacks. Actions: 3 discrete branched actions corresponding to forward, backward, sideways movement, as well as rotation. Visual Observations: None Float Properties: Two ball_scale: Specifies the scale of the ball in the 3 dimensions (equal across the three dimensions) Default: 7.5 Recommended minimum: 4 Recommended maximum: 10 gravity: Magnitude of the gravity Default: 9.81 Recommended minimum: 6 Recommended maximum: 20","title":"Soccer Twos"},{"location":"Learning-Environment-Examples/#strikers-vs-goalie","text":"Set-up: Environment where two agents compete in a 2 vs 1 soccer variant. Goal: Striker: Get the ball into the opponent's goal. Goalie: Keep the ball out of the goal. Agents: The environment contains two different Multi Agent Groups. One with two Strikers and the other one Goalie. Behavior Parameters : Striker, Goalie. Striker Agent Reward Function (dependent): +1 When ball enters opponent's goal. -0.001 Existential penalty. Goalie Agent Reward Function (dependent): -1 When ball enters goal. 0.001 Existential bonus. Behavior Parameters: Striker Vector Observation space: 294 corresponding to 11 ray-casts forward distributed over 120 degrees and 3 ray-casts backward distributed over 90 degrees each detecting 5 possible object types, along with the object's distance. The forward ray-casts contribute 231 state dimensions and backward 63 state dimensions over three observation stacks. Striker Actions: 3 discrete branched actions corresponding to forward, backward, sideways movement, as well as rotation. Goalie Vector Observation space: 738 corresponding to 41 ray-casts distributed over 360 degrees each detecting 4 possible object types, along with the object's distance and 3 observation stacks. Goalie Actions: 3 discrete branched actions corresponding to forward, backward, sideways movement, as well as rotation. Visual Observations: None Float Properties: Two ball_scale: Specifies the scale of the ball in the 3 dimensions (equal across the three dimensions) Default: 7.5 Recommended minimum: 4 Recommended maximum: 10 gravity: Magnitude of the gravity Default: 9.81 Recommended minimum: 6 Recommended maximum: 20","title":"Strikers Vs. Goalie"},{"location":"Learning-Environment-Examples/#walker","text":"Set-up: Physics-based Humanoid agents with 26 degrees of freedom. These DOFs correspond to articulation of the following body-parts: hips, chest, spine, head, thighs, shins, feet, arms, forearms and hands. Goal: The agents must move its body toward the goal direction without falling. Agents: The environment contains 10 independent agents with same Behavior Parameters. Agent Reward Function (independent): The reward function is now geometric meaning the reward each step is a product of all the rewards instead of a sum, this helps the agent try to maximize all rewards instead of the easiest rewards. Body velocity matches goal velocity. (normalized between (0,1)) Head direction alignment with goal direction. (normalized between (0,1)) Behavior Parameters: Vector Observation space: 243 variables corresponding to position, rotation, velocity, and angular velocities of each limb, along with goal direction. Actions: 39 continuous actions, corresponding to target rotations and strength applicable to the joints. Visual Observations: None Float Properties: Four gravity: Magnitude of gravity Default: 9.81 Recommended Minimum: Recommended Maximum: hip_mass: Mass of the hip component of the walker Default: 8 Recommended Minimum: 7 Recommended Maximum: 28 chest_mass: Mass of the chest component of the walker Default: 8 Recommended Minimum: 3 Recommended Maximum: 20 spine_mass: Mass of the spine component of the walker Default: 8 Recommended Minimum: 3 Recommended Maximum: 20 Benchmark Mean Reward : 2500","title":"Walker"},{"location":"Learning-Environment-Examples/#pyramids","text":"Set-up: Environment where the agent needs to press a button to spawn a pyramid, then navigate to the pyramid, knock it over, and move to the gold brick at the top. Goal: Move to the golden brick on top of the spawned pyramid. Agents: The environment contains one agent. Agent Reward Function (independent): +2 For moving to golden brick (minus 0.001 per step). Behavior Parameters: Vector Observation space: 148 corresponding to local ray-casts detecting switch, bricks, golden brick, and walls, plus variable indicating switch state. Actions: 1 discrete action branch, with 4 actions corresponding to agent rotation and forward/backward movement. Float Properties: None Benchmark Mean Reward: 1.75","title":"Pyramids"},{"location":"Learning-Environment-Examples/#match-3","text":"Set-up: Simple match-3 game. Matched pieces are removed, and remaining pieces drop down. New pieces are spawned randomly at the top, with a chance of being \"special\". Goal: Maximize score from matching pieces. Agents: The environment contains several independent Agents. Agent Reward Function (independent): .01 for each normal piece cleared. Special pieces are worth 2x or 3x. Behavior Parameters: None Observations and actions are defined with a sensor and actuator respectively. Float Properties: None Benchmark Mean Reward: 39.5 for visual observations 38.5 for vector observations 34.2 for simple heuristic (pick a random valid move) 37.0 for greedy heuristic (pick the highest-scoring valid move)","title":"Match 3"},{"location":"Learning-Environment-Examples/#sorter","text":"Set-up: The Agent is in a circular room with numbered tiles. The values of the tiles are random between 1 and 20. The tiles present in the room are randomized at each episode. When the Agent visits a tile, it turns green. Goal: Visit all the tiles in ascending order. Agents: The environment contains a single Agent Agent Reward Function: -.0002 Existential penalty. +1 For visiting the right tile -1 For visiting the wrong tile BehaviorParameters: Vector Observations : 4 : 2 floats for Position and 2 floats for orientation Variable Length Observations : Between 1 and 20 entities (one for each tile) each with 22 observations, the first 20 are one hot encoding of the value of the tile, the 21st and 22nd represent the position of the tile relative to the Agent and the 23rd is 1 if the tile was visited and 0 otherwise. Actions: 3 discrete branched actions corresponding to forward, backward, sideways movement, as well as rotation. Float Properties: One num_tiles: The maximum number of tiles to sample. Default: 2 Recommended Minimum: 1 Recommended Maximum: 20 Benchmark Mean Reward: Depends on the number of tiles.","title":"Sorter"},{"location":"Learning-Environment-Examples/#cooperative-push-block","text":"Set-up: Similar to Push Block, the agents are in an area with blocks that need to be pushed into a goal. Small blocks can be pushed by one agents and are worth +1 value, medium blocks require two agents to push in and are worth +2, and large blocks require all 3 agents to push and are worth +3. Goal: Push all blocks into the goal. Agents: The environment contains three Agents in a Multi Agent Group. Agent Reward Function: -0.0001 Existential penalty, as a group reward. +1, +2, or +3 for pushing in a block, added as a group reward. Behavior Parameters: Observation space: A single Grid Sensor with separate tags for each block size, the goal, the walls, and other agents. Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise and counterclockwise, move along four different face directions, or do nothing. Float Properties: None Benchmark Mean Reward: 11 (Group Reward)","title":"Cooperative Push Block"},{"location":"Learning-Environment-Examples/#dungeon-escape","text":"Set-up: Agents are trapped in a dungeon with a dragon, and must work together to escape. To retrieve the key, one of the agents must find and slay the dragon, sacrificing itself to do so. The dragon will drop a key for the others to use. The other agents can then pick up this key and unlock the dungeon door. If the agents take too long, the dragon will escape through a portal and the environment resets. Goal: Unlock the dungeon door and leave. Agents: The environment contains three Agents in a Multi Agent Group and one Dragon, which moves in a predetermined pattern. Agent Reward Function: +1 group reward if any agent successfully unlocks the door and leaves the dungeon. Behavior Parameters: Observation space: A Ray Perception Sensor with separate tags for the walls, other agents, the door, key, the dragon, and the dragon's portal. A single Vector Observation which indicates whether the agent is holding a key. Actions: 1 discrete action branch with 7 actions, corresponding to turn clockwise and counterclockwise, move along four different face directions, or do nothing. Float Properties: None Benchmark Mean Reward: 1.0 (Group Reward)","title":"Dungeon Escape"},{"location":"Learning-Environment-Executable/","text":"Using an Environment Executable This section will help you create and use built environments rather than the Editor to interact with an environment. Using an executable has some advantages over using the Editor: You can exchange executable with other people without having to share your entire repository. You can put your executable on a remote machine for faster training. You can use Server Build ( Headless ) mode for faster training (as long as the executable does not need rendering). You can keep using the Unity Editor for other tasks while the agents are training. Building the 3DBall environment The first step is to open the Unity scene containing the 3D Balance Ball environment: Launch Unity. On the Projects dialog, choose the Open option at the top of the window. Using the file dialog that opens, locate the Project folder within the ML-Agents project and click Open . In the Project window, navigate to the folder Assets/ML-Agents/Examples/3DBall/Scenes/ . Double-click the 3DBall file to load the scene containing the Balance Ball environment. Next, we want the set up scene to play correctly when the training process launches our environment executable. This means: The environment application runs in the background. No dialogs require interaction. The correct scene loads automatically. Open Player Settings (menu: Edit > Project Settings > Player ). Under Resolution and Presentation : Ensure that Run in Background is Checked. Ensure that Display Resolution Dialog is set to Disabled. (Note: this setting may not be available in newer versions of the editor.) Open the Build Settings window (menu: File > Build Settings ). Choose your target platform. (optional) Select \u201cDevelopment Build\u201d to log debug messages . If any scenes are shown in the Scenes in Build list, make sure that the 3DBall Scene is the only one checked. (If the list is empty, then only the current scene is included in the build). Click Build : In the File dialog, navigate to your ML-Agents directory. Assign a file name and click Save . (For Windows\uff09With Unity 2018.1, it will ask you to select a folder instead of a file name. Create a subfolder within the root directory and select that folder to build. In the following steps you will refer to this subfolder's name as env_name . You cannot create builds in the Assets folder Now that we have a Unity executable containing the simulation environment, we can interact with it. Interacting with the Environment If you want to use the Python API to interact with your executable, you can pass the name of the executable with the argument 'file_name' of the UnityEnvironment . For instance: from mlagents_envs.environment import UnityEnvironment env = UnityEnvironment(file_name=<env_name>) Training the Environment Open a command or terminal window. Navigate to the folder where you installed the ML-Agents Toolkit. If you followed the default installation , then navigate to the ml-agents/ folder. Run mlagents-learn <trainer-config-file> --env=<env_name> --run-id=<run-identifier> Where: <trainer-config-file> is the file path of the trainer configuration yaml <env_name> is the name and path to the executable you exported from Unity (without extension) <run-identifier> is a string used to separate the results of different training runs For example, if you are training with a 3DBall executable, and you saved it to the directory where you installed the ML-Agents Toolkit, run: mlagents-learn config/ppo/3DBall.yaml --env=3DBall --run-id=firstRun And you should see something like ml-agents$ mlagents-learn config/ppo/3DBall.yaml --env=3DBall --run-id=first-run \u2584\u2584\u2584\u2593\u2593\u2593\u2593 \u2553\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2593\u2593\u2593\u2593\u2593 ,\u2584\u2584\u2584m\u2580\u2580\u2580' ,\u2593\u2593\u2593\u2580\u2593\u2593\u2584 \u2593\u2593\u2593 \u2593\u2593\u258c \u2584\u2593\u2593\u2593\u2580' \u2584\u2593\u2593\u2580 \u2593\u2593\u2593 \u2584\u2584 \u2584\u2584 ,\u2584\u2584 \u2584\u2584\u2584\u2584 ,\u2584\u2584 \u2584\u2593\u2593\u258c\u2584 \u2584\u2584\u2584 ,\u2584\u2584 \u2584\u2593\u2593\u2593\u2580 \u2584\u2593\u2593\u2580 \u2590\u2593\u2593\u258c \u2593\u2593\u258c \u2590\u2593\u2593 \u2590\u2593\u2593\u2593\u2580\u2580\u2580\u2593\u2593\u258c \u2593\u2593\u2593 \u2580\u2593\u2593\u258c\u2580 ^\u2593\u2593\u258c \u2552\u2593\u2593\u258c \u2584\u2593\u2593\u2593\u2593\u2593\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2593\u2593\u2593 \u2593\u2580 \u2593\u2593\u258c \u2590\u2593\u2593 \u2590\u2593\u2593 \u2593\u2593\u2593 \u2593\u2593\u2593 \u2593\u2593\u258c \u2590\u2593\u2593\u2584 \u2593\u2593\u258c \u2580\u2593\u2593\u2593\u2593\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2593\u2593\u2584 \u2593\u2593 \u2593\u2593\u258c \u2590\u2593\u2593 \u2590\u2593\u2593 \u2593\u2593\u2593 \u2593\u2593\u2593 \u2593\u2593\u258c \u2590\u2593\u2593\u2590\u2593\u2593 ^\u2588\u2593\u2593\u2593 \u2580\u2593\u2593\u2584 \u2590\u2593\u2593\u258c \u2593\u2593\u2593\u2593\u2584\u2593\u2593\u2593\u2593 \u2590\u2593\u2593 \u2593\u2593\u2593 \u2593\u2593\u2593 \u2593\u2593\u2593\u2584 \u2593\u2593\u2593\u2593` '\u2580\u2593\u2593\u2593\u2584 ^\u2593\u2593\u2593 \u2593\u2593\u2593 \u2514\u2580\u2580\u2580\u2580 \u2580\u2580 ^\u2580\u2580 `\u2580\u2580 `\u2580\u2580 '\u2580\u2580 \u2590\u2593\u2593\u258c \u2580\u2580\u2580\u2580\u2593\u2584\u2584\u2584 \u2593\u2593\u2593\u2593\u2593\u2593, \u2593\u2593\u2593\u2593\u2580 `\u2580\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u258c \u00ac`\u2580\u2580\u2580\u2588\u2593 Note : If you're using Anaconda, don't forget to activate the ml-agents environment first. If mlagents-learn runs correctly and starts training, you should see something like this: CrashReporter: initialized Mono path[0] = '/Users/dericp/workspace/ml-agents/3DBall.app/Contents/Resources/Data/Managed' Mono config path = '/Users/dericp/workspace/ml-agents/3DBall.app/Contents/MonoBleedingEdge/etc' INFO:mlagents_envs: 'Ball3DAcademy' started successfully! Unity Academy name: Ball3DAcademy INFO:mlagents_envs:Connected new brain: Unity brain name: Ball3DLearning Number of Visual Observations (per agent): 0 Vector Observation space size (per agent): 8 Number of stacked Vector Observation: 1 INFO:mlagents_envs:Hyperparameters for the PPO Trainer of brain Ball3DLearning: batch_size: 64 beta: 0.001 buffer_size: 12000 epsilon: 0.2 gamma: 0.995 hidden_units: 128 lambd: 0.99 learning_rate: 0.0003 max_steps: 5.0e4 normalize: True num_epoch: 3 num_layers: 2 time_horizon: 1000 sequence_length: 64 summary_freq: 1000 use_recurrent: False memory_size: 256 use_curiosity: False curiosity_strength: 0.01 curiosity_enc_size: 128 output_path: ./results/first-run-0/Ball3DLearning INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 1000. Mean Reward: 1.242. Std of Reward: 0.746. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 2000. Mean Reward: 1.319. Std of Reward: 0.693. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 3000. Mean Reward: 1.804. Std of Reward: 1.056. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 4000. Mean Reward: 2.151. Std of Reward: 1.432. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 5000. Mean Reward: 3.175. Std of Reward: 2.250. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 6000. Mean Reward: 4.898. Std of Reward: 4.019. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 7000. Mean Reward: 6.716. Std of Reward: 5.125. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 8000. Mean Reward: 12.124. Std of Reward: 11.929. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 9000. Mean Reward: 18.151. Std of Reward: 16.871. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 10000. Mean Reward: 27.284. Std of Reward: 28.667. Training. You can press Ctrl+C to stop the training, and your trained model will be at results/<run-identifier>/<behavior_name>.onnx , which corresponds to your model's latest checkpoint. ( Note: There is a known bug on Windows that causes the saving of the model to fail when you early terminate the training, it's recommended to wait until Step has reached the max_steps parameter you set in your config YAML.) You can now embed this trained model into your Agent by following the steps below: Move your model file into Project/Assets/ML-Agents/Examples/3DBall/TFModels/ . Open the Unity Editor, and select the 3DBall scene as described above. Select the 3DBall prefab from the Project window and select Agent . Drag the <behavior_name>.onnx file from the Project window of the Editor to the Model placeholder in the Ball3DAgent inspector window. Press the Play button at the top of the Editor. Training on Headless Server To run training on headless server with no graphics rendering support, you need to turn off graphics display in the Unity executable. There are two ways to achieve this: 1. Pass --no-graphics option to mlagents-learn training command. This is equivalent to adding -nographics -batchmode to the Unity executable's commandline. 2. Build your Unity executable with Server Build . You can find this setting in Build Settings in the Unity Editor. If you want to train with graphics (for example, using camera and visual observations), you'll need to set up display rendering support (e.g. xvfb) on you server machine. In our Colab Notebook Tutorials , the Setup section has examples of setting up xvfb on servers.","title":"Using an Environment Executable"},{"location":"Learning-Environment-Executable/#using-an-environment-executable","text":"This section will help you create and use built environments rather than the Editor to interact with an environment. Using an executable has some advantages over using the Editor: You can exchange executable with other people without having to share your entire repository. You can put your executable on a remote machine for faster training. You can use Server Build ( Headless ) mode for faster training (as long as the executable does not need rendering). You can keep using the Unity Editor for other tasks while the agents are training.","title":"Using an Environment Executable"},{"location":"Learning-Environment-Executable/#building-the-3dball-environment","text":"The first step is to open the Unity scene containing the 3D Balance Ball environment: Launch Unity. On the Projects dialog, choose the Open option at the top of the window. Using the file dialog that opens, locate the Project folder within the ML-Agents project and click Open . In the Project window, navigate to the folder Assets/ML-Agents/Examples/3DBall/Scenes/ . Double-click the 3DBall file to load the scene containing the Balance Ball environment. Next, we want the set up scene to play correctly when the training process launches our environment executable. This means: The environment application runs in the background. No dialogs require interaction. The correct scene loads automatically. Open Player Settings (menu: Edit > Project Settings > Player ). Under Resolution and Presentation : Ensure that Run in Background is Checked. Ensure that Display Resolution Dialog is set to Disabled. (Note: this setting may not be available in newer versions of the editor.) Open the Build Settings window (menu: File > Build Settings ). Choose your target platform. (optional) Select \u201cDevelopment Build\u201d to log debug messages . If any scenes are shown in the Scenes in Build list, make sure that the 3DBall Scene is the only one checked. (If the list is empty, then only the current scene is included in the build). Click Build : In the File dialog, navigate to your ML-Agents directory. Assign a file name and click Save . (For Windows\uff09With Unity 2018.1, it will ask you to select a folder instead of a file name. Create a subfolder within the root directory and select that folder to build. In the following steps you will refer to this subfolder's name as env_name . You cannot create builds in the Assets folder Now that we have a Unity executable containing the simulation environment, we can interact with it.","title":"Building the 3DBall environment"},{"location":"Learning-Environment-Executable/#interacting-with-the-environment","text":"If you want to use the Python API to interact with your executable, you can pass the name of the executable with the argument 'file_name' of the UnityEnvironment . For instance: from mlagents_envs.environment import UnityEnvironment env = UnityEnvironment(file_name=<env_name>)","title":"Interacting with the Environment"},{"location":"Learning-Environment-Executable/#training-the-environment","text":"Open a command or terminal window. Navigate to the folder where you installed the ML-Agents Toolkit. If you followed the default installation , then navigate to the ml-agents/ folder. Run mlagents-learn <trainer-config-file> --env=<env_name> --run-id=<run-identifier> Where: <trainer-config-file> is the file path of the trainer configuration yaml <env_name> is the name and path to the executable you exported from Unity (without extension) <run-identifier> is a string used to separate the results of different training runs For example, if you are training with a 3DBall executable, and you saved it to the directory where you installed the ML-Agents Toolkit, run: mlagents-learn config/ppo/3DBall.yaml --env=3DBall --run-id=firstRun And you should see something like ml-agents$ mlagents-learn config/ppo/3DBall.yaml --env=3DBall --run-id=first-run \u2584\u2584\u2584\u2593\u2593\u2593\u2593 \u2553\u2593\u2593\u2593\u2593\u2593\u2593\u2588\u2593\u2593\u2593\u2593\u2593 ,\u2584\u2584\u2584m\u2580\u2580\u2580' ,\u2593\u2593\u2593\u2580\u2593\u2593\u2584 \u2593\u2593\u2593 \u2593\u2593\u258c \u2584\u2593\u2593\u2593\u2580' \u2584\u2593\u2593\u2580 \u2593\u2593\u2593 \u2584\u2584 \u2584\u2584 ,\u2584\u2584 \u2584\u2584\u2584\u2584 ,\u2584\u2584 \u2584\u2593\u2593\u258c\u2584 \u2584\u2584\u2584 ,\u2584\u2584 \u2584\u2593\u2593\u2593\u2580 \u2584\u2593\u2593\u2580 \u2590\u2593\u2593\u258c \u2593\u2593\u258c \u2590\u2593\u2593 \u2590\u2593\u2593\u2593\u2580\u2580\u2580\u2593\u2593\u258c \u2593\u2593\u2593 \u2580\u2593\u2593\u258c\u2580 ^\u2593\u2593\u258c \u2552\u2593\u2593\u258c \u2584\u2593\u2593\u2593\u2593\u2593\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2593\u2593\u2593 \u2593\u2580 \u2593\u2593\u258c \u2590\u2593\u2593 \u2590\u2593\u2593 \u2593\u2593\u2593 \u2593\u2593\u2593 \u2593\u2593\u258c \u2590\u2593\u2593\u2584 \u2593\u2593\u258c \u2580\u2593\u2593\u2593\u2593\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2593\u2593\u2584 \u2593\u2593 \u2593\u2593\u258c \u2590\u2593\u2593 \u2590\u2593\u2593 \u2593\u2593\u2593 \u2593\u2593\u2593 \u2593\u2593\u258c \u2590\u2593\u2593\u2590\u2593\u2593 ^\u2588\u2593\u2593\u2593 \u2580\u2593\u2593\u2584 \u2590\u2593\u2593\u258c \u2593\u2593\u2593\u2593\u2584\u2593\u2593\u2593\u2593 \u2590\u2593\u2593 \u2593\u2593\u2593 \u2593\u2593\u2593 \u2593\u2593\u2593\u2584 \u2593\u2593\u2593\u2593` '\u2580\u2593\u2593\u2593\u2584 ^\u2593\u2593\u2593 \u2593\u2593\u2593 \u2514\u2580\u2580\u2580\u2580 \u2580\u2580 ^\u2580\u2580 `\u2580\u2580 `\u2580\u2580 '\u2580\u2580 \u2590\u2593\u2593\u258c \u2580\u2580\u2580\u2580\u2593\u2584\u2584\u2584 \u2593\u2593\u2593\u2593\u2593\u2593, \u2593\u2593\u2593\u2593\u2580 `\u2580\u2588\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u258c \u00ac`\u2580\u2580\u2580\u2588\u2593 Note : If you're using Anaconda, don't forget to activate the ml-agents environment first. If mlagents-learn runs correctly and starts training, you should see something like this: CrashReporter: initialized Mono path[0] = '/Users/dericp/workspace/ml-agents/3DBall.app/Contents/Resources/Data/Managed' Mono config path = '/Users/dericp/workspace/ml-agents/3DBall.app/Contents/MonoBleedingEdge/etc' INFO:mlagents_envs: 'Ball3DAcademy' started successfully! Unity Academy name: Ball3DAcademy INFO:mlagents_envs:Connected new brain: Unity brain name: Ball3DLearning Number of Visual Observations (per agent): 0 Vector Observation space size (per agent): 8 Number of stacked Vector Observation: 1 INFO:mlagents_envs:Hyperparameters for the PPO Trainer of brain Ball3DLearning: batch_size: 64 beta: 0.001 buffer_size: 12000 epsilon: 0.2 gamma: 0.995 hidden_units: 128 lambd: 0.99 learning_rate: 0.0003 max_steps: 5.0e4 normalize: True num_epoch: 3 num_layers: 2 time_horizon: 1000 sequence_length: 64 summary_freq: 1000 use_recurrent: False memory_size: 256 use_curiosity: False curiosity_strength: 0.01 curiosity_enc_size: 128 output_path: ./results/first-run-0/Ball3DLearning INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 1000. Mean Reward: 1.242. Std of Reward: 0.746. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 2000. Mean Reward: 1.319. Std of Reward: 0.693. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 3000. Mean Reward: 1.804. Std of Reward: 1.056. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 4000. Mean Reward: 2.151. Std of Reward: 1.432. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 5000. Mean Reward: 3.175. Std of Reward: 2.250. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 6000. Mean Reward: 4.898. Std of Reward: 4.019. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 7000. Mean Reward: 6.716. Std of Reward: 5.125. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 8000. Mean Reward: 12.124. Std of Reward: 11.929. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 9000. Mean Reward: 18.151. Std of Reward: 16.871. Training. INFO:mlagents.trainers: first-run-0: Ball3DLearning: Step: 10000. Mean Reward: 27.284. Std of Reward: 28.667. Training. You can press Ctrl+C to stop the training, and your trained model will be at results/<run-identifier>/<behavior_name>.onnx , which corresponds to your model's latest checkpoint. ( Note: There is a known bug on Windows that causes the saving of the model to fail when you early terminate the training, it's recommended to wait until Step has reached the max_steps parameter you set in your config YAML.) You can now embed this trained model into your Agent by following the steps below: Move your model file into Project/Assets/ML-Agents/Examples/3DBall/TFModels/ . Open the Unity Editor, and select the 3DBall scene as described above. Select the 3DBall prefab from the Project window and select Agent . Drag the <behavior_name>.onnx file from the Project window of the Editor to the Model placeholder in the Ball3DAgent inspector window. Press the Play button at the top of the Editor.","title":"Training the Environment"},{"location":"Learning-Environment-Executable/#training-on-headless-server","text":"To run training on headless server with no graphics rendering support, you need to turn off graphics display in the Unity executable. There are two ways to achieve this: 1. Pass --no-graphics option to mlagents-learn training command. This is equivalent to adding -nographics -batchmode to the Unity executable's commandline. 2. Build your Unity executable with Server Build . You can find this setting in Build Settings in the Unity Editor. If you want to train with graphics (for example, using camera and visual observations), you'll need to set up display rendering support (e.g. xvfb) on you server machine. In our Colab Notebook Tutorials , the Setup section has examples of setting up xvfb on servers.","title":"Training on Headless Server"},{"location":"Limitations/","text":"Limitations See the package-specific Limitations pages: com.unity.mlagents Unity package mlagents Python package mlagents_envs Python package","title":"Limitations"},{"location":"Limitations/#limitations","text":"See the package-specific Limitations pages: com.unity.mlagents Unity package mlagents Python package mlagents_envs Python package","title":"Limitations"},{"location":"ML-Agents-Overview/","text":"ML-Agents Toolkit Overview Table of Contents Running Example: Training NPC Behaviors Key Components Training Modes Built-in Training and Inference Cross-Platform Inference Custom Training and Inference Flexible Training Scenarios Training Methods: Environment-agnostic A Quick Note on Reward Signals Deep Reinforcement Learning Curiosity for Sparse-reward Environments RND for Sparse-reward Environments Imitation Learning GAIL (Generative Adversarial Imitation Learning) Behavioral Cloning (BC) Recording Demonstrations Summary Training Methods: Environment-specific Training in Competitive Multi-Agent Environments with Self-Play Training in Cooperative Multi-Agent Environments with MA-POCA Solving Complex Tasks using Curriculum Learning Training Robust Agents using Environment Parameter Randomization Model Types Learning from Vector Observations Learning from Cameras using Convolutional Neural Networks Learning from Variable Length Observations using Attention Memory-enhanced Agents using Recurrent Neural Networks Additional Features Summary and Next Steps The Unity Machine Learning Agents Toolkit (ML-Agents Toolkit) is an open-source project that enables games and simulations to serve as environments for training intelligent agents. Agents can be trained using reinforcement learning, imitation learning, neuroevolution, or other machine learning methods through a simple-to-use Python API. We also provide implementations (based on PyTorch) of state-of-the-art algorithms to enable game developers and hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games. These trained agents can be used for multiple purposes, including controlling NPC behavior (in a variety of settings such as multi-agent and adversarial), automated testing of game builds and evaluating different game design decisions pre-release. The ML-Agents Toolkit is mutually beneficial for both game developers and AI researchers as it provides a central platform where advances in AI can be evaluated on Unity\u2019s rich environments and then made accessible to the wider research and game developer communities. Depending on your background (i.e. researcher, game developer, hobbyist), you may have very different questions on your mind at the moment. To make your transition to the ML-Agents Toolkit easier, we provide several background pages that include overviews and helpful resources on the Unity Engine , machine learning and PyTorch . We strongly recommend browsing the relevant background pages if you're not familiar with a Unity scene, basic machine learning concepts or have not previously heard of PyTorch. The remainder of this page contains a deep dive into ML-Agents, its key components, different training modes and scenarios. By the end of it, you should have a good sense of what the ML-Agents Toolkit allows you to do. The subsequent documentation pages provide examples of how to use ML-Agents. To get started, watch this demo video of ML-Agents in action . Running Example: Training NPC Behaviors To help explain the material and terminology in this page, we'll use a hypothetical, running example throughout. We will explore the problem of training the behavior of a non-playable character (NPC) in a game. (An NPC is a game character that is never controlled by a human player and its behavior is pre-defined by the game developer.) More specifically, let's assume we're building a multi-player, war-themed game in which players control the soldiers. In this game, we have a single NPC who serves as a medic, finding and reviving wounded players. Lastly, let us assume that there are two teams, each with five players and one NPC medic. The behavior of a medic is quite complex. It first needs to avoid getting injured, which requires detecting when it is in danger and moving to a safe location. Second, it needs to be aware of which of its team members are injured and require assistance. In the case of multiple injuries, it needs to assess the degree of injury and decide who to help first. Lastly, a good medic will always place itself in a position where it can quickly help its team members. Factoring in all of these traits means that at every instance, the medic needs to measure several attributes of the environment (e.g. position of team members, position of enemies, which of its team members are injured and to what degree) and then decide on an action (e.g. hide from enemy fire, move to help one of its members). Given the large number of settings of the environment and the large number of actions that the medic can take, defining and implementing such complex behaviors by hand is challenging and prone to errors. With ML-Agents, it is possible to train the behaviors of such NPCs (called Agents ) using a variety of methods. The basic idea is quite simple. We need to define three entities at every moment of the game (called environment ): Observations - what the medic perceives about the environment. Observations can be numeric and/or visual. Numeric observations measure attributes of the environment from the point of view of the agent. For our medic this would be attributes of the battlefield that are visible to it. For most interesting environments, an agent will require several continuous numeric observations. Visual observations, on the other hand, are images generated from the cameras attached to the agent and represent what the agent is seeing at that point in time. It is common to confuse an agent's observation with the environment (or game) state . The environment state represents information about the entire scene containing all the game characters. The agents observation, however, only contains information that the agent is aware of and is typically a subset of the environment state. For example, the medic observation cannot include information about an enemy in hiding that the medic is unaware of. Actions - what actions the medic can take. Similar to observations, actions can either be continuous or discrete depending on the complexity of the environment and agent. In the case of the medic, if the environment is a simple grid world where only their location matters, then a discrete action taking on one of four values (north, south, east, west) suffices. However, if the environment is more complex and the medic can move freely then using two continuous actions (one for direction and another for speed) is more appropriate. Reward signals - a scalar value indicating how well the medic is doing. Note that the reward signal need not be provided at every moment, but only when the medic performs an action that is good or bad. For example, it can receive a large negative reward if it dies, a modest positive reward whenever it revives a wounded team member, and a modest negative reward when a wounded team member dies due to lack of assistance. Note that the reward signal is how the objectives of the task are communicated to the agent, so they need to be set up in a manner where maximizing reward generates the desired optimal behavior. After defining these three entities (the building blocks of a reinforcement learning task ), we can now train the medic's behavior. This is achieved by simulating the environment for many trials where the medic, over time, learns what is the optimal action to take for every observation it measures by maximizing its future reward. The key is that by learning the actions that maximize its reward, the medic is learning the behaviors that make it a good medic (i.e. one who saves the most number of lives). In reinforcement learning terminology, the behavior that is learned is called a policy , which is essentially a (optimal) mapping from observations to actions. Note that the process of learning a policy through running simulations is called the training phase , while playing the game with an NPC that is using its learned policy is called the inference phase . The ML-Agents Toolkit provides all the necessary tools for using Unity as the simulation engine for learning the policies of different objects in a Unity environment. In the next few sections, we discuss how the ML-Agents Toolkit achieves this and what features it provides. Key Components The ML-Agents Toolkit contains five high-level components: Learning Environment - which contains the Unity scene and all the game characters. The Unity scene provides the environment in which agents observe, act, and learn. How you set up the Unity scene to serve as a learning environment really depends on your goal. You may be trying to solve a specific reinforcement learning problem of limited scope, in which case you can use the same scene for both training and for testing trained agents. Or, you may be training agents to operate in a complex game or simulation. In this case, it might be more efficient and practical to create a purpose-built training scene. The ML-Agents Toolkit includes an ML-Agents Unity SDK ( com.unity.ml-agents package) that enables you to transform any Unity scene into a learning environment by defining the agents and their behaviors. Python Low-Level API - which contains a low-level Python interface for interacting and manipulating a learning environment. Note that, unlike the Learning Environment, the Python API is not part of Unity, but lives outside and communicates with Unity through the Communicator. This API is contained in a dedicated mlagents_envs Python package and is used by the Python training process to communicate with and control the Academy during training. However, it can be used for other purposes as well. For example, you could use the API to use Unity as the simulation engine for your own machine learning algorithms. See Python API for more information. External Communicator - which connects the Learning Environment with the Python Low-Level API. It lives within the Learning Environment. Python Trainers which contains all the machine learning algorithms that enable training agents. The algorithms are implemented in Python and are part of their own mlagents Python package. The package exposes a single command-line utility mlagents-learn that supports all the training methods and options outlined in this document. The Python Trainers interface solely with the Python Low-Level API. Gym Wrapper (not pictured). A common way in which machine learning researchers interact with simulation environments is via a wrapper provided by OpenAI called gym . We provide a gym wrapper in a dedicated gym-unity Python package and instructions for using it with existing machine learning algorithms which utilize gym. Simplified block diagram of ML-Agents. The Learning Environment contains two Unity Components that help organize the Unity scene: Agents - which is attached to a Unity GameObject (any character within a scene) and handles generating its observations, performing the actions it receives and assigning a reward (positive / negative) when appropriate. Each Agent is linked to a Behavior. Behavior - defines specific attributes of the agent such as the number of actions that agent can take. Each Behavior is uniquely identified by a Behavior Name field. A Behavior can be thought as a function that receives observations and rewards from the Agent and returns actions. A Behavior can be of one of three types: Learning, Heuristic or Inference. A Learning Behavior is one that is not, yet, defined but about to be trained. A Heuristic Behavior is one that is defined by a hard-coded set of rules implemented in code. An Inference Behavior is one that includes a trained Neural Network file. In essence, after a Learning Behavior is trained, it becomes an Inference Behavior. Every Learning Environment will always have one Agent for every character in the scene. While each Agent must be linked to a Behavior, it is possible for Agents that have similar observations and actions to have the same Behavior. In our sample game, we have two teams each with their own medic. Thus we will have two Agents in our Learning Environment, one for each medic, but both of these medics can have the same Behavior. This does not mean that at each instance they will have identical observation and action values . Example block diagram of ML-Agents Toolkit for our sample game. Note that in a single environment, there can be multiple Agents and multiple Behaviors at the same time. For example, if we expanded our game to include tank driver NPCs, then the Agent attached to those characters cannot share its Behavior with the Agent linked to the medics (medics and drivers have different actions). The Learning Environment through the Academy (not represented in the diagram) ensures that all the Agents are in sync in addition to controlling environment-wide settings. Lastly, it is possible to exchange data between Unity and Python outside of the machine learning loop through Side Channels . One example of using Side Channels is to exchange data with Python about Environment Parameters . The following diagram illustrates the above. Training Modes Given the flexibility of ML-Agents, there are a few ways in which training and inference can proceed. Built-in Training and Inference As mentioned previously, the ML-Agents Toolkit ships with several implementations of state-of-the-art algorithms for training intelligent agents. More specifically, during training, all the medics in the scene send their observations to the Python API through the External Communicator. The Python API processes these observations and sends back actions for each medic to take. During training these actions are mostly exploratory to help the Python API learn the best policy for each medic. Once training concludes, the learned policy for each medic can be exported as a model file. Then during the inference phase, the medics still continue to generate their observations, but instead of being sent to the Python API, they will be fed into their (internal, embedded) model to generate the optimal action for each medic to take at every point in time. The Getting Started Guide tutorial covers this training mode with the 3D Balance Ball sample environment. Cross-Platform Inference It is important to note that the ML-Agents Toolkit leverages the Unity Inference Engine to run the models within a Unity scene such that an agent can take the optimal action at each step. Given that the Unity Inference Engine support most platforms that Unity does, this means that any model you train with the ML-Agents Toolkit can be embedded into your Unity application that runs on any platform. See our dedicated blog post for additional information. Custom Training and Inference In the previous mode, the Agents were used for training to generate a PyTorch model that the Agents can later use. However, any user of the ML-Agents Toolkit can leverage their own algorithms for training. In this case, the behaviors of all the Agents in the scene will be controlled within Python. You can even turn your environment into a gym. We do not currently have a tutorial highlighting this mode, but you can learn more about the Python API here . Flexible Training Scenarios While the discussion so-far has mostly focused on training a single agent, with ML-Agents, several training scenarios are possible. We are excited to see what kinds of novel and fun environments the community creates. For those new to training intelligent agents, below are a few examples that can serve as inspiration: Single-Agent. A single agent, with its own reward signal. The traditional way of training an agent. An example is any single-player game, such as Chicken. Simultaneous Single-Agent. Multiple independent agents with independent reward signals with same Behavior Parameters . A parallelized version of the traditional training scenario, which can speed-up and stabilize the training process. Helpful when you have multiple versions of the same character in an environment who should learn similar behaviors. An example might be training a dozen robot-arms to each open a door simultaneously. Adversarial Self-Play. Two interacting agents with inverse reward signals. In two-player games, adversarial self-play can allow an agent to become increasingly more skilled, while always having the perfectly matched opponent: itself. This was the strategy employed when training AlphaGo, and more recently used by OpenAI to train a human-beating 1-vs-1 Dota 2 agent. Cooperative Multi-Agent. Multiple interacting agents with a shared reward signal with same or different Behavior Parameters . In this scenario, all agents must work together to accomplish a task that cannot be done alone. Examples include environments where each agent only has access to partial information, which needs to be shared in order to accomplish the task or collaboratively solve a puzzle. Competitive Multi-Agent. Multiple interacting agents with inverse reward signals with same or different Behavior Parameters . In this scenario, agents must compete with one another to either win a competition, or obtain some limited set of resources. All team sports fall into this scenario. Ecosystem. Multiple interacting agents with independent reward signals with same or different Behavior Parameters . This scenario can be thought of as creating a small world in which animals with different goals all interact, such as a savanna in which there might be zebras, elephants and giraffes, or an autonomous driving simulation within an urban environment. Training Methods: Environment-agnostic The remaining sections overview the various state-of-the-art machine learning algorithms that are part of the ML-Agents Toolkit. If you aren't studying machine and reinforcement learning as a subject and just want to train agents to accomplish tasks, you can treat these algorithms as black boxes . There are a few training-related parameters to adjust inside Unity as well as on the Python training side, but you do not need in-depth knowledge of the algorithms themselves to successfully create and train agents. Step-by-step procedures for running the training process are provided in the Training ML-Agents page. This section specifically focuses on the training methods that are available regardless of the specifics of your learning environment. A Quick Note on Reward Signals In this section we introduce the concepts of intrinsic and extrinsic rewards, which helps explain some of the training methods. In reinforcement learning, the end goal for the Agent is to discover a behavior (a Policy) that maximizes a reward. You will need to provide the agent one or more reward signals to use during training. Typically, a reward is defined by your environment, and corresponds to reaching some goal. These are what we refer to as extrinsic rewards, as they are defined external of the learning algorithm. Rewards, however, can be defined outside of the environment as well, to encourage the agent to behave in certain ways, or to aid the learning of the true extrinsic reward. We refer to these rewards as intrinsic reward signals. The total reward that the agent will learn to maximize can be a mix of extrinsic and intrinsic reward signals. The ML-Agents Toolkit allows reward signals to be defined in a modular way, and we provide four reward signals that can the mixed and matched to help shape your agent's behavior: extrinsic : represents the rewards defined in your environment, and is enabled by default gail : represents an intrinsic reward signal that is defined by GAIL (see below) curiosity : represents an intrinsic reward signal that encourages exploration in sparse-reward environments that is defined by the Curiosity module (see below). rnd : represents an intrinsic reward signal that encourages exploration in sparse-reward environments that is defined by the Curiosity module (see below). Deep Reinforcement Learning ML-Agents provide an implementation of two reinforcement learning algorithms: Proximal Policy Optimization (PPO) Soft Actor-Critic (SAC) The default algorithm is PPO. This is a method that has been shown to be more general purpose and stable than many other RL algorithms. In contrast with PPO, SAC is off-policy , which means it can learn from experiences collected at any time during the past. As experiences are collected, they are placed in an experience replay buffer and randomly drawn during training. This makes SAC significantly more sample-efficient, often requiring 5-10 times less samples to learn the same task as PPO. However, SAC tends to require more model updates. SAC is a good choice for heavier or slower environments (about 0.1 seconds per step or more). SAC is also a \"maximum entropy\" algorithm, and enables exploration in an intrinsic way. Read more about maximum entropy RL here . Curiosity for Sparse-reward Environments In environments where the agent receives rare or infrequent rewards (i.e. sparse-reward), an agent may never receive a reward signal on which to bootstrap its training process. This is a scenario where the use of an intrinsic reward signals can be valuable. Curiosity is one such signal which can help the agent explore when extrinsic rewards are sparse. The curiosity Reward Signal enables the Intrinsic Curiosity Module. This is an implementation of the approach described in Curiosity-driven Exploration by Self-supervised Prediction by Pathak, et al. It trains two networks: an inverse model, which takes the current and next observation of the agent, encodes them, and uses the encoding to predict the action that was taken between the observations a forward model, which takes the encoded current observation and action, and predicts the next encoded observation. The loss of the forward model (the difference between the predicted and actual encoded observations) is used as the intrinsic reward, so the more surprised the model is, the larger the reward will be. For more information, see our dedicated blog post on the Curiosity module . RND for Sparse-reward Environments Similarly to Curiosity, Random Network Distillation (RND) is useful in sparse or rare reward environments as it helps the Agent explore. The RND Module is implemented following the paper Exploration by Random Network Distillation . RND uses two networks: - The first is a network with fixed random weights that takes observations as inputs and generates an encoding - The second is a network with similar architecture that is trained to predict the outputs of the first network and uses the observations the Agent collects as training data. The loss (the squared difference between the predicted and actual encoded observations) of the trained model is used as intrinsic reward. The more an Agent visits a state, the more accurate the predictions and the lower the rewards which encourages the Agent to explore new states with higher prediction errors. Imitation Learning It is often more intuitive to simply demonstrate the behavior we want an agent to perform, rather than attempting to have it learn via trial-and-error methods. For example, instead of indirectly training a medic with the help of a reward function, we can give the medic real world examples of observations from the game and actions from a game controller to guide the medic's behavior. Imitation Learning uses pairs of observations and actions from a demonstration to learn a policy. See this video demo of imitation learning . Imitation learning can either be used alone or in conjunction with reinforcement learning. If used alone it can provide a mechanism for learning a specific type of behavior (i.e. a specific style of solving the task). If used in conjunction with reinforcement learning it can dramatically reduce the time the agent takes to solve the environment. This can be especially pronounced in sparse-reward environments. For instance, on the Pyramids environment , using 6 episodes of demonstrations can reduce training steps by more than 4 times. See Behavioral Cloning + GAIL + Curiosity + RL below. The ML-Agents Toolkit provides a way to learn directly from demonstrations, as well as use them to help speed up reward-based training (RL). We include two algorithms called Behavioral Cloning (BC) and Generative Adversarial Imitation Learning (GAIL). In most scenarios, you can combine these two features: If you want to help your agents learn (especially with environments that have sparse rewards) using pre-recorded demonstrations, you can generally enable both GAIL and Behavioral Cloning at low strengths in addition to having an extrinsic reward. An example of this is provided for the PushBlock example environment in config/imitation/PushBlock.yaml . If you want to train purely from demonstrations with GAIL and BC without an extrinsic reward signal, please see the CrawlerStatic example environment under in config/imitation/CrawlerStatic.yaml . Note: GAIL introduces a survivor bias to the learning process. That is, by giving positive rewards based on similarity to the expert, the agent is incentivized to remain alive for as long as possible. This can directly conflict with goal-oriented tasks like our PushBlock or Pyramids example environments where an agent must reach a goal state thus ending the episode as quickly as possible. In these cases, we strongly recommend that you use a low strength GAIL reward signal and a sparse extrinisic signal when the agent achieves the task. This way, the GAIL reward signal will guide the agent until it discovers the extrnisic signal and will not overpower it. If the agent appears to be ignoring the extrinsic reward signal, you should reduce the strength of GAIL. GAIL (Generative Adversarial Imitation Learning) GAIL, or Generative Adversarial Imitation Learning , uses an adversarial approach to reward your Agent for behaving similar to a set of demonstrations. GAIL can be used with or without environment rewards, and works well when there are a limited number of demonstrations. In this framework, a second neural network, the discriminator, is taught to distinguish whether an observation/action is from a demonstration or produced by the agent. This discriminator can then examine a new observation/action and provide it a reward based on how close it believes this new observation/action is to the provided demonstrations. At each training step, the agent tries to learn how to maximize this reward. Then, the discriminator is trained to better distinguish between demonstrations and agent state/actions. In this way, while the agent gets better and better at mimicking the demonstrations, the discriminator keeps getting stricter and stricter and the agent must try harder to \"fool\" it. This approach learns a policy that produces states and actions similar to the demonstrations, requiring fewer demonstrations than direct cloning of the actions. In addition to learning purely from demonstrations, the GAIL reward signal can be mixed with an extrinsic reward signal to guide the learning process. Behavioral Cloning (BC) BC trains the Agent's policy to exactly mimic the actions shown in a set of demonstrations. The BC feature can be enabled on the PPO or SAC trainers. As BC cannot generalize past the examples shown in the demonstrations, BC tends to work best when there exists demonstrations for nearly all of the states that the agent can experience, or in conjunction with GAIL and/or an extrinsic reward. Recording Demonstrations Demonstrations of agent behavior can be recorded from the Unity Editor or build, and saved as assets. These demonstrations contain information on the observations, actions, and rewards for a given agent during the recording session. They can be managed in the Editor, as well as used for training with BC and GAIL. See the Designing Agents page for more information on how to record demonstrations for your agent. Summary To summarize, we provide 3 training methods: BC, GAIL and RL (PPO or SAC) that can be used independently or together: BC can be used on its own or as a pre-training step before GAIL and/or RL GAIL can be used with or without extrinsic rewards RL can be used on its own (either PPO or SAC) or in conjunction with BC and/or GAIL. Leveraging either BC or GAIL requires recording demonstrations to be provided as input to the training algorithms. Training Methods: Environment-specific In addition to the three environment-agnostic training methods introduced in the previous section, the ML-Agents Toolkit provides additional methods that can aid in training behaviors for specific types of environments. Training in Competitive Multi-Agent Environments with Self-Play ML-Agents provides the functionality to train both symmetric and asymmetric adversarial games with Self-Play . A symmetric game is one in which opposing agents are equal in form, function and objective. Examples of symmetric games are our Tennis and Soccer example environments. In reinforcement learning, this means both agents have the same observation and actions and learn from the same reward function and so they can share the same policy . In asymmetric games, this is not the case. An example of an asymmetric games are Hide and Seek. Agents in these types of games do not always have the same observation or actions and so sharing policy networks is not necessarily ideal. With self-play, an agent learns in adversarial games by competing against fixed, past versions of its opponent (which could be itself as in symmetric games) to provide a more stable, stationary learning environment. This is compared to competing against the current, best opponent in every episode, which is constantly changing (because it's learning). Self-play can be used with our implementations of both Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC). However, from the perspective of an individual agent, these scenarios appear to have non-stationary dynamics because the opponent is often changing. This can cause significant issues in the experience replay mechanism used by SAC. Thus, we recommend that users use PPO. For further reading on this issue in particular, see the paper Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning . See our Designing Agents page for more information on setting up teams in your Unity scene. Also, read our blog post on self-play for additional information. Training In Cooperative Multi-Agent Environments with MA-POCA ML-Agents provides the functionality for training cooperative behaviors - i.e., groups of agents working towards a common goal, where the success of the individual is linked to the success of the whole group. In such a scenario, agents typically receive rewards as a group. For instance, if a team of agents wins a game against an opposing team, everyone is rewarded - even agents who did not directly contribute to the win. This makes learning what to do as an individual difficult - you may get a win for doing nothing, and a loss for doing your best. In ML-Agents, we provide MA-POCA (MultiAgent POsthumous Credit Assignment), which is a novel multi-agent trainer that trains a centralized critic , a neural network that acts as a \"coach\" for a whole group of agents. You can then give rewards to the team as a whole, and the agents will learn how best to contribute to achieving that reward. Agents can also be given rewards individually, and the team will work together to help the individual achieve those goals. During an episode, agents can be added or removed from the group, such as when agents spawn or die in a game. If agents are removed mid-episode (e.g., if teammates die or are removed from the game), they will still learn whether their actions contributed to the team winning later, enabling agents to take group-beneficial actions even if they result in the individual being removed from the game (i.e., self-sacrifice). MA-POCA can also be combined with self-play to train teams of agents to play against each other. To learn more about enabling cooperative behaviors for agents in an ML-Agents environment, check out this page . For further reading, MA-POCA builds on previous work in multi-agent cooperative learning ( Lowe et al. , Foerster et al. , among others) to enable the above use-cases. Solving Complex Tasks using Curriculum Learning Curriculum learning is a way of training a machine learning model where more difficult aspects of a problem are gradually introduced in such a way that the model is always optimally challenged. This idea has been around for a long time, and it is how we humans typically learn. If you imagine any childhood primary school education, there is an ordering of classes and topics. Arithmetic is taught before algebra, for example. Likewise, algebra is taught before calculus. The skills and knowledge learned in the earlier subjects provide a scaffolding for later lessons. The same principle can be applied to machine learning, where training on easier tasks can provide a scaffolding for harder tasks in the future. Imagine training the medic to scale a wall to arrive at a wounded team member. The starting point when training a medic to accomplish this task will be a random policy. That starting policy will have the medic running in circles, and will likely never, or very rarely scale the wall properly to revive their team member (and achieve the reward). If we start with a simpler task, such as moving toward an unobstructed team member, then the medic can easily learn to accomplish the task. From there, we can slowly add to the difficulty of the task by increasing the size of the wall until the medic can complete the initially near-impossible task of scaling the wall. We have included an environment to demonstrate this with ML-Agents, called Wall Jump . Demonstration of a hypothetical curriculum training scenario in which a progressively taller wall obstructs the path to the goal. [ Note : The example provided above is for instructional purposes, and was based on an early version of the Wall Jump example environment . As such, it is not possible to directly replicate the results here using that environment.] The ML-Agents Toolkit supports modifying custom environment parameters during the training process to aid in learning. This allows elements of the environment related to difficulty or complexity to be dynamically adjusted based on training progress. The Training ML-Agents page has more information on defining training curriculums. Training Robust Agents using Environment Parameter Randomization An agent trained on a specific environment, may be unable to generalize to any tweaks or variations in the environment (in machine learning this is referred to as overfitting). This becomes problematic in cases where environments are instantiated with varying objects or properties. One mechanism to alleviate this and train more robust agents that can generalize to unseen variations of the environment is to expose them to these variations during training. Similar to Curriculum Learning, where environments become more difficult as the agent learns, the ML-Agents Toolkit provides a way to randomly sample parameters of the environment during training. We refer to this approach as Environment Parameter Randomization . For those familiar with Reinforcement Learning research, this approach is based on the concept of Domain Randomization . By using parameter randomization during training , the agent can be better suited to adapt (with higher performance) to future unseen variations of the environment. Ball scale of 0.5 Ball scale of 4 Example of variations of the 3D Ball environment. The environment parameters are gravity , ball_mass and ball_scale . Model Types Regardless of the training method deployed, there are a few model types that users can train using the ML-Agents Toolkit. This is due to the flexibility in defining agent observations, which include vector, ray cast and visual observations. You can learn more about how to instrument an agent's observation in the Designing Agents guide. Learning from Vector Observations Whether an agent's observations are ray cast or vector, the ML-Agents Toolkit provides a fully connected neural network model to learn from those observations. At training time you can configure different aspects of this model such as the number of hidden units and number of layers. Learning from Cameras using Convolutional Neural Networks Unlike other platforms, where the agent\u2019s observation might be limited to a single vector or image, the ML-Agents Toolkit allows multiple cameras to be used for observations per agent. This enables agents to learn to integrate information from multiple visual streams. This can be helpful in several scenarios such as training a self-driving car which requires multiple cameras with different viewpoints, or a navigational agent which might need to integrate aerial and first-person visuals. You can learn more about adding visual observations to an agent here . When visual observations are utilized, the ML-Agents Toolkit leverages convolutional neural networks (CNN) to learn from the input images. We offer three network architectures: a simple encoder which consists of two convolutional layers the implementation proposed by Mnih et al. , consisting of three convolutional layers, the IMPALA Resnet consisting of three stacked layers, each with two residual blocks, making a much larger network than the other two. The choice of the architecture depends on the visual complexity of the scene and the available computational resources. Learning from Variable Length Observations using Attention Using the ML-Agents Toolkit, it is possible to have agents learn from a varying number of inputs. To do so, each agent can keep track of a buffer of vector observations. At each step, the agent will go through all the elements in the buffer and extract information but the elements in the buffer can change at every step. This can be useful in scenarios in which the agents must keep track of a varying number of elements throughout the episode. For example in a game where an agent must learn to avoid projectiles, but the projectiles can vary in numbers. You can learn more about variable length observations here . When variable length observations are utilized, the ML-Agents Toolkit leverages attention networks to learn from a varying number of entities. Agents using attention will ignore entities that are deemed not relevant and pay special attention to entities relevant to the current situation based on context. Memory-enhanced Agents using Recurrent Neural Networks Have you ever entered a room to get something and immediately forgot what you were looking for? Don't let that happen to your agents. In some scenarios, agents must learn to remember the past in order to take the best decision. When an agent only has partial observability of the environment, keeping track of past observations can help the agent learn. Deciding what the agents should remember in order to solve a task is not easy to do by hand, but our training algorithms can learn to keep track of what is important to remember with LSTM . Additional Features Beyond the flexible training scenarios available, the ML-Agents Toolkit includes additional features which improve the flexibility and interpretability of the training process. Concurrent Unity Instances - We enable developers to run concurrent, parallel instances of the Unity executable during training. For certain scenarios, this should speed up training. Check out our dedicated page on creating a Unity executable and the Training ML-Agents page for instructions on how to set the number of concurrent instances. Recording Statistics from Unity - We enable developers to record statistics from within their Unity environments. These statistics are aggregated and generated during the training process. Custom Side Channels - We enable developers to create custom side channels to manage data transfer between Unity and Python that is unique to their training workflow and/or environment. Custom Samplers - We enable developers to create custom sampling methods for Environment Parameter Randomization. This enables users to customize this training method for their particular environment. Summary and Next Steps To briefly summarize: The ML-Agents Toolkit enables games and simulations built in Unity to serve as the platform for training intelligent agents. It is designed to enable a large variety of training modes and scenarios and comes packed with several features to enable researchers and developers to leverage (and enhance) machine learning within Unity. In terms of next steps: For a walkthrough of running ML-Agents with a simple scene, check out the Getting Started guide. For a \"Hello World\" introduction to creating your own Learning Environment, check out the Making a New Learning Environment page. For an overview on the more complex example environments that are provided in this toolkit, check out the Example Environments page. For more information on the various training options available, check out the Training ML-Agents page.","title":"ML-Agents Toolkit Overview"},{"location":"ML-Agents-Overview/#ml-agents-toolkit-overview","text":"Table of Contents Running Example: Training NPC Behaviors Key Components Training Modes Built-in Training and Inference Cross-Platform Inference Custom Training and Inference Flexible Training Scenarios Training Methods: Environment-agnostic A Quick Note on Reward Signals Deep Reinforcement Learning Curiosity for Sparse-reward Environments RND for Sparse-reward Environments Imitation Learning GAIL (Generative Adversarial Imitation Learning) Behavioral Cloning (BC) Recording Demonstrations Summary Training Methods: Environment-specific Training in Competitive Multi-Agent Environments with Self-Play Training in Cooperative Multi-Agent Environments with MA-POCA Solving Complex Tasks using Curriculum Learning Training Robust Agents using Environment Parameter Randomization Model Types Learning from Vector Observations Learning from Cameras using Convolutional Neural Networks Learning from Variable Length Observations using Attention Memory-enhanced Agents using Recurrent Neural Networks Additional Features Summary and Next Steps The Unity Machine Learning Agents Toolkit (ML-Agents Toolkit) is an open-source project that enables games and simulations to serve as environments for training intelligent agents. Agents can be trained using reinforcement learning, imitation learning, neuroevolution, or other machine learning methods through a simple-to-use Python API. We also provide implementations (based on PyTorch) of state-of-the-art algorithms to enable game developers and hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games. These trained agents can be used for multiple purposes, including controlling NPC behavior (in a variety of settings such as multi-agent and adversarial), automated testing of game builds and evaluating different game design decisions pre-release. The ML-Agents Toolkit is mutually beneficial for both game developers and AI researchers as it provides a central platform where advances in AI can be evaluated on Unity\u2019s rich environments and then made accessible to the wider research and game developer communities. Depending on your background (i.e. researcher, game developer, hobbyist), you may have very different questions on your mind at the moment. To make your transition to the ML-Agents Toolkit easier, we provide several background pages that include overviews and helpful resources on the Unity Engine , machine learning and PyTorch . We strongly recommend browsing the relevant background pages if you're not familiar with a Unity scene, basic machine learning concepts or have not previously heard of PyTorch. The remainder of this page contains a deep dive into ML-Agents, its key components, different training modes and scenarios. By the end of it, you should have a good sense of what the ML-Agents Toolkit allows you to do. The subsequent documentation pages provide examples of how to use ML-Agents. To get started, watch this demo video of ML-Agents in action .","title":"ML-Agents Toolkit Overview"},{"location":"ML-Agents-Overview/#running-example-training-npc-behaviors","text":"To help explain the material and terminology in this page, we'll use a hypothetical, running example throughout. We will explore the problem of training the behavior of a non-playable character (NPC) in a game. (An NPC is a game character that is never controlled by a human player and its behavior is pre-defined by the game developer.) More specifically, let's assume we're building a multi-player, war-themed game in which players control the soldiers. In this game, we have a single NPC who serves as a medic, finding and reviving wounded players. Lastly, let us assume that there are two teams, each with five players and one NPC medic. The behavior of a medic is quite complex. It first needs to avoid getting injured, which requires detecting when it is in danger and moving to a safe location. Second, it needs to be aware of which of its team members are injured and require assistance. In the case of multiple injuries, it needs to assess the degree of injury and decide who to help first. Lastly, a good medic will always place itself in a position where it can quickly help its team members. Factoring in all of these traits means that at every instance, the medic needs to measure several attributes of the environment (e.g. position of team members, position of enemies, which of its team members are injured and to what degree) and then decide on an action (e.g. hide from enemy fire, move to help one of its members). Given the large number of settings of the environment and the large number of actions that the medic can take, defining and implementing such complex behaviors by hand is challenging and prone to errors. With ML-Agents, it is possible to train the behaviors of such NPCs (called Agents ) using a variety of methods. The basic idea is quite simple. We need to define three entities at every moment of the game (called environment ): Observations - what the medic perceives about the environment. Observations can be numeric and/or visual. Numeric observations measure attributes of the environment from the point of view of the agent. For our medic this would be attributes of the battlefield that are visible to it. For most interesting environments, an agent will require several continuous numeric observations. Visual observations, on the other hand, are images generated from the cameras attached to the agent and represent what the agent is seeing at that point in time. It is common to confuse an agent's observation with the environment (or game) state . The environment state represents information about the entire scene containing all the game characters. The agents observation, however, only contains information that the agent is aware of and is typically a subset of the environment state. For example, the medic observation cannot include information about an enemy in hiding that the medic is unaware of. Actions - what actions the medic can take. Similar to observations, actions can either be continuous or discrete depending on the complexity of the environment and agent. In the case of the medic, if the environment is a simple grid world where only their location matters, then a discrete action taking on one of four values (north, south, east, west) suffices. However, if the environment is more complex and the medic can move freely then using two continuous actions (one for direction and another for speed) is more appropriate. Reward signals - a scalar value indicating how well the medic is doing. Note that the reward signal need not be provided at every moment, but only when the medic performs an action that is good or bad. For example, it can receive a large negative reward if it dies, a modest positive reward whenever it revives a wounded team member, and a modest negative reward when a wounded team member dies due to lack of assistance. Note that the reward signal is how the objectives of the task are communicated to the agent, so they need to be set up in a manner where maximizing reward generates the desired optimal behavior. After defining these three entities (the building blocks of a reinforcement learning task ), we can now train the medic's behavior. This is achieved by simulating the environment for many trials where the medic, over time, learns what is the optimal action to take for every observation it measures by maximizing its future reward. The key is that by learning the actions that maximize its reward, the medic is learning the behaviors that make it a good medic (i.e. one who saves the most number of lives). In reinforcement learning terminology, the behavior that is learned is called a policy , which is essentially a (optimal) mapping from observations to actions. Note that the process of learning a policy through running simulations is called the training phase , while playing the game with an NPC that is using its learned policy is called the inference phase . The ML-Agents Toolkit provides all the necessary tools for using Unity as the simulation engine for learning the policies of different objects in a Unity environment. In the next few sections, we discuss how the ML-Agents Toolkit achieves this and what features it provides.","title":"Running Example: Training NPC Behaviors"},{"location":"ML-Agents-Overview/#key-components","text":"The ML-Agents Toolkit contains five high-level components: Learning Environment - which contains the Unity scene and all the game characters. The Unity scene provides the environment in which agents observe, act, and learn. How you set up the Unity scene to serve as a learning environment really depends on your goal. You may be trying to solve a specific reinforcement learning problem of limited scope, in which case you can use the same scene for both training and for testing trained agents. Or, you may be training agents to operate in a complex game or simulation. In this case, it might be more efficient and practical to create a purpose-built training scene. The ML-Agents Toolkit includes an ML-Agents Unity SDK ( com.unity.ml-agents package) that enables you to transform any Unity scene into a learning environment by defining the agents and their behaviors. Python Low-Level API - which contains a low-level Python interface for interacting and manipulating a learning environment. Note that, unlike the Learning Environment, the Python API is not part of Unity, but lives outside and communicates with Unity through the Communicator. This API is contained in a dedicated mlagents_envs Python package and is used by the Python training process to communicate with and control the Academy during training. However, it can be used for other purposes as well. For example, you could use the API to use Unity as the simulation engine for your own machine learning algorithms. See Python API for more information. External Communicator - which connects the Learning Environment with the Python Low-Level API. It lives within the Learning Environment. Python Trainers which contains all the machine learning algorithms that enable training agents. The algorithms are implemented in Python and are part of their own mlagents Python package. The package exposes a single command-line utility mlagents-learn that supports all the training methods and options outlined in this document. The Python Trainers interface solely with the Python Low-Level API. Gym Wrapper (not pictured). A common way in which machine learning researchers interact with simulation environments is via a wrapper provided by OpenAI called gym . We provide a gym wrapper in a dedicated gym-unity Python package and instructions for using it with existing machine learning algorithms which utilize gym. Simplified block diagram of ML-Agents. The Learning Environment contains two Unity Components that help organize the Unity scene: Agents - which is attached to a Unity GameObject (any character within a scene) and handles generating its observations, performing the actions it receives and assigning a reward (positive / negative) when appropriate. Each Agent is linked to a Behavior. Behavior - defines specific attributes of the agent such as the number of actions that agent can take. Each Behavior is uniquely identified by a Behavior Name field. A Behavior can be thought as a function that receives observations and rewards from the Agent and returns actions. A Behavior can be of one of three types: Learning, Heuristic or Inference. A Learning Behavior is one that is not, yet, defined but about to be trained. A Heuristic Behavior is one that is defined by a hard-coded set of rules implemented in code. An Inference Behavior is one that includes a trained Neural Network file. In essence, after a Learning Behavior is trained, it becomes an Inference Behavior. Every Learning Environment will always have one Agent for every character in the scene. While each Agent must be linked to a Behavior, it is possible for Agents that have similar observations and actions to have the same Behavior. In our sample game, we have two teams each with their own medic. Thus we will have two Agents in our Learning Environment, one for each medic, but both of these medics can have the same Behavior. This does not mean that at each instance they will have identical observation and action values . Example block diagram of ML-Agents Toolkit for our sample game. Note that in a single environment, there can be multiple Agents and multiple Behaviors at the same time. For example, if we expanded our game to include tank driver NPCs, then the Agent attached to those characters cannot share its Behavior with the Agent linked to the medics (medics and drivers have different actions). The Learning Environment through the Academy (not represented in the diagram) ensures that all the Agents are in sync in addition to controlling environment-wide settings. Lastly, it is possible to exchange data between Unity and Python outside of the machine learning loop through Side Channels . One example of using Side Channels is to exchange data with Python about Environment Parameters . The following diagram illustrates the above.","title":"Key Components"},{"location":"ML-Agents-Overview/#training-modes","text":"Given the flexibility of ML-Agents, there are a few ways in which training and inference can proceed.","title":"Training Modes"},{"location":"ML-Agents-Overview/#built-in-training-and-inference","text":"As mentioned previously, the ML-Agents Toolkit ships with several implementations of state-of-the-art algorithms for training intelligent agents. More specifically, during training, all the medics in the scene send their observations to the Python API through the External Communicator. The Python API processes these observations and sends back actions for each medic to take. During training these actions are mostly exploratory to help the Python API learn the best policy for each medic. Once training concludes, the learned policy for each medic can be exported as a model file. Then during the inference phase, the medics still continue to generate their observations, but instead of being sent to the Python API, they will be fed into their (internal, embedded) model to generate the optimal action for each medic to take at every point in time. The Getting Started Guide tutorial covers this training mode with the 3D Balance Ball sample environment.","title":"Built-in Training and Inference"},{"location":"ML-Agents-Overview/#cross-platform-inference","text":"It is important to note that the ML-Agents Toolkit leverages the Unity Inference Engine to run the models within a Unity scene such that an agent can take the optimal action at each step. Given that the Unity Inference Engine support most platforms that Unity does, this means that any model you train with the ML-Agents Toolkit can be embedded into your Unity application that runs on any platform. See our dedicated blog post for additional information.","title":"Cross-Platform Inference"},{"location":"ML-Agents-Overview/#custom-training-and-inference","text":"In the previous mode, the Agents were used for training to generate a PyTorch model that the Agents can later use. However, any user of the ML-Agents Toolkit can leverage their own algorithms for training. In this case, the behaviors of all the Agents in the scene will be controlled within Python. You can even turn your environment into a gym. We do not currently have a tutorial highlighting this mode, but you can learn more about the Python API here .","title":"Custom Training and Inference"},{"location":"ML-Agents-Overview/#flexible-training-scenarios","text":"While the discussion so-far has mostly focused on training a single agent, with ML-Agents, several training scenarios are possible. We are excited to see what kinds of novel and fun environments the community creates. For those new to training intelligent agents, below are a few examples that can serve as inspiration: Single-Agent. A single agent, with its own reward signal. The traditional way of training an agent. An example is any single-player game, such as Chicken. Simultaneous Single-Agent. Multiple independent agents with independent reward signals with same Behavior Parameters . A parallelized version of the traditional training scenario, which can speed-up and stabilize the training process. Helpful when you have multiple versions of the same character in an environment who should learn similar behaviors. An example might be training a dozen robot-arms to each open a door simultaneously. Adversarial Self-Play. Two interacting agents with inverse reward signals. In two-player games, adversarial self-play can allow an agent to become increasingly more skilled, while always having the perfectly matched opponent: itself. This was the strategy employed when training AlphaGo, and more recently used by OpenAI to train a human-beating 1-vs-1 Dota 2 agent. Cooperative Multi-Agent. Multiple interacting agents with a shared reward signal with same or different Behavior Parameters . In this scenario, all agents must work together to accomplish a task that cannot be done alone. Examples include environments where each agent only has access to partial information, which needs to be shared in order to accomplish the task or collaboratively solve a puzzle. Competitive Multi-Agent. Multiple interacting agents with inverse reward signals with same or different Behavior Parameters . In this scenario, agents must compete with one another to either win a competition, or obtain some limited set of resources. All team sports fall into this scenario. Ecosystem. Multiple interacting agents with independent reward signals with same or different Behavior Parameters . This scenario can be thought of as creating a small world in which animals with different goals all interact, such as a savanna in which there might be zebras, elephants and giraffes, or an autonomous driving simulation within an urban environment.","title":"Flexible Training Scenarios"},{"location":"ML-Agents-Overview/#training-methods-environment-agnostic","text":"The remaining sections overview the various state-of-the-art machine learning algorithms that are part of the ML-Agents Toolkit. If you aren't studying machine and reinforcement learning as a subject and just want to train agents to accomplish tasks, you can treat these algorithms as black boxes . There are a few training-related parameters to adjust inside Unity as well as on the Python training side, but you do not need in-depth knowledge of the algorithms themselves to successfully create and train agents. Step-by-step procedures for running the training process are provided in the Training ML-Agents page. This section specifically focuses on the training methods that are available regardless of the specifics of your learning environment.","title":"Training Methods: Environment-agnostic"},{"location":"ML-Agents-Overview/#a-quick-note-on-reward-signals","text":"In this section we introduce the concepts of intrinsic and extrinsic rewards, which helps explain some of the training methods. In reinforcement learning, the end goal for the Agent is to discover a behavior (a Policy) that maximizes a reward. You will need to provide the agent one or more reward signals to use during training. Typically, a reward is defined by your environment, and corresponds to reaching some goal. These are what we refer to as extrinsic rewards, as they are defined external of the learning algorithm. Rewards, however, can be defined outside of the environment as well, to encourage the agent to behave in certain ways, or to aid the learning of the true extrinsic reward. We refer to these rewards as intrinsic reward signals. The total reward that the agent will learn to maximize can be a mix of extrinsic and intrinsic reward signals. The ML-Agents Toolkit allows reward signals to be defined in a modular way, and we provide four reward signals that can the mixed and matched to help shape your agent's behavior: extrinsic : represents the rewards defined in your environment, and is enabled by default gail : represents an intrinsic reward signal that is defined by GAIL (see below) curiosity : represents an intrinsic reward signal that encourages exploration in sparse-reward environments that is defined by the Curiosity module (see below). rnd : represents an intrinsic reward signal that encourages exploration in sparse-reward environments that is defined by the Curiosity module (see below).","title":"A Quick Note on Reward Signals"},{"location":"ML-Agents-Overview/#deep-reinforcement-learning","text":"ML-Agents provide an implementation of two reinforcement learning algorithms: Proximal Policy Optimization (PPO) Soft Actor-Critic (SAC) The default algorithm is PPO. This is a method that has been shown to be more general purpose and stable than many other RL algorithms. In contrast with PPO, SAC is off-policy , which means it can learn from experiences collected at any time during the past. As experiences are collected, they are placed in an experience replay buffer and randomly drawn during training. This makes SAC significantly more sample-efficient, often requiring 5-10 times less samples to learn the same task as PPO. However, SAC tends to require more model updates. SAC is a good choice for heavier or slower environments (about 0.1 seconds per step or more). SAC is also a \"maximum entropy\" algorithm, and enables exploration in an intrinsic way. Read more about maximum entropy RL here .","title":"Deep Reinforcement Learning"},{"location":"ML-Agents-Overview/#curiosity-for-sparse-reward-environments","text":"In environments where the agent receives rare or infrequent rewards (i.e. sparse-reward), an agent may never receive a reward signal on which to bootstrap its training process. This is a scenario where the use of an intrinsic reward signals can be valuable. Curiosity is one such signal which can help the agent explore when extrinsic rewards are sparse. The curiosity Reward Signal enables the Intrinsic Curiosity Module. This is an implementation of the approach described in Curiosity-driven Exploration by Self-supervised Prediction by Pathak, et al. It trains two networks: an inverse model, which takes the current and next observation of the agent, encodes them, and uses the encoding to predict the action that was taken between the observations a forward model, which takes the encoded current observation and action, and predicts the next encoded observation. The loss of the forward model (the difference between the predicted and actual encoded observations) is used as the intrinsic reward, so the more surprised the model is, the larger the reward will be. For more information, see our dedicated blog post on the Curiosity module .","title":"Curiosity for Sparse-reward Environments"},{"location":"ML-Agents-Overview/#rnd-for-sparse-reward-environments","text":"Similarly to Curiosity, Random Network Distillation (RND) is useful in sparse or rare reward environments as it helps the Agent explore. The RND Module is implemented following the paper Exploration by Random Network Distillation . RND uses two networks: - The first is a network with fixed random weights that takes observations as inputs and generates an encoding - The second is a network with similar architecture that is trained to predict the outputs of the first network and uses the observations the Agent collects as training data. The loss (the squared difference between the predicted and actual encoded observations) of the trained model is used as intrinsic reward. The more an Agent visits a state, the more accurate the predictions and the lower the rewards which encourages the Agent to explore new states with higher prediction errors.","title":"RND for Sparse-reward Environments"},{"location":"ML-Agents-Overview/#imitation-learning","text":"It is often more intuitive to simply demonstrate the behavior we want an agent to perform, rather than attempting to have it learn via trial-and-error methods. For example, instead of indirectly training a medic with the help of a reward function, we can give the medic real world examples of observations from the game and actions from a game controller to guide the medic's behavior. Imitation Learning uses pairs of observations and actions from a demonstration to learn a policy. See this video demo of imitation learning . Imitation learning can either be used alone or in conjunction with reinforcement learning. If used alone it can provide a mechanism for learning a specific type of behavior (i.e. a specific style of solving the task). If used in conjunction with reinforcement learning it can dramatically reduce the time the agent takes to solve the environment. This can be especially pronounced in sparse-reward environments. For instance, on the Pyramids environment , using 6 episodes of demonstrations can reduce training steps by more than 4 times. See Behavioral Cloning + GAIL + Curiosity + RL below. The ML-Agents Toolkit provides a way to learn directly from demonstrations, as well as use them to help speed up reward-based training (RL). We include two algorithms called Behavioral Cloning (BC) and Generative Adversarial Imitation Learning (GAIL). In most scenarios, you can combine these two features: If you want to help your agents learn (especially with environments that have sparse rewards) using pre-recorded demonstrations, you can generally enable both GAIL and Behavioral Cloning at low strengths in addition to having an extrinsic reward. An example of this is provided for the PushBlock example environment in config/imitation/PushBlock.yaml . If you want to train purely from demonstrations with GAIL and BC without an extrinsic reward signal, please see the CrawlerStatic example environment under in config/imitation/CrawlerStatic.yaml . Note: GAIL introduces a survivor bias to the learning process. That is, by giving positive rewards based on similarity to the expert, the agent is incentivized to remain alive for as long as possible. This can directly conflict with goal-oriented tasks like our PushBlock or Pyramids example environments where an agent must reach a goal state thus ending the episode as quickly as possible. In these cases, we strongly recommend that you use a low strength GAIL reward signal and a sparse extrinisic signal when the agent achieves the task. This way, the GAIL reward signal will guide the agent until it discovers the extrnisic signal and will not overpower it. If the agent appears to be ignoring the extrinsic reward signal, you should reduce the strength of GAIL.","title":"Imitation Learning"},{"location":"ML-Agents-Overview/#gail-generative-adversarial-imitation-learning","text":"GAIL, or Generative Adversarial Imitation Learning , uses an adversarial approach to reward your Agent for behaving similar to a set of demonstrations. GAIL can be used with or without environment rewards, and works well when there are a limited number of demonstrations. In this framework, a second neural network, the discriminator, is taught to distinguish whether an observation/action is from a demonstration or produced by the agent. This discriminator can then examine a new observation/action and provide it a reward based on how close it believes this new observation/action is to the provided demonstrations. At each training step, the agent tries to learn how to maximize this reward. Then, the discriminator is trained to better distinguish between demonstrations and agent state/actions. In this way, while the agent gets better and better at mimicking the demonstrations, the discriminator keeps getting stricter and stricter and the agent must try harder to \"fool\" it. This approach learns a policy that produces states and actions similar to the demonstrations, requiring fewer demonstrations than direct cloning of the actions. In addition to learning purely from demonstrations, the GAIL reward signal can be mixed with an extrinsic reward signal to guide the learning process.","title":"GAIL (Generative Adversarial Imitation Learning)"},{"location":"ML-Agents-Overview/#behavioral-cloning-bc","text":"BC trains the Agent's policy to exactly mimic the actions shown in a set of demonstrations. The BC feature can be enabled on the PPO or SAC trainers. As BC cannot generalize past the examples shown in the demonstrations, BC tends to work best when there exists demonstrations for nearly all of the states that the agent can experience, or in conjunction with GAIL and/or an extrinsic reward.","title":"Behavioral Cloning (BC)"},{"location":"ML-Agents-Overview/#recording-demonstrations","text":"Demonstrations of agent behavior can be recorded from the Unity Editor or build, and saved as assets. These demonstrations contain information on the observations, actions, and rewards for a given agent during the recording session. They can be managed in the Editor, as well as used for training with BC and GAIL. See the Designing Agents page for more information on how to record demonstrations for your agent.","title":"Recording Demonstrations"},{"location":"ML-Agents-Overview/#summary","text":"To summarize, we provide 3 training methods: BC, GAIL and RL (PPO or SAC) that can be used independently or together: BC can be used on its own or as a pre-training step before GAIL and/or RL GAIL can be used with or without extrinsic rewards RL can be used on its own (either PPO or SAC) or in conjunction with BC and/or GAIL. Leveraging either BC or GAIL requires recording demonstrations to be provided as input to the training algorithms.","title":"Summary"},{"location":"ML-Agents-Overview/#training-methods-environment-specific","text":"In addition to the three environment-agnostic training methods introduced in the previous section, the ML-Agents Toolkit provides additional methods that can aid in training behaviors for specific types of environments.","title":"Training Methods: Environment-specific"},{"location":"ML-Agents-Overview/#training-in-competitive-multi-agent-environments-with-self-play","text":"ML-Agents provides the functionality to train both symmetric and asymmetric adversarial games with Self-Play . A symmetric game is one in which opposing agents are equal in form, function and objective. Examples of symmetric games are our Tennis and Soccer example environments. In reinforcement learning, this means both agents have the same observation and actions and learn from the same reward function and so they can share the same policy . In asymmetric games, this is not the case. An example of an asymmetric games are Hide and Seek. Agents in these types of games do not always have the same observation or actions and so sharing policy networks is not necessarily ideal. With self-play, an agent learns in adversarial games by competing against fixed, past versions of its opponent (which could be itself as in symmetric games) to provide a more stable, stationary learning environment. This is compared to competing against the current, best opponent in every episode, which is constantly changing (because it's learning). Self-play can be used with our implementations of both Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC). However, from the perspective of an individual agent, these scenarios appear to have non-stationary dynamics because the opponent is often changing. This can cause significant issues in the experience replay mechanism used by SAC. Thus, we recommend that users use PPO. For further reading on this issue in particular, see the paper Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning . See our Designing Agents page for more information on setting up teams in your Unity scene. Also, read our blog post on self-play for additional information.","title":"Training in Competitive Multi-Agent Environments with Self-Play"},{"location":"ML-Agents-Overview/#training-in-cooperative-multi-agent-environments-with-ma-poca","text":"ML-Agents provides the functionality for training cooperative behaviors - i.e., groups of agents working towards a common goal, where the success of the individual is linked to the success of the whole group. In such a scenario, agents typically receive rewards as a group. For instance, if a team of agents wins a game against an opposing team, everyone is rewarded - even agents who did not directly contribute to the win. This makes learning what to do as an individual difficult - you may get a win for doing nothing, and a loss for doing your best. In ML-Agents, we provide MA-POCA (MultiAgent POsthumous Credit Assignment), which is a novel multi-agent trainer that trains a centralized critic , a neural network that acts as a \"coach\" for a whole group of agents. You can then give rewards to the team as a whole, and the agents will learn how best to contribute to achieving that reward. Agents can also be given rewards individually, and the team will work together to help the individual achieve those goals. During an episode, agents can be added or removed from the group, such as when agents spawn or die in a game. If agents are removed mid-episode (e.g., if teammates die or are removed from the game), they will still learn whether their actions contributed to the team winning later, enabling agents to take group-beneficial actions even if they result in the individual being removed from the game (i.e., self-sacrifice). MA-POCA can also be combined with self-play to train teams of agents to play against each other. To learn more about enabling cooperative behaviors for agents in an ML-Agents environment, check out this page . For further reading, MA-POCA builds on previous work in multi-agent cooperative learning ( Lowe et al. , Foerster et al. , among others) to enable the above use-cases.","title":"Training In Cooperative Multi-Agent Environments with MA-POCA"},{"location":"ML-Agents-Overview/#solving-complex-tasks-using-curriculum-learning","text":"Curriculum learning is a way of training a machine learning model where more difficult aspects of a problem are gradually introduced in such a way that the model is always optimally challenged. This idea has been around for a long time, and it is how we humans typically learn. If you imagine any childhood primary school education, there is an ordering of classes and topics. Arithmetic is taught before algebra, for example. Likewise, algebra is taught before calculus. The skills and knowledge learned in the earlier subjects provide a scaffolding for later lessons. The same principle can be applied to machine learning, where training on easier tasks can provide a scaffolding for harder tasks in the future. Imagine training the medic to scale a wall to arrive at a wounded team member. The starting point when training a medic to accomplish this task will be a random policy. That starting policy will have the medic running in circles, and will likely never, or very rarely scale the wall properly to revive their team member (and achieve the reward). If we start with a simpler task, such as moving toward an unobstructed team member, then the medic can easily learn to accomplish the task. From there, we can slowly add to the difficulty of the task by increasing the size of the wall until the medic can complete the initially near-impossible task of scaling the wall. We have included an environment to demonstrate this with ML-Agents, called Wall Jump . Demonstration of a hypothetical curriculum training scenario in which a progressively taller wall obstructs the path to the goal. [ Note : The example provided above is for instructional purposes, and was based on an early version of the Wall Jump example environment . As such, it is not possible to directly replicate the results here using that environment.] The ML-Agents Toolkit supports modifying custom environment parameters during the training process to aid in learning. This allows elements of the environment related to difficulty or complexity to be dynamically adjusted based on training progress. The Training ML-Agents page has more information on defining training curriculums.","title":"Solving Complex Tasks using Curriculum Learning"},{"location":"ML-Agents-Overview/#training-robust-agents-using-environment-parameter-randomization","text":"An agent trained on a specific environment, may be unable to generalize to any tweaks or variations in the environment (in machine learning this is referred to as overfitting). This becomes problematic in cases where environments are instantiated with varying objects or properties. One mechanism to alleviate this and train more robust agents that can generalize to unseen variations of the environment is to expose them to these variations during training. Similar to Curriculum Learning, where environments become more difficult as the agent learns, the ML-Agents Toolkit provides a way to randomly sample parameters of the environment during training. We refer to this approach as Environment Parameter Randomization . For those familiar with Reinforcement Learning research, this approach is based on the concept of Domain Randomization . By using parameter randomization during training , the agent can be better suited to adapt (with higher performance) to future unseen variations of the environment. Ball scale of 0.5 Ball scale of 4 Example of variations of the 3D Ball environment. The environment parameters are gravity , ball_mass and ball_scale .","title":"Training Robust Agents using Environment Parameter Randomization"},{"location":"ML-Agents-Overview/#model-types","text":"Regardless of the training method deployed, there are a few model types that users can train using the ML-Agents Toolkit. This is due to the flexibility in defining agent observations, which include vector, ray cast and visual observations. You can learn more about how to instrument an agent's observation in the Designing Agents guide.","title":"Model Types"},{"location":"ML-Agents-Overview/#learning-from-vector-observations","text":"Whether an agent's observations are ray cast or vector, the ML-Agents Toolkit provides a fully connected neural network model to learn from those observations. At training time you can configure different aspects of this model such as the number of hidden units and number of layers.","title":"Learning from Vector Observations"},{"location":"ML-Agents-Overview/#learning-from-cameras-using-convolutional-neural-networks","text":"Unlike other platforms, where the agent\u2019s observation might be limited to a single vector or image, the ML-Agents Toolkit allows multiple cameras to be used for observations per agent. This enables agents to learn to integrate information from multiple visual streams. This can be helpful in several scenarios such as training a self-driving car which requires multiple cameras with different viewpoints, or a navigational agent which might need to integrate aerial and first-person visuals. You can learn more about adding visual observations to an agent here . When visual observations are utilized, the ML-Agents Toolkit leverages convolutional neural networks (CNN) to learn from the input images. We offer three network architectures: a simple encoder which consists of two convolutional layers the implementation proposed by Mnih et al. , consisting of three convolutional layers, the IMPALA Resnet consisting of three stacked layers, each with two residual blocks, making a much larger network than the other two. The choice of the architecture depends on the visual complexity of the scene and the available computational resources.","title":"Learning from Cameras using Convolutional Neural Networks"},{"location":"ML-Agents-Overview/#learning-from-variable-length-observations-using-attention","text":"Using the ML-Agents Toolkit, it is possible to have agents learn from a varying number of inputs. To do so, each agent can keep track of a buffer of vector observations. At each step, the agent will go through all the elements in the buffer and extract information but the elements in the buffer can change at every step. This can be useful in scenarios in which the agents must keep track of a varying number of elements throughout the episode. For example in a game where an agent must learn to avoid projectiles, but the projectiles can vary in numbers. You can learn more about variable length observations here . When variable length observations are utilized, the ML-Agents Toolkit leverages attention networks to learn from a varying number of entities. Agents using attention will ignore entities that are deemed not relevant and pay special attention to entities relevant to the current situation based on context.","title":"Learning from Variable Length Observations using Attention"},{"location":"ML-Agents-Overview/#memory-enhanced-agents-using-recurrent-neural-networks","text":"Have you ever entered a room to get something and immediately forgot what you were looking for? Don't let that happen to your agents. In some scenarios, agents must learn to remember the past in order to take the best decision. When an agent only has partial observability of the environment, keeping track of past observations can help the agent learn. Deciding what the agents should remember in order to solve a task is not easy to do by hand, but our training algorithms can learn to keep track of what is important to remember with LSTM .","title":"Memory-enhanced Agents using Recurrent Neural Networks"},{"location":"ML-Agents-Overview/#additional-features","text":"Beyond the flexible training scenarios available, the ML-Agents Toolkit includes additional features which improve the flexibility and interpretability of the training process. Concurrent Unity Instances - We enable developers to run concurrent, parallel instances of the Unity executable during training. For certain scenarios, this should speed up training. Check out our dedicated page on creating a Unity executable and the Training ML-Agents page for instructions on how to set the number of concurrent instances. Recording Statistics from Unity - We enable developers to record statistics from within their Unity environments. These statistics are aggregated and generated during the training process. Custom Side Channels - We enable developers to create custom side channels to manage data transfer between Unity and Python that is unique to their training workflow and/or environment. Custom Samplers - We enable developers to create custom sampling methods for Environment Parameter Randomization. This enables users to customize this training method for their particular environment.","title":"Additional Features"},{"location":"ML-Agents-Overview/#summary-and-next-steps","text":"To briefly summarize: The ML-Agents Toolkit enables games and simulations built in Unity to serve as the platform for training intelligent agents. It is designed to enable a large variety of training modes and scenarios and comes packed with several features to enable researchers and developers to leverage (and enhance) machine learning within Unity. In terms of next steps: For a walkthrough of running ML-Agents with a simple scene, check out the Getting Started guide. For a \"Hello World\" introduction to creating your own Learning Environment, check out the Making a New Learning Environment page. For an overview on the more complex example environments that are provided in this toolkit, check out the Example Environments page. For more information on the various training options available, check out the Training ML-Agents page.","title":"Summary and Next Steps"},{"location":"Migrating/","text":"Upgrading Migrating Migrating the package to version 2.0 The official version of Unity ML-Agents supports is now 2020.3 LTS. If you run into issues, please consider deleting your project's Library folder and reponening your project. If you used any of the APIs that were deprecated before version 2.0, you need to use their replacement. These deprecated APIs have been removed. See the migration steps bellow for specific API replacements. Deprecated methods removed Deprecated API Suggested Replacement IActuator ActuatorComponent.CreateActuator() IActuator[] ActuatorComponent.CreateActuators() IActionReceiver.PackActions(in float[] destination) none Agent.CollectDiscreteActionMasks(DiscreteActionMasker actionMasker) Agent.WriteDiscreteActionMask(IDiscreteActionMask actionMask) Agent.Heuristic(float[] actionsOut) Agent.Heuristic(in ActionBuffers actionsOut) Agent.OnActionReceived(float[] vectorAction) Agent.OnActionReceived(ActionBuffers actions) Agent.GetAction() Agent.GetStoredActionBuffers() BrainParameters.SpaceType , VectorActionSize , VectorActionSpaceType , and NumActions BrainParameters.ActionSpec ObservationWriter.AddRange(IEnumerable<float> data, int writeOffset = 0) ObservationWriter. AddList(IList<float> data, int writeOffset = 0 SensorComponent.IsVisual() and IsVector() none VectorSensor.AddObservation(IEnumerable<float> observation) VectorSensor.AddObservation(IList<float> observation) SideChannelsManager SideChannelManager IDiscreteActionMask changes The interface for disabling specific discrete actions has changed. IDiscreteActionMask.WriteMask() was removed, and replaced with SetActionEnabled() . Instead of returning an IEnumerable with indices to disable, you can now call SetActionEnabled for each index to disable (or enable). As an example, if you overrode Agent.WriteDiscreteActionMask() with something that looked like: public override void WriteDiscreteActionMask(IDiscreteActionMask actionMask) { var branch = 2; var actionsToDisable = new[] {1, 3}; actionMask.WriteMask(branch, actionsToDisable); } the equivalent code would now be public override void WriteDiscreteActionMask(IDiscreteActionMask actionMask) { var branch = 2; actionMask.SetActionEnabled(branch, 1, false); actionMask.SetActionEnabled(branch, 3, false); } IActuator changes The IActuator interface now implements IHeuristicProvider . Please add the corresponding Heuristic(in ActionBuffers) method to your custom Actuator classes. ISensor and SensorComponent changes The ISensor.GetObservationShape() method and ITypedSensor and IDimensionPropertiesSensor interfaces were removed, and GetObservationSpec() was added. You can use ObservationSpec.Vector() or ObservationSpec.Visual() to generate ObservationSpec s that are equivalent to the previous shape. For example, if your old ISensor looked like: public override int[] GetObservationShape() { return new[] { m_Height, m_Width, m_NumChannels }; } the equivalent code would now be public override ObservationSpec GetObservationSpec() { return ObservationSpec.Visual(m_Height, m_Width, m_NumChannels); } The ISensor.GetCompressionType() method and ISparseChannelSensor interface was removed, and GetCompressionSpec() was added. You can use CompressionSpec.Default() or CompressionSpec.Compressed() to generate CompressionSpec s that are equivalent to the previous values. For example, if your old ISensor looked like: ```csharp public virtual SensorCompressionType GetCompressionType() { return SensorCompressionType.None; } the equivalent code would now be ```csharp public CompressionSpec GetCompressionSpec() { return CompressionSpec.Default(); } The abstract method SensorComponent.GetObservationShape() was removed. The abstract method SensorComponent.CreateSensor() was replaced with CreateSensors() , which returns an ISensor[] . Match3 integration changes The Match-3 integration utilities were moved from com.unity.ml-agents.extensions to com.unity.ml-agents . The AbstractBoard interface was changed: * AbstractBoard no longer contains Rows , Columns , NumCellTypes , and NumSpecialTypes fields. * public abstract BoardSize GetMaxBoardSize() was added as an abstract method. BoardSize is a new struct that contains Rows , Columns , NumCellTypes , and NumSpecialTypes fields, with the same meanings as the old AbstractBoard fields. * public virtual BoardSize GetCurrentBoardSize() is an optional method; by default it returns GetMaxBoardSize() . If you wish to use a single behavior to work with multiple board sizes, override GetCurrentBoardSize() to return the current BoardSize . The values returned by GetCurrentBoardSize() must be less than or equal to the corresponding values from GetMaxBoardSize() . GridSensor changes The sensor configuration has changed: * The sensor implementation has been refactored and exsisting GridSensor created from extension package will not work in newer version. Some errors might show up when loading the old sensor in the scene. You'll need to remove the old sensor and create a new GridSensor. * These parameters names have changed but still refer to the same concept in the sensor: GridNumSide -> GridSize , RotateToAgent -> RotateWithAgent , ObserveMask -> ColliderMask , DetectableObjects -> DetectableTags * DepthType ( ChanelBase / ChannelHot ) option and ChannelDepth are removed. Now the default is one-hot encoding for detected tag. If you were using original GridSensor without overriding any method, switching to new GridSensor will produce similar effect for training although the actual observations will be slightly different. For creating your GridSensor implementation with custom data: * To create custom GridSensor, derive from GridSensorBase instead of GridSensor . Besides overriding GetObjectData() , you will also need to consider override GetCellObservationSize() , IsDataNormalized() and GetProcessCollidersMethod() according to the data you collect. Also you'll need to override GridSensorComponent.GetGridSensors() and return your custom GridSensor. * The input argument tagIndex in GetObjectData() has changed from 1-indexed to 0-indexed and the data type changed from float to int . The index of first detectable tag will be 0 instead of 1. normalizedDistance was removed from input. * The observation data should be written to the input dataBuffer instead of creating and returning a new array. * Removed the constraint of all data required to be normalized. You should specify it in IsDataNormalized() . Sensors with non-normalized data cannot use PNG compression type. * The sensor will not further encode the data recieved from GetObjectData() anymore. The values recieved from GetObjectData() will be the observation sent to the trainer. LSTM models from previous releases no longer supported The way the Unity Inference Engine processes LSTM (recurrent neural networks) has changed. As a result, models trained with previous versions of ML-Agents will not be usable at inference if they were trained with a memory setting in the .yaml config file. If you want to use a model that has a recurrent neural network in this release of ML-Agents, you need to train the model using the python trainer from this release. Migrating to Release 13 Implementing IHeuristic in your IActuator implementations If you have any custom actuators, you can now implement the IHeuristicProvider interface to have your actuator handle the generation of actions when an Agent is running in heuristic mode. VectorSensor.AddObservation(IEnumerable<float>) is deprecated. Use VectorSensor.AddObservation(IList<float>) instead. ObservationWriter.AddRange() is deprecated. Use ObservationWriter.AddList() instead. ActuatorComponent.CreateAcuator() is deprecated. Please use override ActuatorComponent.CreateActuators instead. Since ActuatorComponent.CreateActuator() is abstract, you will still need to override it in your class until it is removed. It is only ever called if you don't override ActuatorComponent.CreateActuators . You can suppress the warnings by surrounding the method with the following pragma: c# #pragma warning disable 672 public IActuator CreateActuator() { ... } #pragma warning restore 672 Migrating Migrating to Release 11 Agent virtual method deprecation Agent.CollectDiscreteActionMasks() was deprecated and should be replaced with Agent.WriteDiscreteActionMask() Agent.Heuristic(float[]) was deprecated and should be replaced with Agent.Heuristic(ActionBuffers) . Agent.OnActionReceived(float[]) was deprecated and should be replaced with Agent.OnActionReceived(ActionBuffers) . Agent.GetAction() was deprecated and should be replaced with Agent.GetStoredActionBuffers() . The default implementation of these will continue to call the deprecated versions where appropriate. However, the deprecated versions may not be compatible with continuous and discrete actions on the same Agent. BrainParameters field and method deprecation BrainParameters.VectorActionSize was deprecated; you can now set BrainParameters.ActionSpec.NumContinuousActions or BrainParameters.ActionSpec.BranchSizes instead. BrainParameters.VectorActionSpaceType was deprecated, since both continuous and discrete actions can now be used. BrainParameters.NumActions() was deprecated. Use BrainParameters.ActionSpec.NumContinuousActions and BrainParameters.ActionSpec.NumDiscreteActions instead. Migrating from Release 7 to latest Important changes Some trainer files were moved. If you were using the TrainerFactory class, it was moved to the trainers/trainer folder. The components folder containing bc and reward_signals code was moved to the trainers/tf folder Steps to Migrate Replace calls to from mlagents.trainers.trainer_util import TrainerFactory to from mlagents.trainers.trainer import TrainerFactory Replace calls to from mlagents.trainers.trainer_util import handle_existing_directories to from mlagents.trainers.directory_utils import validate_existing_directories Replace mlagents.trainers.components with mlagents.trainers.tf.components in your import statements. Migrating from Release 3 to Release 7 Important changes The Parameter Randomization feature has been merged with the Curriculum feature. It is now possible to specify a sampler in the lesson of a Curriculum. Curriculum has been refactored and is now specified at the level of the parameter, not the behavior. More information here .(#4160) Steps to Migrate The configuration format for curriculum and parameter randomization has changed. To upgrade your configuration files, an upgrade script has been provided. Run python -m mlagents.trainers.upgrade_config -h to see the script usage. Note that you will have had to upgrade to/install the current version of ML-Agents before running the script. To update manually: If your config file used a parameter_randomization section, rename that section to environment_parameters If your config file used a curriculum section, you will need to rewrite your curriculum with this format . Migrating from Release 1 to Release 3 Important changes Training artifacts (trained models, summaries) are now found under results/ instead of summaries/ and models/ . Trainer configuration, curriculum configuration, and parameter randomization configuration have all been moved to a single YAML file. (#3791) Trainer configuration format has changed, and using a \"default\" behavior name has been deprecated. (#3936) max_step in the TerminalStep and TerminalSteps objects was renamed interrupted . On the UnityEnvironment API, get_behavior_names() and get_behavior_specs() methods were combined into the property behavior_specs that contains a mapping from behavior names to behavior spec. use_visual and allow_multiple_visual_obs in the UnityToGymWrapper constructor were replaced by allow_multiple_obs which allows one or more visual observations and vector observations to be used simultaneously. --save-freq has been removed from the CLI and is now configurable in the trainer configuration file. --lesson has been removed from the CLI. Lessons will resume when using --resume . To start at a different lesson, modify your Curriculum configuration. Steps to Migrate To upgrade your configuration files, an upgrade script has been provided. Run python -m mlagents.trainers.upgrade_config -h to see the script usage. Note that you will have had to upgrade to/install the current version of ML-Agents before running the script. To do it manually, copy your <BehaviorName> sections from trainer_config.yaml into a separate trainer configuration file, under a behaviors section. The default section is no longer needed. This new file should be specific to your environment, and not contain configurations for multiple environments (unless they have the same Behavior Names). - You will need to reformat your trainer settings as per the example . - If your training uses curriculum , move those configurations under a curriculum section. - If your training uses parameter randomization , move the contents of the sampler config to parameter_randomization in the main trainer configuration. - If you are using UnityEnvironment directly, replace max_step with interrupted in the TerminalStep and TerminalSteps objects. - Replace usage of get_behavior_names() and get_behavior_specs() in UnityEnvironment with behavior_specs . - If you use the UnityToGymWrapper , remove use_visual and allow_multiple_visual_obs from the constructor and add allow_multiple_obs = True if the environment contains either both visual and vector observations or multiple visual observations. - If you were setting --save-freq in the CLI, add a checkpoint_interval value in your trainer configuration, and set it equal to save-freq * n_agents_in_scene . Migrating from 0.15 to Release 1 Important changes The MLAgents C# namespace was renamed to Unity.MLAgents , and other nested namespaces were similarly renamed (#3843). The --load and --train command-line flags have been deprecated and replaced with --resume and --inference . Running with the same --run-id twice will now throw an error. The play_against_current_self_ratio self-play trainer hyperparameter has been renamed to play_against_latest_model_ratio Removed the multi-agent gym option from the gym wrapper. For multi-agent scenarios, use the Low Level Python API . The low level Python API has changed. You can look at the document Low Level Python API documentation for more information. If you use mlagents-learn for training, this should be a transparent change. The obsolete Agent methods GiveModel , Done , InitializeAgent , AgentAction and AgentReset have been removed. The signature of Agent.Heuristic() was changed to take a float[] as a parameter, instead of returning the array. This was done to prevent a common source of error where users would return arrays of the wrong size. The SideChannel API has changed (#3833, #3660) : Introduced the SideChannelManager to register, unregister and access side channels. EnvironmentParameters replaces the default FloatProperties . You can access the EnvironmentParameters with Academy.Instance.EnvironmentParameters on C#. If you were previously creating a UnityEnvironment in python and passing it a FloatPropertiesChannel , create an EnvironmentParametersChannel instead. SideChannel.OnMessageReceived is now a protected method (was public) SideChannel IncomingMessages methods now take an optional default argument, which is used when trying to read more data than the message contains. Added a feature to allow sending stats from C# environments to TensorBoard (and other python StatsWriters). To do this from your code, use Academy.Instance.StatsRecorder.Add(key, value) (#3660) num_updates and train_interval for SAC have been replaced with steps_per_update . The UnityEnv class from the gym-unity package was renamed UnityToGymWrapper and no longer creates the UnityEnvironment . Instead, the UnityEnvironment must be passed as input to the constructor of UnityToGymWrapper Public fields and properties on several classes were renamed to follow Unity's C# style conventions. All public fields and properties now use \"PascalCase\" instead of \"camelCase\"; for example, Agent.maxStep was renamed to Agent.MaxStep . For a full list of changes, see the pull request. (#3828) WriteAdapter was renamed to ObservationWriter . (#3834) Steps to Migrate In C# code, replace using MLAgents with using Unity.MLAgents . Replace other nested namespaces such as using MLAgents.Sensors with using Unity.MLAgents.Sensors Replace the --load flag with --resume when calling mlagents-learn , and don't use the --train flag as training will happen by default. To run without training, use --inference . To force-overwrite files from a pre-existing run, add the --force command-line flag. The Jupyter notebooks have been removed from the repository. If your Agent class overrides Heuristic() , change the signature to public override void Heuristic(float[] actionsOut) and assign values to actionsOut instead of returning an array. If you used SideChannels you must: Replace Academy.FloatProperties with Academy.Instance.EnvironmentParameters . Academy.RegisterSideChannel and Academy.UnregisterSideChannel were removed. Use SideChannelManager.RegisterSideChannel and SideChannelManager.UnregisterSideChannel instead. Set steps_per_update to be around equal to the number of agents in your environment, times num_updates and divided by train_interval . Replace UnityEnv with UnityToGymWrapper in your code. The constructor no longer takes a file name as input but a fully constructed UnityEnvironment instead. Update uses of \"camelCase\" fields and properties to \"PascalCase\". Migrating from 0.14 to 0.15 Important changes The Agent.CollectObservations() virtual method now takes as input a VectorSensor sensor as argument. The Agent.AddVectorObs() methods were removed. The SetMask was renamed to SetMask method must now be called on the DiscreteActionMasker argument of the CollectDiscreteActionMasks virtual method. We consolidated our API for DiscreteActionMasker . SetMask takes two arguments : the branch index and the list of masked actions for that branch. The Monitor class has been moved to the Examples Project. (It was prone to errors during testing) The MLAgents.Sensors namespace has been introduced. All sensors classes are part of the MLAgents.Sensors namespace. The MLAgents.SideChannels namespace has been introduced. All side channel classes are part of the MLAgents.SideChannels namespace. The interface for RayPerceptionSensor.PerceiveStatic() was changed to take an input class and write to an output class, and the method was renamed to Perceive() . The SetMask method must now be called on the DiscreteActionMasker argument of the CollectDiscreteActionMasks method. The method GetStepCount() on the Agent class has been replaced with the property getter StepCount The --multi-gpu option has been removed temporarily. AgentInfo.actionMasks has been renamed to AgentInfo.discreteActionMasks . BrainParameters and SpaceType have been removed from the public API BehaviorParameters have been removed from the public API. DecisionRequester has been made internal (you can still use the DecisionRequesterComponent from the inspector). RepeatAction was renamed TakeActionsBetweenDecisions for clarity. The following methods in the Agent class have been renamed. The original method names will be removed in a later release: InitializeAgent() was renamed to Initialize() AgentAction() was renamed to OnActionReceived() AgentReset() was renamed to OnEpsiodeBegin() Done() was renamed to EndEpisode() GiveModel() was renamed to SetModel() The IFloatProperties interface has been removed. The interface for SideChannels was changed: In C#, OnMessageReceived now takes a IncomingMessage argument, and QueueMessageToSend takes an OutgoingMessage argument. In python, on_message_received now takes a IncomingMessage argument, and queue_message_to_send takes an OutgoingMessage argument. Automatic stepping for Academy is now controlled from the AutomaticSteppingEnabled property. Steps to Migrate Add the using MLAgents.Sensors; in addition to using MLAgents; on top of your Agent's script. Replace your Agent's implementation of CollectObservations() with CollectObservations(VectorSensor sensor) . In addition, replace all calls to AddVectorObs() with sensor.AddObservation() or sensor.AddOneHotObservation() on the VectorSensor passed as argument. Replace your calls to SetActionMask on your Agent to DiscreteActionMasker.SetActionMask in CollectDiscreteActionMasks . If you call RayPerceptionSensor.PerceiveStatic() manually, add your inputs to a RayPerceptionInput . To get the previous float array output, iterate through RayPerceptionOutput.rayOutputs and call RayPerceptionOutput.RayOutput.ToFloatArray() . Replace all calls to Agent.GetStepCount() with Agent.StepCount We strongly recommend replacing the following methods with their new equivalent as they will be removed in a later release: InitializeAgent() to Initialize() AgentAction() to OnActionReceived() AgentReset() to OnEpisodeBegin() Done() to EndEpisode() GiveModel() to SetModel() Replace IFloatProperties variables with FloatPropertiesChannel variables. If you implemented custom SideChannels , update the signatures of your methods, and add your data to the OutgoingMessage or read it from the IncomingMessage . Replace calls to Academy.EnableAutomaticStepping()/DisableAutomaticStepping() with Academy.AutomaticSteppingEnabled = true/false. Migrating from 0.13 to 0.14 Important changes The UnitySDK folder has been split into a Unity Package ( com.unity.ml-agents ) and an examples project ( Project ). Please follow the Installation Guide to get up and running with this new repo structure. Several changes were made to how agents are reset and marked as done: Calling Done() on the Agent will now reset it immediately and call the AgentReset virtual method. (This is to simplify the previous logic in which the Agent had to wait for the next EnvironmentStep to reset) The \"Reset on Done\" setting in AgentParameters was removed; this is now effectively always true. AgentOnDone virtual method on the Agent has been removed. The Decision Period and On Demand decision checkbox have been removed from the Agent. On demand decision is now the default (calling RequestDecision on the Agent manually.) The Academy class was changed to a singleton, and its virtual methods were removed. Trainer steps are now counted per-Agent, not per-environment as in previous versions. For instance, if you have 10 Agents in the scene, 20 environment steps now corresponds to 200 steps as printed in the terminal and in Tensorboard. Curriculum config files are now YAML formatted and all curricula for a training run are combined into a single file. The --num-runs command-line option has been removed from mlagents-learn . Several fields on the Agent were removed or made private in order to simplify the interface. The agentParameters field of the Agent has been removed. (Contained only maxStep information) maxStep is now a public field on the Agent. (Was moved from agentParameters ) The Info field of the Agent has been made private. (Was only used internally and not meant to be modified outside of the Agent) The GetReward() method on the Agent has been removed. (It was being confused with GetCumulativeReward() ) The AgentAction struct no longer contains a value field. (Value estimates were not set during inference) The GetValueEstimate() method on the Agent has been removed. The UpdateValueAction() method on the Agent has been removed. The deprecated RayPerception3D and RayPerception2D classes were removed, and the legacyHitFractionBehavior argument was removed from RayPerceptionSensor.PerceiveStatic() . RayPerceptionSensor was inconsistent in how it handle scale on the Agent's transform. It now scales the ray length and sphere size for casting as the transform's scale changes. Steps to Migrate Follow the instructions on how to install the com.unity.ml-agents package into your project in the Installation Guide . If your Agent implemented AgentOnDone and did not have the checkbox Reset On Done checked in the inspector, you must call the code that was in AgentOnDone manually. If you give your Agent a reward or penalty at the end of an episode (e.g. for reaching a goal or falling off of a platform), make sure you call AddReward() or SetReward() before calling Done() . Previously, the order didn't matter. If you were not using On Demand Decision for your Agent, you must add a DecisionRequester component to your Agent GameObject and set its Decision Period field to the old Decision Period of the Agent. If you have a class that inherits from Academy: If the class didn't override any of the virtual methods and didn't store any additional data, you can just remove the old script from the scene. If the class had additional data, create a new MonoBehaviour and store the data in the new MonoBehaviour instead. If the class overrode the virtual methods, create a new MonoBehaviour and move the logic to it: Move the InitializeAcademy code to MonoBehaviour.Awake Move the AcademyStep code to MonoBehaviour.FixedUpdate Move the OnDestroy code to MonoBehaviour.OnDestroy. Move the AcademyReset code to a new method and add it to the Academy.OnEnvironmentReset action. Multiply max_steps and summary_freq in your trainer_config.yaml by the number of Agents in the scene. Combine curriculum configs into a single file. See the WallJump curricula for an example of the new curriculum config format. A tool like https://www.json2yaml.com may be useful to help with the conversion. If you have a model trained which uses RayPerceptionSensor and has non-1.0 scale in the Agent's transform, it must be retrained. Migrating from ML-Agents Toolkit v0.12.0 to v0.13.0 Important changes The low level Python API has changed. You can look at the document Low Level Python API documentation for more information. This should only affect you if you're writing a custom trainer; if you use mlagents-learn for training, this should be a transparent change. reset() on the Low-Level Python API no longer takes a train_mode argument. To modify the performance/speed of the engine, you must use an EngineConfigurationChannel reset() on the Low-Level Python API no longer takes a config argument. UnityEnvironment no longer has a reset_parameters field. To modify float properties in the environment, you must use a FloatPropertiesChannel . For more information, refer to the Low Level Python API documentation CustomResetParameters are now removed. The Academy no longer has a Training Configuration nor Inference Configuration field in the inspector. To modify the configuration from the Low-Level Python API, use an EngineConfigurationChannel . To modify it during training, use the new command line arguments --width , --height , --quality-level , --time-scale and --target-frame-rate in mlagents-learn . The Academy no longer has a Default Reset Parameters field in the inspector. The Academy class no longer has a ResetParameters . To access shared float properties with Python, use the new FloatProperties field on the Academy. Offline Behavioral Cloning has been removed. To learn from demonstrations, use the GAIL and Behavioral Cloning features with either PPO or SAC. mlagents.envs was renamed to mlagents_envs . The previous repo layout depended on PEP420 , which caused problems with some of our tooling such as mypy and pylint. The official version of Unity ML-Agents supports is now 2018.4 LTS. If you run into issues, please consider deleting your library folder and reponening your projects. You will need to install the Barracuda package into your project in order to ML-Agents to compile correctly. Steps to Migrate If you had a custom Training Configuration in the Academy inspector, you will need to pass your custom configuration at every training run using the new command line arguments --width , --height , --quality-level , --time-scale and --target-frame-rate . If you were using --slow in mlagents-learn , you will need to pass your old Inference Configuration of the Academy inspector with the new command line arguments --width , --height , --quality-level , --time-scale and --target-frame-rate instead. Any imports from mlagents.envs should be replaced with mlagents_envs . Migrating from ML-Agents Toolkit v0.11.0 to v0.12.0 Important Changes Text actions and observations, and custom action and observation protos have been removed. RayPerception3D and RayPerception2D are marked deprecated, and will be removed in a future release. They can be replaced by RayPerceptionSensorComponent3D and RayPerceptionSensorComponent2D. The Use Heuristic checkbox in Behavior Parameters has been replaced with a Behavior Type dropdown menu. This has the following options: Default corresponds to the previous unchecked behavior, meaning that Agents will train if they connect to a python trainer, otherwise they will perform inference. Heuristic Only means the Agent will always use the Heuristic() method. This corresponds to having \"Use Heuristic\" selected in 0.11.0. Inference Only means the Agent will always perform inference. Barracuda was upgraded to 0.3.2, and it is now installed via the Unity Package Manager. Steps to Migrate We fixed a bug in RayPerception3d.Perceive() that was causing the endOffset to be used incorrectly. However this may produce different behavior from previous versions if you use a non-zero startOffset . To reproduce the old behavior, you should increase the value of endOffset by startOffset . You can verify your raycasts are performing as expected in scene view using the debug rays. If you use RayPerception3D, replace it with RayPerceptionSensorComponent3D (and similarly for 2D). The settings, such as ray angles and detectable tags, are configured on the component now. RayPerception3D would contribute (# of rays) * (# of tags + 2) to the State Size in Behavior Parameters, but this is no longer necessary, so you should reduce the State Size by this amount. Making this change will require retraining your model, since the observations that RayPerceptionSensorComponent3D produces are different from the old behavior. If you see messages such as The type or namespace 'Barracuda' could not be found or The type or namespace 'Google' could not be found , you will need to install the Barracuda preview package . Migrating from ML-Agents Toolkit v0.10 to v0.11.0 Important Changes The definition of the gRPC service has changed. The online BC training feature has been removed. The BroadcastHub has been deprecated. If there is a training Python process, all LearningBrains in the scene will automatically be trained. If there is no Python process, inference will be used. The Brain ScriptableObjects have been deprecated. The Brain Parameters are now on the Agent and are referred to as Behavior Parameters. Make sure the Behavior Parameters is attached to the Agent GameObject. To use a heuristic behavior, implement the Heuristic() method in the Agent class and check the use heuristic checkbox in the Behavior Parameters. Several changes were made to the setup for visual observations (i.e. using Cameras or RenderTextures): Camera resolutions are no longer stored in the Brain Parameters. AgentParameters no longer stores lists of Cameras and RenderTextures To add visual observations to an Agent, you must now attach a CameraSensorComponent or RenderTextureComponent to the agent. The corresponding Camera or RenderTexture can be added to these in the editor, and the resolution and color/grayscale is configured on the component itself. Steps to Migrate In order to be able to train, make sure both your ML-Agents Python package and UnitySDK code come from the v0.11 release. Training will not work, for example, if you update the ML-Agents Python package, and only update the API Version in UnitySDK. If your Agents used visual observations, you must add a CameraSensorComponent corresponding to each old Camera in the Agent's camera list (and similarly for RenderTextures). Since Brain ScriptableObjects have been removed, you will need to delete all the Brain ScriptableObjects from your Assets folder. Then, add a Behavior Parameters component to each Agent GameObject. You will then need to complete the fields on the new Behavior Parameters component with the BrainParameters of the old Brain. Migrating from ML-Agents Toolkit v0.9 to v0.10 Important Changes We have updated the C# code in our repository to be in line with Unity Coding Conventions. This has changed the name of some public facing classes and enums. The example environments have been updated. If you were using these environments to benchmark your training, please note that the resulting rewards may be slightly different in v0.10. Steps to Migrate UnitySDK/Assets/ML-Agents/Scripts/Communicator.cs and its class Communicator have been renamed to UnitySDK/Assets/ML-Agents/Scripts/ICommunicator.cs and ICommunicator respectively. The SpaceType Enums discrete , and continuous have been renamed to Discrete and Continuous . We have removed the Done call as well as the capacity to set Max Steps on the Academy. Therefore an AcademyReset will never be triggered from C# (only from Python). If you want to reset the simulation after a fixed number of steps, or when an event in the simulation occurs, we recommend looking at our multi-agent example environments (such as FoodCollector). In our examples, groups of Agents can be reset through an \"Area\" that can reset groups of Agents. The import for mlagents.envs.UnityEnvironment was removed. If you are using the Python API, change from mlagents_envs import UnityEnvironment to from mlagents_envs.environment import UnityEnvironment . Migrating from ML-Agents Toolkit v0.8 to v0.9 Important Changes We have changed the way reward signals (including Curiosity) are defined in the trainer_config.yaml . When using multiple environments, every \"step\" is recorded in TensorBoard. The steps in the command line console corresponds to a single step of a single environment. Previously, each step corresponded to one step for all environments (i.e., num_envs steps). Steps to Migrate If you were overriding any of these following parameters in your config file, remove them from the top-level config and follow the steps below: gamma : Define a new extrinsic reward signal and set it's gamma to your new gamma. use_curiosity , curiosity_strength , curiosity_enc_size : Define a curiosity reward signal and set its strength to curiosity_strength , and encoding_size to curiosity_enc_size . Give it the same gamma as your extrinsic signal to mimic previous behavior. TensorBoards generated when running multiple environments in v0.8 are not comparable to those generated in v0.9 in terms of step count. Multiply your v0.8 step count by num_envs for an approximate comparison. You may need to change max_steps in your config as appropriate as well. Migrating from ML-Agents Toolkit v0.7 to v0.8 Important Changes We have split the Python packages into two separate packages ml-agents and ml-agents-envs . --worker-id option of learn.py has been removed, use --base-port instead if you'd like to run multiple instances of learn.py . Steps to Migrate If you are installing via PyPI, there is no change. If you intend to make modifications to ml-agents or ml-agents-envs please check the Installing for Development in the Installation documentation . Migrating from ML-Agents Toolkit v0.6 to v0.7 Important Changes We no longer support TFS and are now using the Unity Inference Engine Steps to Migrate Make sure to remove the ENABLE_TENSORFLOW flag in your Unity Project settings Migrating from ML-Agents Toolkit v0.5 to v0.6 Important Changes Brains are now Scriptable Objects instead of MonoBehaviors. You can no longer modify the type of a Brain. If you want to switch between PlayerBrain and LearningBrain for multiple agents, you will need to assign a new Brain to each agent separately. Note: You can pass the same Brain to multiple agents in a scene by leveraging Unity's prefab system or look for all the agents in a scene using the search bar of the Hierarchy window with the word Agent . We replaced the Internal and External Brain with Learning Brain . When you need to train a model, you need to drag it into the Broadcast Hub inside the Academy and check the Control checkbox. We removed the Broadcast checkbox of the Brain, to use the broadcast functionality, you need to drag the Brain into the Broadcast Hub . When training multiple Brains at the same time, each model is now stored into a separate model file rather than in the same file under different graph scopes. The Learning Brain graph scope, placeholder names, output names and custom placeholders can no longer be modified. Steps to Migrate To update a scene from v0.5 to v0.6, you must: Remove the Brain GameObjects in the scene. (Delete all of the Brain GameObjects under Academy in the scene.) Create new Brain Scriptable Objects using Assets -> Create -> ML-Agents for each type of the Brain you plan to use, and put the created files under a folder called Brains within your project. Edit their Brain Parameters to be the same as the parameters used in the Brain GameObjects. Agents have a Brain field in the Inspector, you need to drag the appropriate Brain ScriptableObject in it. The Academy has a Broadcast Hub field in the inspector, which is list of brains used in the scene. To train or control your Brain from the mlagents-learn Python script, you need to drag the relevant LearningBrain ScriptableObjects used in your scene into entries into this list. Migrating from ML-Agents Toolkit v0.4 to v0.5 Important The Unity project unity-environment has been renamed UnitySDK . The python folder has been renamed to ml-agents . It now contains two packages, mlagents.env and mlagents.trainers . mlagents.env can be used to interact directly with a Unity environment, while mlagents.trainers contains the classes for training agents. The supported Unity version has changed from 2017.1 or later to 2017.4 or later . 2017.4 is an LTS (Long Term Support) version that helps us maintain good quality and support. Earlier versions of Unity might still work, but you may encounter an error listed here. Unity API Discrete Actions now use branches . You can now specify concurrent discrete actions. You will need to update the Brain Parameters in the Brain Inspector in all your environments that use discrete actions. Refer to the discrete action documentation for more information. Python API In order to run a training session, you can now use the command mlagents-learn instead of python3 learn.py after installing the mlagents packages. This change is documented here . For example, if we previously ran sh python3 learn.py 3DBall --train from the python subdirectory (which is changed to ml-agents subdirectory in v0.5), we now run sh mlagents-learn config/trainer_config.yaml --env=3DBall --train from the root directory where we installed the ML-Agents Toolkit. It is now required to specify the path to the yaml trainer configuration file when running mlagents-learn . For an example trainer configuration file, see trainer_config.yaml . An example of passing a trainer configuration to mlagents-learn is shown above. The environment name is now passed through the --env option. Curriculum learning has been changed. In summary: Curriculum files for the same environment must now be placed into a folder. Each curriculum file should be named after the Brain whose curriculum it specifies. min_lesson_length now specifies the minimum number of episodes in a lesson and affects reward thresholding. It is no longer necessary to specify the Max Steps of the Academy to use curriculum learning. Migrating from ML-Agents Toolkit v0.3 to v0.4 Unity API using MLAgents; needs to be added in all of the C# scripts that use ML-Agents. Python API We've changed some of the Python packages dependencies in requirement.txt file. Make sure to run pip3 install -e . within your ml-agents/python folder to update your Python packages. Migrating from ML-Agents Toolkit v0.2 to v0.3 There are a large number of new features and improvements in the ML-Agents toolkit v0.3 which change both the training process and Unity API in ways which will cause incompatibilities with environments made using older versions. This page is designed to highlight those changes for users familiar with v0.1 or v0.2 in order to ensure a smooth transition. Important The ML-Agents Toolkit is no longer compatible with Python 2. Python Training The training script ppo.py and PPO.ipynb Python notebook have been replaced with a single learn.py script as the launching point for training with ML-Agents. For more information on using learn.py , see here . Hyperparameters for training Brains are now stored in the trainer_config.yaml file. For more information on using this file, see here . Unity API Modifications to an Agent's rewards must now be done using either AddReward() or SetReward() . Setting an Agent to done now requires the use of the Done() method. CollectStates() has been replaced by CollectObservations() , which now no longer returns a list of floats. To collect observations, call AddVectorObs() within CollectObservations() . Note that you can call AddVectorObs() with floats, integers, lists and arrays of floats, Vector3 and Quaternions. AgentStep() has been replaced by AgentAction() . WaitTime() has been removed. The Frame Skip field of the Academy is replaced by the Agent's Decision Frequency field, enabling the Agent to make decisions at different frequencies. The names of the inputs in the Internal Brain have been changed. You must replace state with vector_observation and observation with visual_observation . In addition, you must remove the epsilon placeholder. Semantics In order to more closely align with the terminology used in the Reinforcement Learning field, and to be more descriptive, we have changed the names of some of the concepts used in ML-Agents. The changes are highlighted in the table below. Old - v0.2 and earlier New - v0.3 and later State Vector Observation Observation Visual Observation Action Vector Action N/A Text Observation N/A Text Action","title":"Migrating"},{"location":"Migrating/#upgrading","text":"","title":"Upgrading"},{"location":"Migrating/#migrating","text":"","title":"Migrating"},{"location":"Migrating/#migrating-the-package-to-version-20","text":"The official version of Unity ML-Agents supports is now 2020.3 LTS. If you run into issues, please consider deleting your project's Library folder and reponening your project. If you used any of the APIs that were deprecated before version 2.0, you need to use their replacement. These deprecated APIs have been removed. See the migration steps bellow for specific API replacements.","title":"Migrating the package to version 2.0"},{"location":"Migrating/#deprecated-methods-removed","text":"Deprecated API Suggested Replacement IActuator ActuatorComponent.CreateActuator() IActuator[] ActuatorComponent.CreateActuators() IActionReceiver.PackActions(in float[] destination) none Agent.CollectDiscreteActionMasks(DiscreteActionMasker actionMasker) Agent.WriteDiscreteActionMask(IDiscreteActionMask actionMask) Agent.Heuristic(float[] actionsOut) Agent.Heuristic(in ActionBuffers actionsOut) Agent.OnActionReceived(float[] vectorAction) Agent.OnActionReceived(ActionBuffers actions) Agent.GetAction() Agent.GetStoredActionBuffers() BrainParameters.SpaceType , VectorActionSize , VectorActionSpaceType , and NumActions BrainParameters.ActionSpec ObservationWriter.AddRange(IEnumerable<float> data, int writeOffset = 0) ObservationWriter. AddList(IList<float> data, int writeOffset = 0 SensorComponent.IsVisual() and IsVector() none VectorSensor.AddObservation(IEnumerable<float> observation) VectorSensor.AddObservation(IList<float> observation) SideChannelsManager SideChannelManager","title":"Deprecated methods removed"},{"location":"Migrating/#idiscreteactionmask-changes","text":"The interface for disabling specific discrete actions has changed. IDiscreteActionMask.WriteMask() was removed, and replaced with SetActionEnabled() . Instead of returning an IEnumerable with indices to disable, you can now call SetActionEnabled for each index to disable (or enable). As an example, if you overrode Agent.WriteDiscreteActionMask() with something that looked like: public override void WriteDiscreteActionMask(IDiscreteActionMask actionMask) { var branch = 2; var actionsToDisable = new[] {1, 3}; actionMask.WriteMask(branch, actionsToDisable); } the equivalent code would now be public override void WriteDiscreteActionMask(IDiscreteActionMask actionMask) { var branch = 2; actionMask.SetActionEnabled(branch, 1, false); actionMask.SetActionEnabled(branch, 3, false); }","title":"IDiscreteActionMask changes"},{"location":"Migrating/#iactuator-changes","text":"The IActuator interface now implements IHeuristicProvider . Please add the corresponding Heuristic(in ActionBuffers) method to your custom Actuator classes.","title":"IActuator changes"},{"location":"Migrating/#isensor-and-sensorcomponent-changes","text":"The ISensor.GetObservationShape() method and ITypedSensor and IDimensionPropertiesSensor interfaces were removed, and GetObservationSpec() was added. You can use ObservationSpec.Vector() or ObservationSpec.Visual() to generate ObservationSpec s that are equivalent to the previous shape. For example, if your old ISensor looked like: public override int[] GetObservationShape() { return new[] { m_Height, m_Width, m_NumChannels }; } the equivalent code would now be public override ObservationSpec GetObservationSpec() { return ObservationSpec.Visual(m_Height, m_Width, m_NumChannels); } The ISensor.GetCompressionType() method and ISparseChannelSensor interface was removed, and GetCompressionSpec() was added. You can use CompressionSpec.Default() or CompressionSpec.Compressed() to generate CompressionSpec s that are equivalent to the previous values. For example, if your old ISensor looked like: ```csharp public virtual SensorCompressionType GetCompressionType() { return SensorCompressionType.None; } the equivalent code would now be ```csharp public CompressionSpec GetCompressionSpec() { return CompressionSpec.Default(); } The abstract method SensorComponent.GetObservationShape() was removed. The abstract method SensorComponent.CreateSensor() was replaced with CreateSensors() , which returns an ISensor[] .","title":"ISensor and SensorComponent changes"},{"location":"Migrating/#match3-integration-changes","text":"The Match-3 integration utilities were moved from com.unity.ml-agents.extensions to com.unity.ml-agents . The AbstractBoard interface was changed: * AbstractBoard no longer contains Rows , Columns , NumCellTypes , and NumSpecialTypes fields. * public abstract BoardSize GetMaxBoardSize() was added as an abstract method. BoardSize is a new struct that contains Rows , Columns , NumCellTypes , and NumSpecialTypes fields, with the same meanings as the old AbstractBoard fields. * public virtual BoardSize GetCurrentBoardSize() is an optional method; by default it returns GetMaxBoardSize() . If you wish to use a single behavior to work with multiple board sizes, override GetCurrentBoardSize() to return the current BoardSize . The values returned by GetCurrentBoardSize() must be less than or equal to the corresponding values from GetMaxBoardSize() .","title":"Match3 integration changes"},{"location":"Migrating/#gridsensor-changes","text":"The sensor configuration has changed: * The sensor implementation has been refactored and exsisting GridSensor created from extension package will not work in newer version. Some errors might show up when loading the old sensor in the scene. You'll need to remove the old sensor and create a new GridSensor. * These parameters names have changed but still refer to the same concept in the sensor: GridNumSide -> GridSize , RotateToAgent -> RotateWithAgent , ObserveMask -> ColliderMask , DetectableObjects -> DetectableTags * DepthType ( ChanelBase / ChannelHot ) option and ChannelDepth are removed. Now the default is one-hot encoding for detected tag. If you were using original GridSensor without overriding any method, switching to new GridSensor will produce similar effect for training although the actual observations will be slightly different. For creating your GridSensor implementation with custom data: * To create custom GridSensor, derive from GridSensorBase instead of GridSensor . Besides overriding GetObjectData() , you will also need to consider override GetCellObservationSize() , IsDataNormalized() and GetProcessCollidersMethod() according to the data you collect. Also you'll need to override GridSensorComponent.GetGridSensors() and return your custom GridSensor. * The input argument tagIndex in GetObjectData() has changed from 1-indexed to 0-indexed and the data type changed from float to int . The index of first detectable tag will be 0 instead of 1. normalizedDistance was removed from input. * The observation data should be written to the input dataBuffer instead of creating and returning a new array. * Removed the constraint of all data required to be normalized. You should specify it in IsDataNormalized() . Sensors with non-normalized data cannot use PNG compression type. * The sensor will not further encode the data recieved from GetObjectData() anymore. The values recieved from GetObjectData() will be the observation sent to the trainer.","title":"GridSensor changes"},{"location":"Migrating/#lstm-models-from-previous-releases-no-longer-supported","text":"The way the Unity Inference Engine processes LSTM (recurrent neural networks) has changed. As a result, models trained with previous versions of ML-Agents will not be usable at inference if they were trained with a memory setting in the .yaml config file. If you want to use a model that has a recurrent neural network in this release of ML-Agents, you need to train the model using the python trainer from this release.","title":"LSTM models from previous releases no longer supported"},{"location":"Migrating/#migrating-to-release-13","text":"","title":"Migrating to Release 13"},{"location":"Migrating/#implementing-iheuristic-in-your-iactuator-implementations","text":"If you have any custom actuators, you can now implement the IHeuristicProvider interface to have your actuator handle the generation of actions when an Agent is running in heuristic mode. VectorSensor.AddObservation(IEnumerable<float>) is deprecated. Use VectorSensor.AddObservation(IList<float>) instead. ObservationWriter.AddRange() is deprecated. Use ObservationWriter.AddList() instead. ActuatorComponent.CreateAcuator() is deprecated. Please use override ActuatorComponent.CreateActuators instead. Since ActuatorComponent.CreateActuator() is abstract, you will still need to override it in your class until it is removed. It is only ever called if you don't override ActuatorComponent.CreateActuators . You can suppress the warnings by surrounding the method with the following pragma: c# #pragma warning disable 672 public IActuator CreateActuator() { ... } #pragma warning restore 672","title":"Implementing IHeuristic in your IActuator implementations"},{"location":"Migrating/#migrating_1","text":"","title":"Migrating"},{"location":"Migrating/#migrating-to-release-11","text":"","title":"Migrating to Release 11"},{"location":"Migrating/#agent-virtual-method-deprecation","text":"Agent.CollectDiscreteActionMasks() was deprecated and should be replaced with Agent.WriteDiscreteActionMask() Agent.Heuristic(float[]) was deprecated and should be replaced with Agent.Heuristic(ActionBuffers) . Agent.OnActionReceived(float[]) was deprecated and should be replaced with Agent.OnActionReceived(ActionBuffers) . Agent.GetAction() was deprecated and should be replaced with Agent.GetStoredActionBuffers() . The default implementation of these will continue to call the deprecated versions where appropriate. However, the deprecated versions may not be compatible with continuous and discrete actions on the same Agent.","title":"Agent virtual method deprecation"},{"location":"Migrating/#brainparameters-field-and-method-deprecation","text":"BrainParameters.VectorActionSize was deprecated; you can now set BrainParameters.ActionSpec.NumContinuousActions or BrainParameters.ActionSpec.BranchSizes instead. BrainParameters.VectorActionSpaceType was deprecated, since both continuous and discrete actions can now be used. BrainParameters.NumActions() was deprecated. Use BrainParameters.ActionSpec.NumContinuousActions and BrainParameters.ActionSpec.NumDiscreteActions instead.","title":"BrainParameters field and method deprecation"},{"location":"Migrating/#migrating-from-release-7-to-latest","text":"","title":"Migrating from Release 7 to latest"},{"location":"Migrating/#important-changes","text":"Some trainer files were moved. If you were using the TrainerFactory class, it was moved to the trainers/trainer folder. The components folder containing bc and reward_signals code was moved to the trainers/tf folder","title":"Important changes"},{"location":"Migrating/#steps-to-migrate","text":"Replace calls to from mlagents.trainers.trainer_util import TrainerFactory to from mlagents.trainers.trainer import TrainerFactory Replace calls to from mlagents.trainers.trainer_util import handle_existing_directories to from mlagents.trainers.directory_utils import validate_existing_directories Replace mlagents.trainers.components with mlagents.trainers.tf.components in your import statements.","title":"Steps to Migrate"},{"location":"Migrating/#migrating-from-release-3-to-release-7","text":"","title":"Migrating from Release 3 to Release 7"},{"location":"Migrating/#important-changes_1","text":"The Parameter Randomization feature has been merged with the Curriculum feature. It is now possible to specify a sampler in the lesson of a Curriculum. Curriculum has been refactored and is now specified at the level of the parameter, not the behavior. More information here .(#4160)","title":"Important changes"},{"location":"Migrating/#steps-to-migrate_1","text":"The configuration format for curriculum and parameter randomization has changed. To upgrade your configuration files, an upgrade script has been provided. Run python -m mlagents.trainers.upgrade_config -h to see the script usage. Note that you will have had to upgrade to/install the current version of ML-Agents before running the script. To update manually: If your config file used a parameter_randomization section, rename that section to environment_parameters If your config file used a curriculum section, you will need to rewrite your curriculum with this format .","title":"Steps to Migrate"},{"location":"Migrating/#migrating-from-release-1-to-release-3","text":"","title":"Migrating from Release 1 to Release 3"},{"location":"Migrating/#important-changes_2","text":"Training artifacts (trained models, summaries) are now found under results/ instead of summaries/ and models/ . Trainer configuration, curriculum configuration, and parameter randomization configuration have all been moved to a single YAML file. (#3791) Trainer configuration format has changed, and using a \"default\" behavior name has been deprecated. (#3936) max_step in the TerminalStep and TerminalSteps objects was renamed interrupted . On the UnityEnvironment API, get_behavior_names() and get_behavior_specs() methods were combined into the property behavior_specs that contains a mapping from behavior names to behavior spec. use_visual and allow_multiple_visual_obs in the UnityToGymWrapper constructor were replaced by allow_multiple_obs which allows one or more visual observations and vector observations to be used simultaneously. --save-freq has been removed from the CLI and is now configurable in the trainer configuration file. --lesson has been removed from the CLI. Lessons will resume when using --resume . To start at a different lesson, modify your Curriculum configuration.","title":"Important changes"},{"location":"Migrating/#steps-to-migrate_2","text":"To upgrade your configuration files, an upgrade script has been provided. Run python -m mlagents.trainers.upgrade_config -h to see the script usage. Note that you will have had to upgrade to/install the current version of ML-Agents before running the script. To do it manually, copy your <BehaviorName> sections from trainer_config.yaml into a separate trainer configuration file, under a behaviors section. The default section is no longer needed. This new file should be specific to your environment, and not contain configurations for multiple environments (unless they have the same Behavior Names). - You will need to reformat your trainer settings as per the example . - If your training uses curriculum , move those configurations under a curriculum section. - If your training uses parameter randomization , move the contents of the sampler config to parameter_randomization in the main trainer configuration. - If you are using UnityEnvironment directly, replace max_step with interrupted in the TerminalStep and TerminalSteps objects. - Replace usage of get_behavior_names() and get_behavior_specs() in UnityEnvironment with behavior_specs . - If you use the UnityToGymWrapper , remove use_visual and allow_multiple_visual_obs from the constructor and add allow_multiple_obs = True if the environment contains either both visual and vector observations or multiple visual observations. - If you were setting --save-freq in the CLI, add a checkpoint_interval value in your trainer configuration, and set it equal to save-freq * n_agents_in_scene .","title":"Steps to Migrate"},{"location":"Migrating/#migrating-from-015-to-release-1","text":"","title":"Migrating from 0.15 to Release 1"},{"location":"Migrating/#important-changes_3","text":"The MLAgents C# namespace was renamed to Unity.MLAgents , and other nested namespaces were similarly renamed (#3843). The --load and --train command-line flags have been deprecated and replaced with --resume and --inference . Running with the same --run-id twice will now throw an error. The play_against_current_self_ratio self-play trainer hyperparameter has been renamed to play_against_latest_model_ratio Removed the multi-agent gym option from the gym wrapper. For multi-agent scenarios, use the Low Level Python API . The low level Python API has changed. You can look at the document Low Level Python API documentation for more information. If you use mlagents-learn for training, this should be a transparent change. The obsolete Agent methods GiveModel , Done , InitializeAgent , AgentAction and AgentReset have been removed. The signature of Agent.Heuristic() was changed to take a float[] as a parameter, instead of returning the array. This was done to prevent a common source of error where users would return arrays of the wrong size. The SideChannel API has changed (#3833, #3660) : Introduced the SideChannelManager to register, unregister and access side channels. EnvironmentParameters replaces the default FloatProperties . You can access the EnvironmentParameters with Academy.Instance.EnvironmentParameters on C#. If you were previously creating a UnityEnvironment in python and passing it a FloatPropertiesChannel , create an EnvironmentParametersChannel instead. SideChannel.OnMessageReceived is now a protected method (was public) SideChannel IncomingMessages methods now take an optional default argument, which is used when trying to read more data than the message contains. Added a feature to allow sending stats from C# environments to TensorBoard (and other python StatsWriters). To do this from your code, use Academy.Instance.StatsRecorder.Add(key, value) (#3660) num_updates and train_interval for SAC have been replaced with steps_per_update . The UnityEnv class from the gym-unity package was renamed UnityToGymWrapper and no longer creates the UnityEnvironment . Instead, the UnityEnvironment must be passed as input to the constructor of UnityToGymWrapper Public fields and properties on several classes were renamed to follow Unity's C# style conventions. All public fields and properties now use \"PascalCase\" instead of \"camelCase\"; for example, Agent.maxStep was renamed to Agent.MaxStep . For a full list of changes, see the pull request. (#3828) WriteAdapter was renamed to ObservationWriter . (#3834)","title":"Important changes"},{"location":"Migrating/#steps-to-migrate_3","text":"In C# code, replace using MLAgents with using Unity.MLAgents . Replace other nested namespaces such as using MLAgents.Sensors with using Unity.MLAgents.Sensors Replace the --load flag with --resume when calling mlagents-learn , and don't use the --train flag as training will happen by default. To run without training, use --inference . To force-overwrite files from a pre-existing run, add the --force command-line flag. The Jupyter notebooks have been removed from the repository. If your Agent class overrides Heuristic() , change the signature to public override void Heuristic(float[] actionsOut) and assign values to actionsOut instead of returning an array. If you used SideChannels you must: Replace Academy.FloatProperties with Academy.Instance.EnvironmentParameters . Academy.RegisterSideChannel and Academy.UnregisterSideChannel were removed. Use SideChannelManager.RegisterSideChannel and SideChannelManager.UnregisterSideChannel instead. Set steps_per_update to be around equal to the number of agents in your environment, times num_updates and divided by train_interval . Replace UnityEnv with UnityToGymWrapper in your code. The constructor no longer takes a file name as input but a fully constructed UnityEnvironment instead. Update uses of \"camelCase\" fields and properties to \"PascalCase\".","title":"Steps to Migrate"},{"location":"Migrating/#migrating-from-014-to-015","text":"","title":"Migrating from 0.14 to 0.15"},{"location":"Migrating/#important-changes_4","text":"The Agent.CollectObservations() virtual method now takes as input a VectorSensor sensor as argument. The Agent.AddVectorObs() methods were removed. The SetMask was renamed to SetMask method must now be called on the DiscreteActionMasker argument of the CollectDiscreteActionMasks virtual method. We consolidated our API for DiscreteActionMasker . SetMask takes two arguments : the branch index and the list of masked actions for that branch. The Monitor class has been moved to the Examples Project. (It was prone to errors during testing) The MLAgents.Sensors namespace has been introduced. All sensors classes are part of the MLAgents.Sensors namespace. The MLAgents.SideChannels namespace has been introduced. All side channel classes are part of the MLAgents.SideChannels namespace. The interface for RayPerceptionSensor.PerceiveStatic() was changed to take an input class and write to an output class, and the method was renamed to Perceive() . The SetMask method must now be called on the DiscreteActionMasker argument of the CollectDiscreteActionMasks method. The method GetStepCount() on the Agent class has been replaced with the property getter StepCount The --multi-gpu option has been removed temporarily. AgentInfo.actionMasks has been renamed to AgentInfo.discreteActionMasks . BrainParameters and SpaceType have been removed from the public API BehaviorParameters have been removed from the public API. DecisionRequester has been made internal (you can still use the DecisionRequesterComponent from the inspector). RepeatAction was renamed TakeActionsBetweenDecisions for clarity. The following methods in the Agent class have been renamed. The original method names will be removed in a later release: InitializeAgent() was renamed to Initialize() AgentAction() was renamed to OnActionReceived() AgentReset() was renamed to OnEpsiodeBegin() Done() was renamed to EndEpisode() GiveModel() was renamed to SetModel() The IFloatProperties interface has been removed. The interface for SideChannels was changed: In C#, OnMessageReceived now takes a IncomingMessage argument, and QueueMessageToSend takes an OutgoingMessage argument. In python, on_message_received now takes a IncomingMessage argument, and queue_message_to_send takes an OutgoingMessage argument. Automatic stepping for Academy is now controlled from the AutomaticSteppingEnabled property.","title":"Important changes"},{"location":"Migrating/#steps-to-migrate_4","text":"Add the using MLAgents.Sensors; in addition to using MLAgents; on top of your Agent's script. Replace your Agent's implementation of CollectObservations() with CollectObservations(VectorSensor sensor) . In addition, replace all calls to AddVectorObs() with sensor.AddObservation() or sensor.AddOneHotObservation() on the VectorSensor passed as argument. Replace your calls to SetActionMask on your Agent to DiscreteActionMasker.SetActionMask in CollectDiscreteActionMasks . If you call RayPerceptionSensor.PerceiveStatic() manually, add your inputs to a RayPerceptionInput . To get the previous float array output, iterate through RayPerceptionOutput.rayOutputs and call RayPerceptionOutput.RayOutput.ToFloatArray() . Replace all calls to Agent.GetStepCount() with Agent.StepCount We strongly recommend replacing the following methods with their new equivalent as they will be removed in a later release: InitializeAgent() to Initialize() AgentAction() to OnActionReceived() AgentReset() to OnEpisodeBegin() Done() to EndEpisode() GiveModel() to SetModel() Replace IFloatProperties variables with FloatPropertiesChannel variables. If you implemented custom SideChannels , update the signatures of your methods, and add your data to the OutgoingMessage or read it from the IncomingMessage . Replace calls to Academy.EnableAutomaticStepping()/DisableAutomaticStepping() with Academy.AutomaticSteppingEnabled = true/false.","title":"Steps to Migrate"},{"location":"Migrating/#migrating-from-013-to-014","text":"","title":"Migrating from 0.13 to 0.14"},{"location":"Migrating/#important-changes_5","text":"The UnitySDK folder has been split into a Unity Package ( com.unity.ml-agents ) and an examples project ( Project ). Please follow the Installation Guide to get up and running with this new repo structure. Several changes were made to how agents are reset and marked as done: Calling Done() on the Agent will now reset it immediately and call the AgentReset virtual method. (This is to simplify the previous logic in which the Agent had to wait for the next EnvironmentStep to reset) The \"Reset on Done\" setting in AgentParameters was removed; this is now effectively always true. AgentOnDone virtual method on the Agent has been removed. The Decision Period and On Demand decision checkbox have been removed from the Agent. On demand decision is now the default (calling RequestDecision on the Agent manually.) The Academy class was changed to a singleton, and its virtual methods were removed. Trainer steps are now counted per-Agent, not per-environment as in previous versions. For instance, if you have 10 Agents in the scene, 20 environment steps now corresponds to 200 steps as printed in the terminal and in Tensorboard. Curriculum config files are now YAML formatted and all curricula for a training run are combined into a single file. The --num-runs command-line option has been removed from mlagents-learn . Several fields on the Agent were removed or made private in order to simplify the interface. The agentParameters field of the Agent has been removed. (Contained only maxStep information) maxStep is now a public field on the Agent. (Was moved from agentParameters ) The Info field of the Agent has been made private. (Was only used internally and not meant to be modified outside of the Agent) The GetReward() method on the Agent has been removed. (It was being confused with GetCumulativeReward() ) The AgentAction struct no longer contains a value field. (Value estimates were not set during inference) The GetValueEstimate() method on the Agent has been removed. The UpdateValueAction() method on the Agent has been removed. The deprecated RayPerception3D and RayPerception2D classes were removed, and the legacyHitFractionBehavior argument was removed from RayPerceptionSensor.PerceiveStatic() . RayPerceptionSensor was inconsistent in how it handle scale on the Agent's transform. It now scales the ray length and sphere size for casting as the transform's scale changes.","title":"Important changes"},{"location":"Migrating/#steps-to-migrate_5","text":"Follow the instructions on how to install the com.unity.ml-agents package into your project in the Installation Guide . If your Agent implemented AgentOnDone and did not have the checkbox Reset On Done checked in the inspector, you must call the code that was in AgentOnDone manually. If you give your Agent a reward or penalty at the end of an episode (e.g. for reaching a goal or falling off of a platform), make sure you call AddReward() or SetReward() before calling Done() . Previously, the order didn't matter. If you were not using On Demand Decision for your Agent, you must add a DecisionRequester component to your Agent GameObject and set its Decision Period field to the old Decision Period of the Agent. If you have a class that inherits from Academy: If the class didn't override any of the virtual methods and didn't store any additional data, you can just remove the old script from the scene. If the class had additional data, create a new MonoBehaviour and store the data in the new MonoBehaviour instead. If the class overrode the virtual methods, create a new MonoBehaviour and move the logic to it: Move the InitializeAcademy code to MonoBehaviour.Awake Move the AcademyStep code to MonoBehaviour.FixedUpdate Move the OnDestroy code to MonoBehaviour.OnDestroy. Move the AcademyReset code to a new method and add it to the Academy.OnEnvironmentReset action. Multiply max_steps and summary_freq in your trainer_config.yaml by the number of Agents in the scene. Combine curriculum configs into a single file. See the WallJump curricula for an example of the new curriculum config format. A tool like https://www.json2yaml.com may be useful to help with the conversion. If you have a model trained which uses RayPerceptionSensor and has non-1.0 scale in the Agent's transform, it must be retrained.","title":"Steps to Migrate"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v0120-to-v0130","text":"","title":"Migrating from ML-Agents Toolkit v0.12.0 to v0.13.0"},{"location":"Migrating/#important-changes_6","text":"The low level Python API has changed. You can look at the document Low Level Python API documentation for more information. This should only affect you if you're writing a custom trainer; if you use mlagents-learn for training, this should be a transparent change. reset() on the Low-Level Python API no longer takes a train_mode argument. To modify the performance/speed of the engine, you must use an EngineConfigurationChannel reset() on the Low-Level Python API no longer takes a config argument. UnityEnvironment no longer has a reset_parameters field. To modify float properties in the environment, you must use a FloatPropertiesChannel . For more information, refer to the Low Level Python API documentation CustomResetParameters are now removed. The Academy no longer has a Training Configuration nor Inference Configuration field in the inspector. To modify the configuration from the Low-Level Python API, use an EngineConfigurationChannel . To modify it during training, use the new command line arguments --width , --height , --quality-level , --time-scale and --target-frame-rate in mlagents-learn . The Academy no longer has a Default Reset Parameters field in the inspector. The Academy class no longer has a ResetParameters . To access shared float properties with Python, use the new FloatProperties field on the Academy. Offline Behavioral Cloning has been removed. To learn from demonstrations, use the GAIL and Behavioral Cloning features with either PPO or SAC. mlagents.envs was renamed to mlagents_envs . The previous repo layout depended on PEP420 , which caused problems with some of our tooling such as mypy and pylint. The official version of Unity ML-Agents supports is now 2018.4 LTS. If you run into issues, please consider deleting your library folder and reponening your projects. You will need to install the Barracuda package into your project in order to ML-Agents to compile correctly.","title":"Important changes"},{"location":"Migrating/#steps-to-migrate_6","text":"If you had a custom Training Configuration in the Academy inspector, you will need to pass your custom configuration at every training run using the new command line arguments --width , --height , --quality-level , --time-scale and --target-frame-rate . If you were using --slow in mlagents-learn , you will need to pass your old Inference Configuration of the Academy inspector with the new command line arguments --width , --height , --quality-level , --time-scale and --target-frame-rate instead. Any imports from mlagents.envs should be replaced with mlagents_envs .","title":"Steps to Migrate"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v0110-to-v0120","text":"","title":"Migrating from ML-Agents Toolkit v0.11.0 to v0.12.0"},{"location":"Migrating/#important-changes_7","text":"Text actions and observations, and custom action and observation protos have been removed. RayPerception3D and RayPerception2D are marked deprecated, and will be removed in a future release. They can be replaced by RayPerceptionSensorComponent3D and RayPerceptionSensorComponent2D. The Use Heuristic checkbox in Behavior Parameters has been replaced with a Behavior Type dropdown menu. This has the following options: Default corresponds to the previous unchecked behavior, meaning that Agents will train if they connect to a python trainer, otherwise they will perform inference. Heuristic Only means the Agent will always use the Heuristic() method. This corresponds to having \"Use Heuristic\" selected in 0.11.0. Inference Only means the Agent will always perform inference. Barracuda was upgraded to 0.3.2, and it is now installed via the Unity Package Manager.","title":"Important Changes"},{"location":"Migrating/#steps-to-migrate_7","text":"We fixed a bug in RayPerception3d.Perceive() that was causing the endOffset to be used incorrectly. However this may produce different behavior from previous versions if you use a non-zero startOffset . To reproduce the old behavior, you should increase the value of endOffset by startOffset . You can verify your raycasts are performing as expected in scene view using the debug rays. If you use RayPerception3D, replace it with RayPerceptionSensorComponent3D (and similarly for 2D). The settings, such as ray angles and detectable tags, are configured on the component now. RayPerception3D would contribute (# of rays) * (# of tags + 2) to the State Size in Behavior Parameters, but this is no longer necessary, so you should reduce the State Size by this amount. Making this change will require retraining your model, since the observations that RayPerceptionSensorComponent3D produces are different from the old behavior. If you see messages such as The type or namespace 'Barracuda' could not be found or The type or namespace 'Google' could not be found , you will need to install the Barracuda preview package .","title":"Steps to Migrate"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v010-to-v0110","text":"","title":"Migrating from ML-Agents Toolkit v0.10 to v0.11.0"},{"location":"Migrating/#important-changes_8","text":"The definition of the gRPC service has changed. The online BC training feature has been removed. The BroadcastHub has been deprecated. If there is a training Python process, all LearningBrains in the scene will automatically be trained. If there is no Python process, inference will be used. The Brain ScriptableObjects have been deprecated. The Brain Parameters are now on the Agent and are referred to as Behavior Parameters. Make sure the Behavior Parameters is attached to the Agent GameObject. To use a heuristic behavior, implement the Heuristic() method in the Agent class and check the use heuristic checkbox in the Behavior Parameters. Several changes were made to the setup for visual observations (i.e. using Cameras or RenderTextures): Camera resolutions are no longer stored in the Brain Parameters. AgentParameters no longer stores lists of Cameras and RenderTextures To add visual observations to an Agent, you must now attach a CameraSensorComponent or RenderTextureComponent to the agent. The corresponding Camera or RenderTexture can be added to these in the editor, and the resolution and color/grayscale is configured on the component itself.","title":"Important Changes"},{"location":"Migrating/#steps-to-migrate_8","text":"In order to be able to train, make sure both your ML-Agents Python package and UnitySDK code come from the v0.11 release. Training will not work, for example, if you update the ML-Agents Python package, and only update the API Version in UnitySDK. If your Agents used visual observations, you must add a CameraSensorComponent corresponding to each old Camera in the Agent's camera list (and similarly for RenderTextures). Since Brain ScriptableObjects have been removed, you will need to delete all the Brain ScriptableObjects from your Assets folder. Then, add a Behavior Parameters component to each Agent GameObject. You will then need to complete the fields on the new Behavior Parameters component with the BrainParameters of the old Brain.","title":"Steps to Migrate"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v09-to-v010","text":"","title":"Migrating from ML-Agents Toolkit v0.9 to v0.10"},{"location":"Migrating/#important-changes_9","text":"We have updated the C# code in our repository to be in line with Unity Coding Conventions. This has changed the name of some public facing classes and enums. The example environments have been updated. If you were using these environments to benchmark your training, please note that the resulting rewards may be slightly different in v0.10.","title":"Important Changes"},{"location":"Migrating/#steps-to-migrate_9","text":"UnitySDK/Assets/ML-Agents/Scripts/Communicator.cs and its class Communicator have been renamed to UnitySDK/Assets/ML-Agents/Scripts/ICommunicator.cs and ICommunicator respectively. The SpaceType Enums discrete , and continuous have been renamed to Discrete and Continuous . We have removed the Done call as well as the capacity to set Max Steps on the Academy. Therefore an AcademyReset will never be triggered from C# (only from Python). If you want to reset the simulation after a fixed number of steps, or when an event in the simulation occurs, we recommend looking at our multi-agent example environments (such as FoodCollector). In our examples, groups of Agents can be reset through an \"Area\" that can reset groups of Agents. The import for mlagents.envs.UnityEnvironment was removed. If you are using the Python API, change from mlagents_envs import UnityEnvironment to from mlagents_envs.environment import UnityEnvironment .","title":"Steps to Migrate"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v08-to-v09","text":"","title":"Migrating from ML-Agents Toolkit v0.8 to v0.9"},{"location":"Migrating/#important-changes_10","text":"We have changed the way reward signals (including Curiosity) are defined in the trainer_config.yaml . When using multiple environments, every \"step\" is recorded in TensorBoard. The steps in the command line console corresponds to a single step of a single environment. Previously, each step corresponded to one step for all environments (i.e., num_envs steps).","title":"Important Changes"},{"location":"Migrating/#steps-to-migrate_10","text":"If you were overriding any of these following parameters in your config file, remove them from the top-level config and follow the steps below: gamma : Define a new extrinsic reward signal and set it's gamma to your new gamma. use_curiosity , curiosity_strength , curiosity_enc_size : Define a curiosity reward signal and set its strength to curiosity_strength , and encoding_size to curiosity_enc_size . Give it the same gamma as your extrinsic signal to mimic previous behavior. TensorBoards generated when running multiple environments in v0.8 are not comparable to those generated in v0.9 in terms of step count. Multiply your v0.8 step count by num_envs for an approximate comparison. You may need to change max_steps in your config as appropriate as well.","title":"Steps to Migrate"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v07-to-v08","text":"","title":"Migrating from ML-Agents Toolkit v0.7 to v0.8"},{"location":"Migrating/#important-changes_11","text":"We have split the Python packages into two separate packages ml-agents and ml-agents-envs . --worker-id option of learn.py has been removed, use --base-port instead if you'd like to run multiple instances of learn.py .","title":"Important Changes"},{"location":"Migrating/#steps-to-migrate_11","text":"If you are installing via PyPI, there is no change. If you intend to make modifications to ml-agents or ml-agents-envs please check the Installing for Development in the Installation documentation .","title":"Steps to Migrate"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v06-to-v07","text":"","title":"Migrating from ML-Agents Toolkit v0.6 to v0.7"},{"location":"Migrating/#important-changes_12","text":"We no longer support TFS and are now using the Unity Inference Engine","title":"Important Changes"},{"location":"Migrating/#steps-to-migrate_12","text":"Make sure to remove the ENABLE_TENSORFLOW flag in your Unity Project settings","title":"Steps to Migrate"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v05-to-v06","text":"","title":"Migrating from ML-Agents Toolkit v0.5 to v0.6"},{"location":"Migrating/#important-changes_13","text":"Brains are now Scriptable Objects instead of MonoBehaviors. You can no longer modify the type of a Brain. If you want to switch between PlayerBrain and LearningBrain for multiple agents, you will need to assign a new Brain to each agent separately. Note: You can pass the same Brain to multiple agents in a scene by leveraging Unity's prefab system or look for all the agents in a scene using the search bar of the Hierarchy window with the word Agent . We replaced the Internal and External Brain with Learning Brain . When you need to train a model, you need to drag it into the Broadcast Hub inside the Academy and check the Control checkbox. We removed the Broadcast checkbox of the Brain, to use the broadcast functionality, you need to drag the Brain into the Broadcast Hub . When training multiple Brains at the same time, each model is now stored into a separate model file rather than in the same file under different graph scopes. The Learning Brain graph scope, placeholder names, output names and custom placeholders can no longer be modified.","title":"Important Changes"},{"location":"Migrating/#steps-to-migrate_13","text":"To update a scene from v0.5 to v0.6, you must: Remove the Brain GameObjects in the scene. (Delete all of the Brain GameObjects under Academy in the scene.) Create new Brain Scriptable Objects using Assets -> Create -> ML-Agents for each type of the Brain you plan to use, and put the created files under a folder called Brains within your project. Edit their Brain Parameters to be the same as the parameters used in the Brain GameObjects. Agents have a Brain field in the Inspector, you need to drag the appropriate Brain ScriptableObject in it. The Academy has a Broadcast Hub field in the inspector, which is list of brains used in the scene. To train or control your Brain from the mlagents-learn Python script, you need to drag the relevant LearningBrain ScriptableObjects used in your scene into entries into this list.","title":"Steps to Migrate"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v04-to-v05","text":"","title":"Migrating from ML-Agents Toolkit v0.4 to v0.5"},{"location":"Migrating/#important","text":"The Unity project unity-environment has been renamed UnitySDK . The python folder has been renamed to ml-agents . It now contains two packages, mlagents.env and mlagents.trainers . mlagents.env can be used to interact directly with a Unity environment, while mlagents.trainers contains the classes for training agents. The supported Unity version has changed from 2017.1 or later to 2017.4 or later . 2017.4 is an LTS (Long Term Support) version that helps us maintain good quality and support. Earlier versions of Unity might still work, but you may encounter an error listed here.","title":"Important"},{"location":"Migrating/#unity-api","text":"Discrete Actions now use branches . You can now specify concurrent discrete actions. You will need to update the Brain Parameters in the Brain Inspector in all your environments that use discrete actions. Refer to the discrete action documentation for more information.","title":"Unity API"},{"location":"Migrating/#python-api","text":"In order to run a training session, you can now use the command mlagents-learn instead of python3 learn.py after installing the mlagents packages. This change is documented here . For example, if we previously ran sh python3 learn.py 3DBall --train from the python subdirectory (which is changed to ml-agents subdirectory in v0.5), we now run sh mlagents-learn config/trainer_config.yaml --env=3DBall --train from the root directory where we installed the ML-Agents Toolkit. It is now required to specify the path to the yaml trainer configuration file when running mlagents-learn . For an example trainer configuration file, see trainer_config.yaml . An example of passing a trainer configuration to mlagents-learn is shown above. The environment name is now passed through the --env option. Curriculum learning has been changed. In summary: Curriculum files for the same environment must now be placed into a folder. Each curriculum file should be named after the Brain whose curriculum it specifies. min_lesson_length now specifies the minimum number of episodes in a lesson and affects reward thresholding. It is no longer necessary to specify the Max Steps of the Academy to use curriculum learning.","title":"Python API"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v03-to-v04","text":"","title":"Migrating from ML-Agents Toolkit v0.3 to v0.4"},{"location":"Migrating/#unity-api_1","text":"using MLAgents; needs to be added in all of the C# scripts that use ML-Agents.","title":"Unity API"},{"location":"Migrating/#python-api_1","text":"We've changed some of the Python packages dependencies in requirement.txt file. Make sure to run pip3 install -e . within your ml-agents/python folder to update your Python packages.","title":"Python API"},{"location":"Migrating/#migrating-from-ml-agents-toolkit-v02-to-v03","text":"There are a large number of new features and improvements in the ML-Agents toolkit v0.3 which change both the training process and Unity API in ways which will cause incompatibilities with environments made using older versions. This page is designed to highlight those changes for users familiar with v0.1 or v0.2 in order to ensure a smooth transition.","title":"Migrating from ML-Agents Toolkit v0.2 to v0.3"},{"location":"Migrating/#important_1","text":"The ML-Agents Toolkit is no longer compatible with Python 2.","title":"Important"},{"location":"Migrating/#python-training","text":"The training script ppo.py and PPO.ipynb Python notebook have been replaced with a single learn.py script as the launching point for training with ML-Agents. For more information on using learn.py , see here . Hyperparameters for training Brains are now stored in the trainer_config.yaml file. For more information on using this file, see here .","title":"Python Training"},{"location":"Migrating/#unity-api_2","text":"Modifications to an Agent's rewards must now be done using either AddReward() or SetReward() . Setting an Agent to done now requires the use of the Done() method. CollectStates() has been replaced by CollectObservations() , which now no longer returns a list of floats. To collect observations, call AddVectorObs() within CollectObservations() . Note that you can call AddVectorObs() with floats, integers, lists and arrays of floats, Vector3 and Quaternions. AgentStep() has been replaced by AgentAction() . WaitTime() has been removed. The Frame Skip field of the Academy is replaced by the Agent's Decision Frequency field, enabling the Agent to make decisions at different frequencies. The names of the inputs in the Internal Brain have been changed. You must replace state with vector_observation and observation with visual_observation . In addition, you must remove the epsilon placeholder.","title":"Unity API"},{"location":"Migrating/#semantics","text":"In order to more closely align with the terminology used in the Reinforcement Learning field, and to be more descriptive, we have changed the names of some of the concepts used in ML-Agents. The changes are highlighted in the table below. Old - v0.2 and earlier New - v0.3 and later State Vector Observation Observation Visual Observation Action Vector Action N/A Text Observation N/A Text Action","title":"Semantics"},{"location":"Package-Settings/","text":"ML-Agents Package Settings ML-Agents Package Settings contains settings that apply to the whole project. It allows you to configure ML-Agents-specific settings in the Editor. These settings are available for use in both the Editor and Player. You can find them at Edit > Project Settings... > ML-Agents . It lists out all the available settings and their default values. Create Custom Settings In order to to use your own settings for your project, you'll need to create a settings asset. You can do this by clicking the Create Settings Asset buttom or clicking the gear on the top right and select New Settings Asset... . The asset file can be placed anywhere in the Asset/ folder in your project. After Creating the settings asset, you'll be able to modify the settings for your project and your settings will be saved in the asset. Multiple Custom Settings for Different Scenarios You can create multiple settings assets in one project. By clicking the gear on the top right you'll see all available settings listed in the drop-down menu to choose from. This allows you to create different settings for different scenatios. For example, you can create two separate settings for training and inference, and specify which one you want to use according to what you're currently running.","title":"ML-Agents Package Settings"},{"location":"Package-Settings/#ml-agents-package-settings","text":"ML-Agents Package Settings contains settings that apply to the whole project. It allows you to configure ML-Agents-specific settings in the Editor. These settings are available for use in both the Editor and Player. You can find them at Edit > Project Settings... > ML-Agents . It lists out all the available settings and their default values.","title":"ML-Agents Package Settings"},{"location":"Package-Settings/#create-custom-settings","text":"In order to to use your own settings for your project, you'll need to create a settings asset. You can do this by clicking the Create Settings Asset buttom or clicking the gear on the top right and select New Settings Asset... . The asset file can be placed anywhere in the Asset/ folder in your project. After Creating the settings asset, you'll be able to modify the settings for your project and your settings will be saved in the asset.","title":"Create Custom Settings"},{"location":"Package-Settings/#multiple-custom-settings-for-different-scenarios","text":"You can create multiple settings assets in one project. By clicking the gear on the top right you'll see all available settings listed in the drop-down menu to choose from. This allows you to create different settings for different scenatios. For example, you can create two separate settings for training and inference, and specify which one you want to use according to what you're currently running.","title":"Multiple Custom Settings for Different Scenarios"},{"location":"Profiling-Python/","text":"Profiling in Python As part of the ML-Agents Tookit, we provide a lightweight profiling system, in order to identity hotspots in the training process and help spot regressions from changes. Timers are hierarchical, meaning that the time tracked in a block of code can be further split into other blocks if desired. This also means that a function that is called from multiple places in the code will appear in multiple places in the timing output. All timers operate using a \"global\" instance by default, but this can be overridden if necessary (mainly for testing). Adding Profiling There are two ways to indicate code should be included in profiling. The simplest way is to add the @timed decorator to a function or method of interested. class TrainerController: # .... @timed def advance(self, env: EnvManager) -> int: # do stuff You can also used the hierarchical_timer context manager. with hierarchical_timer(\"communicator.exchange\"): outputs = self.communicator.exchange(step_input) The context manager may be easier than the @timed decorator for profiling different parts of a large function, or profiling calls to abstract methods that might not use decorator. Output By default, at the end of training, timers are collected and written in json format to {summaries_dir}/{run_id}_timers.json . The output consists of node objects with the following keys: total (float): The total time in seconds spent in the block, including child calls. count (int): The number of times the block was called. self (float): The total time in seconds spent in the block, excluding child calls. children (dictionary): A dictionary of child nodes, keyed by the node name. is_parallel (bool): Indicates that the block of code was executed in multiple threads or processes (see below). This is optional and defaults to false. Parallel execution Subprocesses For code that executes in multiple processes (for example, SubprocessEnvManager), we periodically send the timer information back to the \"main\" process, aggregate the timers there, and flush them in the subprocess. Note that (depending on the number of processes) this can result in timers where the total time may exceed the parent's total time. This is analogous to the difference between \"real\" and \"user\" values reported from the unix time command. In the timer output, blocks that were run in parallel are indicated by the is_parallel flag. Threads Timers currently use time.perf_counter() to track time spent, which may not give accurate results for multiple threads. If this is problematic, set threaded: false in your trainer configuration.","title":"Profiling in Python"},{"location":"Profiling-Python/#profiling-in-python","text":"As part of the ML-Agents Tookit, we provide a lightweight profiling system, in order to identity hotspots in the training process and help spot regressions from changes. Timers are hierarchical, meaning that the time tracked in a block of code can be further split into other blocks if desired. This also means that a function that is called from multiple places in the code will appear in multiple places in the timing output. All timers operate using a \"global\" instance by default, but this can be overridden if necessary (mainly for testing).","title":"Profiling in Python"},{"location":"Profiling-Python/#adding-profiling","text":"There are two ways to indicate code should be included in profiling. The simplest way is to add the @timed decorator to a function or method of interested. class TrainerController: # .... @timed def advance(self, env: EnvManager) -> int: # do stuff You can also used the hierarchical_timer context manager. with hierarchical_timer(\"communicator.exchange\"): outputs = self.communicator.exchange(step_input) The context manager may be easier than the @timed decorator for profiling different parts of a large function, or profiling calls to abstract methods that might not use decorator.","title":"Adding Profiling"},{"location":"Profiling-Python/#output","text":"By default, at the end of training, timers are collected and written in json format to {summaries_dir}/{run_id}_timers.json . The output consists of node objects with the following keys: total (float): The total time in seconds spent in the block, including child calls. count (int): The number of times the block was called. self (float): The total time in seconds spent in the block, excluding child calls. children (dictionary): A dictionary of child nodes, keyed by the node name. is_parallel (bool): Indicates that the block of code was executed in multiple threads or processes (see below). This is optional and defaults to false.","title":"Output"},{"location":"Profiling-Python/#parallel-execution","text":"","title":"Parallel execution"},{"location":"Profiling-Python/#subprocesses","text":"For code that executes in multiple processes (for example, SubprocessEnvManager), we periodically send the timer information back to the \"main\" process, aggregate the timers there, and flush them in the subprocess. Note that (depending on the number of processes) this can result in timers where the total time may exceed the parent's total time. This is analogous to the difference between \"real\" and \"user\" values reported from the unix time command. In the timer output, blocks that were run in parallel are indicated by the is_parallel flag.","title":"Subprocesses"},{"location":"Profiling-Python/#threads","text":"Timers currently use time.perf_counter() to track time spent, which may not give accurate results for multiple threads. If this is problematic, set threaded: false in your trainer configuration.","title":"Threads"},{"location":"Python-Gym-API-Documentation/","text":"Table of Contents mlagents_envs.envs.unity_gym_env UnityGymException UnityToGymWrapper __init__ reset step render close seed ActionFlattener __init__ lookup_action mlagents_envs.envs.unity_gym_env UnityGymException Objects class UnityGymException(error.Error) Any error related to the gym wrapper of ml-agents. UnityToGymWrapper Objects class UnityToGymWrapper(gym.Env) Provides Gym wrapper for Unity Learning Environments. __init__ | __init__(unity_env: BaseEnv, uint8_visual: bool = False, flatten_branched: bool = False, allow_multiple_obs: bool = False, action_space_seed: Optional[int] = None) Environment initialization Arguments : unity_env : The Unity BaseEnv to be wrapped in the gym. Will be closed when the UnityToGymWrapper closes. uint8_visual : Return visual observations as uint8 (0-255) matrices instead of float (0.0-1.0). flatten_branched : If True, turn branched discrete action spaces into a Discrete space rather than MultiDiscrete. allow_multiple_obs : If True, return a list of np.ndarrays as observations with the first elements containing the visual observations and the last element containing the array of vector observations. If False, returns a single np.ndarray containing either only a single visual observation or the array of vector observations. action_space_seed : If non-None, will be used to set the random seed on created gym.Space instances. reset | reset() -> Union[List[np.ndarray], np.ndarray] Resets the state of the environment and returns an initial observation. Returns: observation (object/list): the initial observation of the space. step | step(action: List[Any]) -> GymStepResult Run one timestep of the environment's dynamics. When end of episode is reached, you are responsible for calling reset() to reset this environment's state. Accepts an action and returns a tuple (observation, reward, done, info). Arguments : action object/list - an action provided by the environment Returns : observation object/list - agent's observation of the current environment reward (float/list) : amount of reward returned after previous action done boolean/list - whether the episode has ended. info dict - contains auxiliary diagnostic information. render | render(mode=\"rgb_array\") Return the latest visual observations. Note that it will not render a new frame of the environment. close | close() -> None Override _close in your subclass to perform any necessary cleanup. Environments will automatically close() themselves when garbage collected or when the program exits. seed | seed(seed: Any = None) -> None Sets the seed for this env's random number generator(s). Currently not implemented. ActionFlattener Objects class ActionFlattener() Flattens branched discrete action spaces into single-branch discrete action spaces. __init__ | __init__(branched_action_space) Initialize the flattener. Arguments : branched_action_space : A List containing the sizes of each branch of the action space, e.g. [2,3,3] for three branches with size 2, 3, and 3 respectively. lookup_action | lookup_action(action) Convert a scalar discrete action into a unique set of branched actions. Arguments : action : A scalar value representing one of the discrete actions. Returns : The List containing the branched actions.","title":"Gym API Documentation"},{"location":"Python-Gym-API-Documentation/#table-of-contents","text":"mlagents_envs.envs.unity_gym_env UnityGymException UnityToGymWrapper __init__ reset step render close seed ActionFlattener __init__ lookup_action","title":"Table of Contents"},{"location":"Python-Gym-API-Documentation/#mlagents_envsenvsunity_gym_env","text":"","title":"mlagents_envs.envs.unity_gym_env"},{"location":"Python-Gym-API-Documentation/#unitygymexception-objects","text":"class UnityGymException(error.Error) Any error related to the gym wrapper of ml-agents.","title":"UnityGymException Objects"},{"location":"Python-Gym-API-Documentation/#unitytogymwrapper-objects","text":"class UnityToGymWrapper(gym.Env) Provides Gym wrapper for Unity Learning Environments.","title":"UnityToGymWrapper Objects"},{"location":"Python-Gym-API-Documentation/#__init__","text":"| __init__(unity_env: BaseEnv, uint8_visual: bool = False, flatten_branched: bool = False, allow_multiple_obs: bool = False, action_space_seed: Optional[int] = None) Environment initialization Arguments : unity_env : The Unity BaseEnv to be wrapped in the gym. Will be closed when the UnityToGymWrapper closes. uint8_visual : Return visual observations as uint8 (0-255) matrices instead of float (0.0-1.0). flatten_branched : If True, turn branched discrete action spaces into a Discrete space rather than MultiDiscrete. allow_multiple_obs : If True, return a list of np.ndarrays as observations with the first elements containing the visual observations and the last element containing the array of vector observations. If False, returns a single np.ndarray containing either only a single visual observation or the array of vector observations. action_space_seed : If non-None, will be used to set the random seed on created gym.Space instances.","title":"__init__"},{"location":"Python-Gym-API-Documentation/#reset","text":"| reset() -> Union[List[np.ndarray], np.ndarray] Resets the state of the environment and returns an initial observation. Returns: observation (object/list): the initial observation of the space.","title":"reset"},{"location":"Python-Gym-API-Documentation/#step","text":"| step(action: List[Any]) -> GymStepResult Run one timestep of the environment's dynamics. When end of episode is reached, you are responsible for calling reset() to reset this environment's state. Accepts an action and returns a tuple (observation, reward, done, info). Arguments : action object/list - an action provided by the environment Returns : observation object/list - agent's observation of the current environment reward (float/list) : amount of reward returned after previous action done boolean/list - whether the episode has ended. info dict - contains auxiliary diagnostic information.","title":"step"},{"location":"Python-Gym-API-Documentation/#render","text":"| render(mode=\"rgb_array\") Return the latest visual observations. Note that it will not render a new frame of the environment.","title":"render"},{"location":"Python-Gym-API-Documentation/#close","text":"| close() -> None Override _close in your subclass to perform any necessary cleanup. Environments will automatically close() themselves when garbage collected or when the program exits.","title":"close"},{"location":"Python-Gym-API-Documentation/#seed","text":"| seed(seed: Any = None) -> None Sets the seed for this env's random number generator(s). Currently not implemented.","title":"seed"},{"location":"Python-Gym-API-Documentation/#actionflattener-objects","text":"class ActionFlattener() Flattens branched discrete action spaces into single-branch discrete action spaces.","title":"ActionFlattener Objects"},{"location":"Python-Gym-API-Documentation/#__init___1","text":"| __init__(branched_action_space) Initialize the flattener. Arguments : branched_action_space : A List containing the sizes of each branch of the action space, e.g. [2,3,3] for three branches with size 2, 3, and 3 respectively.","title":"__init__"},{"location":"Python-Gym-API-Documentation/#lookup_action","text":"| lookup_action(action) Convert a scalar discrete action into a unique set of branched actions. Arguments : action : A scalar value representing one of the discrete actions. Returns : The List containing the branched actions.","title":"lookup_action"},{"location":"Python-Gym-API/","text":"Unity ML-Agents Gym Wrapper A common way in which machine learning researchers interact with simulation environments is via a wrapper provided by OpenAI called gym . For more information on the gym interface, see here . We provide a gym wrapper and instructions for using it with existing machine learning algorithms which utilize gym. Our wrapper provides interfaces on top of our UnityEnvironment class, which is the default way of interfacing with a Unity environment via Python. Installation The gym wrapper can be installed using: pip3 install gym_unity or by running the following from the /gym-unity directory of the repository: pip3 install -e . Using the Gym Wrapper The gym interface is available from gym_unity.envs . To launch an environment from the root of the project repository use: from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper env = UnityToGymWrapper(unity_env, uint8_visual, flatten_branched, allow_multiple_obs) unity_env refers to the Unity environment to be wrapped. uint8_visual refers to whether to output visual observations as uint8 values (0-255). Many common Gym environments (e.g. Atari) do this. By default they will be floats (0.0-1.0). Defaults to False . flatten_branched will flatten a branched discrete action space into a Gym Discrete. Otherwise, it will be converted into a MultiDiscrete. Defaults to False . allow_multiple_obs will return a list of observations. The first elements contain the visual observations and the last element contains the array of vector observations. If False the environment returns a single array (containing a single visual observations, if present, otherwise the vector observation). Defaults to False . action_space_seed is the optional seed for action sampling. If non-None, will be used to set the random seed on created gym.Space instances. The returned environment env will function as a gym. Limitations It is only possible to use an environment with a single Agent. By default, the first visual observation is provided as the observation , if present. Otherwise, vector observations are provided. You can receive all visual and vector observations by using the allow_multiple_obs=True option in the gym parameters. If set to True , you will receive a list of observation instead of only one. The TerminalSteps or DecisionSteps output from the environment can still be accessed from the info provided by env.step(action) . Stacked vector observations are not supported. Environment registration for use with gym.make() is currently not supported. Calling env.render() will not render a new frame of the environment. It will return the latest visual observation if using visual observations. Running OpenAI Baselines Algorithms OpenAI provides a set of open-source maintained and tested Reinforcement Learning algorithms called the Baselines . Using the provided Gym wrapper, it is possible to train ML-Agents environments using these algorithms. This requires the creation of custom training scripts to launch each algorithm. In most cases these scripts can be created by making slight modifications to the ones provided for Atari and Mujoco environments. These examples were tested with baselines version 0.1.6. Example - DQN Baseline In order to train an agent to play the GridWorld environment using the Baselines DQN algorithm, you first need to install the baselines package using pip: pip install git+git://github.com/openai/baselines Next, create a file called train_unity.py . Then create an /envs/ directory and build the environment to that directory. For more information on building Unity environments, see here . Note that because of limitations of the DQN baseline, the environment must have a single visual observation, a single discrete action and a single Agent in the scene. Add the following code to the train_unity.py file: import gym from baselines import deepq from baselines import logger from mlagents_envs.environment import UnityEnvironment from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper def main(): unity_env = UnityEnvironment( < path - to - environment >) env = UnityToGymWrapper(unity_env, uint8_visual=True) logger.configure('./logs') # Change to log in a different directory act = deepq.learn( env, \"cnn\", # For visual inputs lr=2.5e-4, total_timesteps=1000000, buffer_size=50000, exploration_fraction=0.05, exploration_final_eps=0.1, print_freq=20, train_freq=5, learning_starts=20000, target_network_update_freq=50, gamma=0.99, prioritized_replay=False, checkpoint_freq=1000, checkpoint_path='./logs', # Change to save model in a different directory dueling=True ) print(\"Saving model to unity_model.pkl\") act.save(\"unity_model.pkl\") if __name__ == '__main__': main() To start the training process, run the following from the directory containing train_unity.py : python -m train_unity Other Algorithms Other algorithms in the Baselines repository can be run using scripts similar to the examples from the baselines package. In most cases, the primary changes needed to use a Unity environment are to import UnityToGymWrapper , and to replace the environment creation code, typically gym.make() , with a call to UnityToGymWrapper(unity_environment) passing the environment as input. A typical rule of thumb is that for vision-based environments, modification should be done to Atari training scripts, and for vector observation environments, modification should be done to Mujoco scripts. Some algorithms will make use of make_env() or make_mujoco_env() functions. You can define a similar function for Unity environments. An example of such a method using the PPO2 baseline: from mlagents_envs.environment import UnityEnvironment from mlagents_envs.envs import UnityToGymWrapper from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv from baselines.common.vec_env.dummy_vec_env import DummyVecEnv from baselines.bench import Monitor from baselines import logger import baselines.ppo2.ppo2 as ppo2 import os try: from mpi4py import MPI except ImportError: MPI = None def make_unity_env(env_directory, num_env, visual, start_index=0): \"\"\" Create a wrapped, monitored Unity environment. \"\"\" def make_env(rank, use_visual=True): # pylint: disable=C0111 def _thunk(): unity_env = UnityEnvironment(env_directory, base_port=5000 + rank) env = UnityToGymWrapper(unity_env, uint8_visual=True) env = Monitor(env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank))) return env return _thunk if visual: return SubprocVecEnv([make_env(i + start_index) for i in range(num_env)]) else: rank = MPI.COMM_WORLD.Get_rank() if MPI else 0 return DummyVecEnv([make_env(rank, use_visual=False)]) def main(): env = make_unity_env( < path - to - environment >, 4, True) ppo2.learn( network=\"mlp\", env=env, total_timesteps=100000, lr=1e-3, ) if __name__ == '__main__': main() Run Google Dopamine Algorithms Google provides a framework Dopamine , and implementations of algorithms, e.g. DQN, Rainbow, and the C51 variant of Rainbow. Using the Gym wrapper, we can run Unity environments using Dopamine. First, after installing the Gym wrapper, clone the Dopamine repository. git clone https://github.com/google/dopamine Then, follow the appropriate install instructions as specified on Dopamine's homepage . Note that the Dopamine guide specifies using a virtualenv. If you choose to do so, make sure your unity_env package is also installed within the same virtualenv as Dopamine. Adapting Dopamine's Scripts First, open dopamine/atari/run_experiment.py . Alternatively, copy the entire atari folder, and name it something else (e.g. unity ). If you choose the copy approach, be sure to change the package names in the import statements in train.py to your new directory. Within run_experiment.py , we will need to make changes to which environment is instantiated, just as in the Baselines example. At the top of the file, insert from mlagents_envs.environment import UnityEnvironment from mlagents_envs.envs import UnityToGymWrapper to import the Gym Wrapper. Navigate to the create_atari_environment method in the same file, and switch to instantiating a Unity environment by replacing the method with the following code. game_version = 'v0' if sticky_actions else 'v4' full_game_name = '{}NoFrameskip-{}'.format(game_name, game_version) unity_env = UnityEnvironment(<path-to-environment>) env = UnityToGymWrapper(unity_env, uint8_visual=True) return env <path-to-environment> is the path to your built Unity executable. For more information on building Unity environments, see here , and note the Limitations section below. Note that we are not using the preprocessor from Dopamine, as it uses many Atari-specific calls. Furthermore, frame-skipping can be done from within Unity, rather than on the Python side. Limitations Since Dopamine is designed around variants of DQN, it is only compatible with discrete action spaces, and specifically the Discrete Gym space. For environments that use branched discrete action spaces, you can enable the flatten_branched parameter in UnityToGymWrapper , which treats each combination of branched actions as separate actions. Furthermore, when building your environments, ensure that your Agent is using visual observations with greyscale enabled, and that the dimensions of the visual observations is 84 by 84 (matches the parameter found in dqn_agent.py and rainbow_agent.py ). Dopamine's agents currently do not automatically adapt to the observation dimensions or number of channels. Hyperparameters The hyperparameters provided by Dopamine are tailored to the Atari games, and you will likely need to adjust them for ML-Agents environments. Here is a sample dopamine/agents/rainbow/configs/rainbow.gin file that is known to work with a simple GridWorld. import dopamine.agents.rainbow.rainbow_agent import dopamine.unity.run_experiment import dopamine.replay_memory.prioritized_replay_buffer import gin.tf.external_configurables RainbowAgent.num_atoms = 51 RainbowAgent.stack_size = 1 RainbowAgent.vmax = 10. RainbowAgent.gamma = 0.99 RainbowAgent.update_horizon = 3 RainbowAgent.min_replay_history = 20000 # agent steps RainbowAgent.update_period = 5 RainbowAgent.target_update_period = 50 # agent steps RainbowAgent.epsilon_train = 0.1 RainbowAgent.epsilon_eval = 0.01 RainbowAgent.epsilon_decay_period = 50000 # agent steps RainbowAgent.replay_scheme = 'prioritized' RainbowAgent.tf_device = '/cpu:0' # use '/cpu:*' for non-GPU version RainbowAgent.optimizer = @tf.train.AdamOptimizer() tf.train.AdamOptimizer.learning_rate = 0.00025 tf.train.AdamOptimizer.epsilon = 0.0003125 Runner.game_name = \"Unity\" # any name can be used here Runner.sticky_actions = False Runner.num_iterations = 200 Runner.training_steps = 10000 # agent steps Runner.evaluation_steps = 500 # agent steps Runner.max_steps_per_episode = 27000 # agent steps WrappedPrioritizedReplayBuffer.replay_capacity = 1000000 WrappedPrioritizedReplayBuffer.batch_size = 32 This example assumed you copied atari to a separate folder named unity . Replace unity in import dopamine.unity.run_experiment with the folder you copied your run_experiment.py and trainer.py files to. If you directly modified the existing files, then use atari here. Starting a Run You can now run Dopamine as you would normally: python -um dopamine.unity.train \\ --agent_name=rainbow \\ --base_dir=/tmp/dopamine \\ --gin_files='dopamine/agents/rainbow/configs/rainbow.gin' Again, we assume that you've copied atari into a separate folder. Remember to replace unity with the directory you copied your files into. If you edited the Atari files directly, this should be atari . Example: GridWorld As a baseline, here are rewards over time for the three algorithms provided with Dopamine as run on the GridWorld example environment. All Dopamine (DQN, Rainbow, C51) runs were done with the same epsilon, epsilon decay, replay history, training steps, and buffer settings as specified above. Note that the first 20000 steps are used to pre-fill the training buffer, and no learning happens. We provide results from our PPO implementation and the DQN from Baselines as reference. Note that all runs used the same greyscale GridWorld as Dopamine. For PPO, num_layers was set to 2, and all other hyperparameters are the default for GridWorld in config/ppo/GridWorld.yaml . For Baselines DQN, the provided hyperparameters in the previous section are used. Note that Baselines implements certain features (e.g. dueling-Q) that are not enabled in Dopamine DQN.","title":"Getting started with the Gym API"},{"location":"Python-Gym-API/#unity-ml-agents-gym-wrapper","text":"A common way in which machine learning researchers interact with simulation environments is via a wrapper provided by OpenAI called gym . For more information on the gym interface, see here . We provide a gym wrapper and instructions for using it with existing machine learning algorithms which utilize gym. Our wrapper provides interfaces on top of our UnityEnvironment class, which is the default way of interfacing with a Unity environment via Python.","title":"Unity ML-Agents Gym Wrapper"},{"location":"Python-Gym-API/#installation","text":"The gym wrapper can be installed using: pip3 install gym_unity or by running the following from the /gym-unity directory of the repository: pip3 install -e .","title":"Installation"},{"location":"Python-Gym-API/#using-the-gym-wrapper","text":"The gym interface is available from gym_unity.envs . To launch an environment from the root of the project repository use: from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper env = UnityToGymWrapper(unity_env, uint8_visual, flatten_branched, allow_multiple_obs) unity_env refers to the Unity environment to be wrapped. uint8_visual refers to whether to output visual observations as uint8 values (0-255). Many common Gym environments (e.g. Atari) do this. By default they will be floats (0.0-1.0). Defaults to False . flatten_branched will flatten a branched discrete action space into a Gym Discrete. Otherwise, it will be converted into a MultiDiscrete. Defaults to False . allow_multiple_obs will return a list of observations. The first elements contain the visual observations and the last element contains the array of vector observations. If False the environment returns a single array (containing a single visual observations, if present, otherwise the vector observation). Defaults to False . action_space_seed is the optional seed for action sampling. If non-None, will be used to set the random seed on created gym.Space instances. The returned environment env will function as a gym.","title":"Using the Gym Wrapper"},{"location":"Python-Gym-API/#limitations","text":"It is only possible to use an environment with a single Agent. By default, the first visual observation is provided as the observation , if present. Otherwise, vector observations are provided. You can receive all visual and vector observations by using the allow_multiple_obs=True option in the gym parameters. If set to True , you will receive a list of observation instead of only one. The TerminalSteps or DecisionSteps output from the environment can still be accessed from the info provided by env.step(action) . Stacked vector observations are not supported. Environment registration for use with gym.make() is currently not supported. Calling env.render() will not render a new frame of the environment. It will return the latest visual observation if using visual observations.","title":"Limitations"},{"location":"Python-Gym-API/#running-openai-baselines-algorithms","text":"OpenAI provides a set of open-source maintained and tested Reinforcement Learning algorithms called the Baselines . Using the provided Gym wrapper, it is possible to train ML-Agents environments using these algorithms. This requires the creation of custom training scripts to launch each algorithm. In most cases these scripts can be created by making slight modifications to the ones provided for Atari and Mujoco environments. These examples were tested with baselines version 0.1.6.","title":"Running OpenAI Baselines Algorithms"},{"location":"Python-Gym-API/#example-dqn-baseline","text":"In order to train an agent to play the GridWorld environment using the Baselines DQN algorithm, you first need to install the baselines package using pip: pip install git+git://github.com/openai/baselines Next, create a file called train_unity.py . Then create an /envs/ directory and build the environment to that directory. For more information on building Unity environments, see here . Note that because of limitations of the DQN baseline, the environment must have a single visual observation, a single discrete action and a single Agent in the scene. Add the following code to the train_unity.py file: import gym from baselines import deepq from baselines import logger from mlagents_envs.environment import UnityEnvironment from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper def main(): unity_env = UnityEnvironment( < path - to - environment >) env = UnityToGymWrapper(unity_env, uint8_visual=True) logger.configure('./logs') # Change to log in a different directory act = deepq.learn( env, \"cnn\", # For visual inputs lr=2.5e-4, total_timesteps=1000000, buffer_size=50000, exploration_fraction=0.05, exploration_final_eps=0.1, print_freq=20, train_freq=5, learning_starts=20000, target_network_update_freq=50, gamma=0.99, prioritized_replay=False, checkpoint_freq=1000, checkpoint_path='./logs', # Change to save model in a different directory dueling=True ) print(\"Saving model to unity_model.pkl\") act.save(\"unity_model.pkl\") if __name__ == '__main__': main() To start the training process, run the following from the directory containing train_unity.py : python -m train_unity","title":"Example - DQN Baseline"},{"location":"Python-Gym-API/#other-algorithms","text":"Other algorithms in the Baselines repository can be run using scripts similar to the examples from the baselines package. In most cases, the primary changes needed to use a Unity environment are to import UnityToGymWrapper , and to replace the environment creation code, typically gym.make() , with a call to UnityToGymWrapper(unity_environment) passing the environment as input. A typical rule of thumb is that for vision-based environments, modification should be done to Atari training scripts, and for vector observation environments, modification should be done to Mujoco scripts. Some algorithms will make use of make_env() or make_mujoco_env() functions. You can define a similar function for Unity environments. An example of such a method using the PPO2 baseline: from mlagents_envs.environment import UnityEnvironment from mlagents_envs.envs import UnityToGymWrapper from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv from baselines.common.vec_env.dummy_vec_env import DummyVecEnv from baselines.bench import Monitor from baselines import logger import baselines.ppo2.ppo2 as ppo2 import os try: from mpi4py import MPI except ImportError: MPI = None def make_unity_env(env_directory, num_env, visual, start_index=0): \"\"\" Create a wrapped, monitored Unity environment. \"\"\" def make_env(rank, use_visual=True): # pylint: disable=C0111 def _thunk(): unity_env = UnityEnvironment(env_directory, base_port=5000 + rank) env = UnityToGymWrapper(unity_env, uint8_visual=True) env = Monitor(env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank))) return env return _thunk if visual: return SubprocVecEnv([make_env(i + start_index) for i in range(num_env)]) else: rank = MPI.COMM_WORLD.Get_rank() if MPI else 0 return DummyVecEnv([make_env(rank, use_visual=False)]) def main(): env = make_unity_env( < path - to - environment >, 4, True) ppo2.learn( network=\"mlp\", env=env, total_timesteps=100000, lr=1e-3, ) if __name__ == '__main__': main()","title":"Other Algorithms"},{"location":"Python-Gym-API/#run-google-dopamine-algorithms","text":"Google provides a framework Dopamine , and implementations of algorithms, e.g. DQN, Rainbow, and the C51 variant of Rainbow. Using the Gym wrapper, we can run Unity environments using Dopamine. First, after installing the Gym wrapper, clone the Dopamine repository. git clone https://github.com/google/dopamine Then, follow the appropriate install instructions as specified on Dopamine's homepage . Note that the Dopamine guide specifies using a virtualenv. If you choose to do so, make sure your unity_env package is also installed within the same virtualenv as Dopamine.","title":"Run Google Dopamine Algorithms"},{"location":"Python-Gym-API/#adapting-dopamines-scripts","text":"First, open dopamine/atari/run_experiment.py . Alternatively, copy the entire atari folder, and name it something else (e.g. unity ). If you choose the copy approach, be sure to change the package names in the import statements in train.py to your new directory. Within run_experiment.py , we will need to make changes to which environment is instantiated, just as in the Baselines example. At the top of the file, insert from mlagents_envs.environment import UnityEnvironment from mlagents_envs.envs import UnityToGymWrapper to import the Gym Wrapper. Navigate to the create_atari_environment method in the same file, and switch to instantiating a Unity environment by replacing the method with the following code. game_version = 'v0' if sticky_actions else 'v4' full_game_name = '{}NoFrameskip-{}'.format(game_name, game_version) unity_env = UnityEnvironment(<path-to-environment>) env = UnityToGymWrapper(unity_env, uint8_visual=True) return env <path-to-environment> is the path to your built Unity executable. For more information on building Unity environments, see here , and note the Limitations section below. Note that we are not using the preprocessor from Dopamine, as it uses many Atari-specific calls. Furthermore, frame-skipping can be done from within Unity, rather than on the Python side.","title":"Adapting Dopamine's Scripts"},{"location":"Python-Gym-API/#limitations_1","text":"Since Dopamine is designed around variants of DQN, it is only compatible with discrete action spaces, and specifically the Discrete Gym space. For environments that use branched discrete action spaces, you can enable the flatten_branched parameter in UnityToGymWrapper , which treats each combination of branched actions as separate actions. Furthermore, when building your environments, ensure that your Agent is using visual observations with greyscale enabled, and that the dimensions of the visual observations is 84 by 84 (matches the parameter found in dqn_agent.py and rainbow_agent.py ). Dopamine's agents currently do not automatically adapt to the observation dimensions or number of channels.","title":"Limitations"},{"location":"Python-Gym-API/#hyperparameters","text":"The hyperparameters provided by Dopamine are tailored to the Atari games, and you will likely need to adjust them for ML-Agents environments. Here is a sample dopamine/agents/rainbow/configs/rainbow.gin file that is known to work with a simple GridWorld. import dopamine.agents.rainbow.rainbow_agent import dopamine.unity.run_experiment import dopamine.replay_memory.prioritized_replay_buffer import gin.tf.external_configurables RainbowAgent.num_atoms = 51 RainbowAgent.stack_size = 1 RainbowAgent.vmax = 10. RainbowAgent.gamma = 0.99 RainbowAgent.update_horizon = 3 RainbowAgent.min_replay_history = 20000 # agent steps RainbowAgent.update_period = 5 RainbowAgent.target_update_period = 50 # agent steps RainbowAgent.epsilon_train = 0.1 RainbowAgent.epsilon_eval = 0.01 RainbowAgent.epsilon_decay_period = 50000 # agent steps RainbowAgent.replay_scheme = 'prioritized' RainbowAgent.tf_device = '/cpu:0' # use '/cpu:*' for non-GPU version RainbowAgent.optimizer = @tf.train.AdamOptimizer() tf.train.AdamOptimizer.learning_rate = 0.00025 tf.train.AdamOptimizer.epsilon = 0.0003125 Runner.game_name = \"Unity\" # any name can be used here Runner.sticky_actions = False Runner.num_iterations = 200 Runner.training_steps = 10000 # agent steps Runner.evaluation_steps = 500 # agent steps Runner.max_steps_per_episode = 27000 # agent steps WrappedPrioritizedReplayBuffer.replay_capacity = 1000000 WrappedPrioritizedReplayBuffer.batch_size = 32 This example assumed you copied atari to a separate folder named unity . Replace unity in import dopamine.unity.run_experiment with the folder you copied your run_experiment.py and trainer.py files to. If you directly modified the existing files, then use atari here.","title":"Hyperparameters"},{"location":"Python-Gym-API/#starting-a-run","text":"You can now run Dopamine as you would normally: python -um dopamine.unity.train \\ --agent_name=rainbow \\ --base_dir=/tmp/dopamine \\ --gin_files='dopamine/agents/rainbow/configs/rainbow.gin' Again, we assume that you've copied atari into a separate folder. Remember to replace unity with the directory you copied your files into. If you edited the Atari files directly, this should be atari .","title":"Starting a Run"},{"location":"Python-Gym-API/#example-gridworld","text":"As a baseline, here are rewards over time for the three algorithms provided with Dopamine as run on the GridWorld example environment. All Dopamine (DQN, Rainbow, C51) runs were done with the same epsilon, epsilon decay, replay history, training steps, and buffer settings as specified above. Note that the first 20000 steps are used to pre-fill the training buffer, and no learning happens. We provide results from our PPO implementation and the DQN from Baselines as reference. Note that all runs used the same greyscale GridWorld as Dopamine. For PPO, num_layers was set to 2, and all other hyperparameters are the default for GridWorld in config/ppo/GridWorld.yaml . For Baselines DQN, the provided hyperparameters in the previous section are used. Note that Baselines implements certain features (e.g. dueling-Q) that are not enabled in Dopamine DQN.","title":"Example: GridWorld"},{"location":"Python-LLAPI-Documentation/","text":"Table of Contents mlagents_envs.base_env DecisionStep DecisionSteps agent_id_to_index __getitem__ empty TerminalStep TerminalSteps agent_id_to_index __getitem__ empty ActionTuple discrete_dtype ActionSpec is_discrete is_continuous discrete_size empty_action random_action create_continuous create_discrete DimensionProperty UNSPECIFIED NONE TRANSLATIONAL_EQUIVARIANCE VARIABLE_SIZE ObservationType DEFAULT GOAL_SIGNAL ObservationSpec BehaviorSpec BaseEnv step reset close behavior_specs set_actions set_action_for_agent get_steps mlagents_envs.environment UnityEnvironment __init__ close mlagents_envs.registry mlagents_envs.registry.unity_env_registry UnityEnvRegistry register register_from_yaml clear __getitem__ mlagents_envs.side_channel mlagents_envs.side_channel.raw_bytes_channel RawBytesChannel on_message_received get_and_clear_received_messages send_raw_data mlagents_envs.side_channel.outgoing_message OutgoingMessage __init__ write_bool write_int32 write_float32 write_float32_list write_string set_raw_bytes mlagents_envs.side_channel.engine_configuration_channel EngineConfigurationChannel on_message_received set_configuration_parameters set_configuration mlagents_envs.side_channel.side_channel_manager SideChannelManager process_side_channel_message generate_side_channel_messages mlagents_envs.side_channel.stats_side_channel StatsSideChannel on_message_received get_and_reset_stats mlagents_envs.side_channel.incoming_message IncomingMessage __init__ read_bool read_int32 read_float32 read_float32_list read_string get_raw_bytes mlagents_envs.side_channel.float_properties_channel FloatPropertiesChannel on_message_received set_property get_property list_properties get_property_dict_copy mlagents_envs.side_channel.environment_parameters_channel EnvironmentParametersChannel set_float_parameter set_uniform_sampler_parameters set_gaussian_sampler_parameters set_multirangeuniform_sampler_parameters mlagents_envs.side_channel.side_channel SideChannel queue_message_to_send on_message_received channel_id mlagents_envs.base_env Python Environment API for the ML-Agents Toolkit The aim of this API is to expose Agents evolving in a simulation to perform reinforcement learning on. This API supports multi-agent scenarios and groups similar Agents (same observations, actions spaces and behavior) together. These groups of Agents are identified by their BehaviorName. For performance reasons, the data of each group of agents is processed in a batched manner. Agents are identified by a unique AgentId identifier that allows tracking of Agents across simulation steps. Note that there is no guarantee that the number or order of the Agents in the state will be consistent across simulation steps. A simulation steps corresponds to moving the simulation forward until at least one agent in the simulation sends its observations to Python again. Since Agents can request decisions at different frequencies, a simulation step does not necessarily correspond to a fixed simulation time increment. DecisionStep Objects class DecisionStep(NamedTuple) Contains the data a single Agent collected since the last simulation step. - obs is a list of numpy arrays observations collected by the agent. - reward is a float. Corresponds to the rewards collected by the agent since the last simulation step. - agent_id is an int and an unique identifier for the corresponding Agent. - action_mask is an optional list of one dimensional array of booleans. Only available when using multi-discrete actions. Each array corresponds to an action branch. Each array contains a mask for each action of the branch. If true, the action is not available for the agent during this simulation step. DecisionSteps Objects class DecisionSteps(Mapping) Contains the data a batch of similar Agents collected since the last simulation step. Note that all Agents do not necessarily have new information to send at each simulation step. Therefore, the ordering of agents and the batch size of the DecisionSteps are not fixed across simulation steps. - obs is a list of numpy arrays observations collected by the batch of agent. Each obs has one extra dimension compared to DecisionStep: the first dimension of the array corresponds to the batch size of the batch. - reward is a float vector of length batch size. Corresponds to the rewards collected by each agent since the last simulation step. - agent_id is an int vector of length batch size containing unique identifier for the corresponding Agent. This is used to track Agents across simulation steps. - action_mask is an optional list of two dimensional array of booleans. Only available when using multi-discrete actions. Each array corresponds to an action branch. The first dimension of each array is the batch size and the second contains a mask for each action of the branch. If true, the action is not available for the agent during this simulation step. agent_id_to_index | @property | agent_id_to_index() -> Dict[AgentId, int] Returns : A Dict that maps agent_id to the index of those agents in this DecisionSteps. __getitem__ | __getitem__(agent_id: AgentId) -> DecisionStep returns the DecisionStep for a specific agent. Arguments : agent_id : The id of the agent Returns : The DecisionStep empty | @staticmethod | empty(spec: \"BehaviorSpec\") -> \"DecisionSteps\" Returns an empty DecisionSteps. Arguments : spec : The BehaviorSpec for the DecisionSteps TerminalStep Objects class TerminalStep(NamedTuple) Contains the data a single Agent collected when its episode ended. - obs is a list of numpy arrays observations collected by the agent. - reward is a float. Corresponds to the rewards collected by the agent since the last simulation step. - interrupted is a bool. Is true if the Agent was interrupted since the last decision step. For example, if the Agent reached the maximum number of steps for the episode. - agent_id is an int and an unique identifier for the corresponding Agent. TerminalSteps Objects class TerminalSteps(Mapping) Contains the data a batch of Agents collected when their episode terminated. All Agents present in the TerminalSteps have ended their episode. - obs is a list of numpy arrays observations collected by the batch of agent. Each obs has one extra dimension compared to DecisionStep: the first dimension of the array corresponds to the batch size of the batch. - reward is a float vector of length batch size. Corresponds to the rewards collected by each agent since the last simulation step. - interrupted is an array of booleans of length batch size. Is true if the associated Agent was interrupted since the last decision step. For example, if the Agent reached the maximum number of steps for the episode. - agent_id is an int vector of length batch size containing unique identifier for the corresponding Agent. This is used to track Agents across simulation steps. agent_id_to_index | @property | agent_id_to_index() -> Dict[AgentId, int] Returns : A Dict that maps agent_id to the index of those agents in this TerminalSteps. __getitem__ | __getitem__(agent_id: AgentId) -> TerminalStep returns the TerminalStep for a specific agent. Arguments : agent_id : The id of the agent Returns : obs, reward, done, agent_id and optional action mask for a specific agent empty | @staticmethod | empty(spec: \"BehaviorSpec\") -> \"TerminalSteps\" Returns an empty TerminalSteps. Arguments : spec : The BehaviorSpec for the TerminalSteps ActionTuple Objects class ActionTuple(_ActionTupleBase) An object whose fields correspond to actions of different types. Continuous and discrete actions are numpy arrays of type float32 and int32, respectively and are type checked on construction. Dimensions are of (n_agents, continuous_size) and (n_agents, discrete_size), respectively. Note, this also holds when continuous or discrete size is zero. discrete_dtype | @property | discrete_dtype() -> np.dtype The dtype of a discrete action. ActionSpec Objects class ActionSpec(NamedTuple) A NamedTuple containing utility functions and information about the action spaces for a group of Agents under the same behavior. - num_continuous_actions is an int corresponding to the number of floats which constitute the action. - discrete_branch_sizes is a Tuple of int where each int corresponds to the number of discrete actions available to the agent on an independent action branch. is_discrete | is_discrete() -> bool Returns true if this Behavior uses discrete actions is_continuous | is_continuous() -> bool Returns true if this Behavior uses continuous actions discrete_size | @property | discrete_size() -> int Returns a an int corresponding to the number of discrete branches. empty_action | empty_action(n_agents: int) -> ActionTuple Generates ActionTuple corresponding to an empty action (all zeros) for a number of agents. Arguments : n_agents : The number of agents that will have actions generated random_action | random_action(n_agents: int) -> ActionTuple Generates ActionTuple corresponding to a random action (either discrete or continuous) for a number of agents. Arguments : n_agents : The number of agents that will have actions generated create_continuous | @staticmethod | create_continuous(continuous_size: int) -> \"ActionSpec\" Creates an ActionSpec that is homogenously continuous create_discrete | @staticmethod | create_discrete(discrete_branches: Tuple[int]) -> \"ActionSpec\" Creates an ActionSpec that is homogenously discrete DimensionProperty Objects class DimensionProperty(IntFlag) The dimension property of a dimension of an observation. UNSPECIFIED No properties specified. NONE No Property of the observation in that dimension. Observation can be processed with Fully connected networks. TRANSLATIONAL_EQUIVARIANCE Means it is suitable to do a convolution in this dimension. VARIABLE_SIZE Means that there can be a variable number of observations in this dimension. The observations are unordered. ObservationType Objects class ObservationType(Enum) An Enum which defines the type of information carried in the observation of the agent. DEFAULT Observation information is generic. GOAL_SIGNAL Observation contains goal information for current task. ObservationSpec Objects class ObservationSpec(NamedTuple) A NamedTuple containing information about the observation of Agents. - shape is a Tuple of int : It corresponds to the shape of an observation's dimensions. - dimension_property is a Tuple of DimensionProperties flag, one flag for each dimension. - observation_type is an enum of ObservationType. BehaviorSpec Objects class BehaviorSpec(NamedTuple) A NamedTuple containing information about the observation and action spaces for a group of Agents under the same behavior. - observation_specs is a List of ObservationSpec NamedTuple containing information about the information of the Agent's observations such as their shapes. The order of the ObservationSpec is the same as the order of the observations of an agent. - action_spec is an ActionSpec NamedTuple. BaseEnv Objects class BaseEnv(ABC) step | @abstractmethod | step() -> None Signals the environment that it must move the simulation forward by one step. reset | @abstractmethod | reset() -> None Signals the environment that it must reset the simulation. close | @abstractmethod | close() -> None Signals the environment that it must close. behavior_specs | @property | @abstractmethod | behavior_specs() -> MappingType[str, BehaviorSpec] Returns a Mapping from behavior names to behavior specs. Agents grouped under the same behavior name have the same action and observation specs, and are expected to behave similarly in the environment. Note that new keys can be added to this mapping as new policies are instantiated. set_actions | @abstractmethod | set_actions(behavior_name: BehaviorName, action: ActionTuple) -> None Sets the action for all of the agents in the simulation for the next step. The Actions must be in the same order as the order received in the DecisionSteps. Arguments : behavior_name : The name of the behavior the agents are part of action : ActionTuple tuple of continuous and/or discrete action. Actions are np.arrays with dimensions (n_agents, continuous_size) and (n_agents, discrete_size), respectively. set_action_for_agent | @abstractmethod | set_action_for_agent(behavior_name: BehaviorName, agent_id: AgentId, action: ActionTuple) -> None Sets the action for one of the agents in the simulation for the next step. Arguments : behavior_name : The name of the behavior the agent is part of agent_id : The id of the agent the action is set for action : ActionTuple tuple of continuous and/or discrete action Actions are np.arrays with dimensions (1, continuous_size) and (1, discrete_size), respectively. Note, this initial dimensions of 1 is because this action is meant for a single agent. get_steps | @abstractmethod | get_steps(behavior_name: BehaviorName) -> Tuple[DecisionSteps, TerminalSteps] Retrieves the steps of the agents that requested a step in the simulation. Arguments : behavior_name : The name of the behavior the agents are part of Returns : A tuple containing : - A DecisionSteps NamedTuple containing the observations, the rewards, the agent ids and the action masks for the Agents of the specified behavior. These Agents need an action this step. - A TerminalSteps NamedTuple containing the observations, rewards, agent ids and interrupted flags of the agents that had their episode terminated last step. mlagents_envs.environment UnityEnvironment Objects class UnityEnvironment(BaseEnv) __init__ | __init__(file_name: Optional[str] = None, worker_id: int = 0, base_port: Optional[int] = None, seed: int = 0, no_graphics: bool = False, timeout_wait: int = 60, additional_args: Optional[List[str]] = None, side_channels: Optional[List[SideChannel]] = None, log_folder: Optional[str] = None, num_areas: int = 1) Starts a new unity environment and establishes a connection with the environment. Notice: Currently communication between Unity and Python takes place over an open socket without authentication. Ensure that the network where training takes place is secure. :string file_name: Name of Unity environment binary. :int base_port: Baseline port number to connect to Unity environment over. worker_id increments over this. If no environment is specified (i.e. file_name is None), the DEFAULT_EDITOR_PORT will be used. :int worker_id: Offset from base_port. Used for training multiple environments simultaneously. :bool no_graphics: Whether to run the Unity simulator in no-graphics mode :int timeout_wait: Time (in seconds) to wait for connection from environment. :list args: Addition Unity command line arguments :list side_channels: Additional side channel for no-rl communication with Unity :str log_folder: Optional folder to write the Unity Player log file into. Requires absolute path. close | close() Sends a shutdown signal to the unity environment, and closes the socket connection. mlagents_envs.registry mlagents_envs.registry.unity_env_registry UnityEnvRegistry Objects class UnityEnvRegistry(Mapping) UnityEnvRegistry Provides a library of Unity environments that can be launched without the need of downloading the Unity Editor. The UnityEnvRegistry implements a Map, to access an entry of the Registry, use: registry = UnityEnvRegistry() entry = registry[<environment_identifyier>] An entry has the following properties : * identifier : Uniquely identifies this environment * expected_reward : Corresponds to the reward an agent must obtained for the task to be considered completed. * description : A human readable description of the environment. To launch a Unity environment from a registry entry, use the make method: registry = UnityEnvRegistry() env = registry[<environment_identifyier>].make() register | register(new_entry: BaseRegistryEntry) -> None Registers a new BaseRegistryEntry to the registry. The BaseRegistryEntry.identifier value will be used as indexing key. If two are more environments are registered under the same key, the most recentry added will replace the others. register_from_yaml | register_from_yaml(path_to_yaml: str) -> None Registers the environments listed in a yaml file (either local or remote). Note that the entries are registered lazily: the registration will only happen when an environment is accessed. The yaml file must have the following format : environments: - <identifier of the first environment>: expected_reward: <expected reward of the environment> description: | <a multi line description of the environment> <continued multi line description> linux_url: <The url for the Linux executable zip file> darwin_url: <The url for the OSX executable zip file> win_url: <The url for the Windows executable zip file> - <identifier of the second environment>: expected_reward: <expected reward of the environment> description: | <a multi line description of the environment> <continued multi line description> linux_url: <The url for the Linux executable zip file> darwin_url: <The url for the OSX executable zip file> win_url: <The url for the Windows executable zip file> - ... Arguments : path_to_yaml : A local path or url to the yaml file clear | clear() -> None Deletes all entries in the registry. __getitem__ | __getitem__(identifier: str) -> BaseRegistryEntry Returns the BaseRegistryEntry with the provided identifier. BaseRegistryEntry can then be used to make a Unity Environment. Arguments : identifier : The identifier of the BaseRegistryEntry Returns : The associated BaseRegistryEntry mlagents_envs.side_channel mlagents_envs.side_channel.raw_bytes_channel RawBytesChannel Objects class RawBytesChannel(SideChannel) This is an example of what the SideChannel for raw bytes exchange would look like. Is meant to be used for general research purpose. on_message_received | on_message_received(msg: IncomingMessage) -> None Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel. get_and_clear_received_messages | get_and_clear_received_messages() -> List[bytes] returns a list of bytearray received from the environment. send_raw_data | send_raw_data(data: bytearray) -> None Queues a message to be sent by the environment at the next call to step. mlagents_envs.side_channel.outgoing_message OutgoingMessage Objects class OutgoingMessage() Utility class for forming the message that is written to a SideChannel. All data is written in little-endian format using the struct module. __init__ | __init__() Create an OutgoingMessage with an empty buffer. write_bool | write_bool(b: bool) -> None Append a boolean value. write_int32 | write_int32(i: int) -> None Append an integer value. write_float32 | write_float32(f: float) -> None Append a float value. It will be truncated to 32-bit precision. write_float32_list | write_float32_list(float_list: List[float]) -> None Append a list of float values. They will be truncated to 32-bit precision. write_string | write_string(s: str) -> None Append a string value. Internally, it will be encoded to ascii, and the encoded length will also be written to the message. set_raw_bytes | set_raw_bytes(buffer: bytearray) -> None Set the internal buffer to a new bytearray. This will overwrite any existing data. Arguments : buffer : Returns : mlagents_envs.side_channel.engine_configuration_channel EngineConfigurationChannel Objects class EngineConfigurationChannel(SideChannel) This is the SideChannel for engine configuration exchange. The data in the engine configuration is as follows : - int width; - int height; - int qualityLevel; - float timeScale; - int targetFrameRate; - int captureFrameRate; on_message_received | on_message_received(msg: IncomingMessage) -> None Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel. Note that Python should never receive an engine configuration from Unity set_configuration_parameters | set_configuration_parameters(width: Optional[int] = None, height: Optional[int] = None, quality_level: Optional[int] = None, time_scale: Optional[float] = None, target_frame_rate: Optional[int] = None, capture_frame_rate: Optional[int] = None) -> None Sets the engine configuration. Takes as input the configurations of the engine. Arguments : width : Defines the width of the display. (Must be set alongside height) height : Defines the height of the display. (Must be set alongside width) quality_level : Defines the quality level of the simulation. time_scale : Defines the multiplier for the deltatime in the simulation. If set to a higher value, time will pass faster in the simulation but the physics might break. target_frame_rate : Instructs simulation to try to render at a specified frame rate. capture_frame_rate : Instructs the simulation to consider time between updates to always be constant, regardless of the actual frame rate. set_configuration | set_configuration(config: EngineConfig) -> None Sets the engine configuration. Takes as input an EngineConfig. mlagents_envs.side_channel.side_channel_manager SideChannelManager Objects class SideChannelManager() process_side_channel_message | process_side_channel_message(data: bytes) -> None Separates the data received from Python into individual messages for each registered side channel and calls on_message_received on them. Arguments : data : The packed message sent by Unity generate_side_channel_messages | generate_side_channel_messages() -> bytearray Gathers the messages that the registered side channels will send to Unity and combines them into a single message ready to be sent. mlagents_envs.side_channel.stats_side_channel StatsSideChannel Objects class StatsSideChannel(SideChannel) Side channel that receives (string, float) pairs from the environment, so that they can eventually be passed to a StatsReporter. on_message_received | on_message_received(msg: IncomingMessage) -> None Receive the message from the environment, and save it for later retrieval. Arguments : msg : Returns : get_and_reset_stats | get_and_reset_stats() -> EnvironmentStats Returns the current stats, and resets the internal storage of the stats. Returns : mlagents_envs.side_channel.incoming_message IncomingMessage Objects class IncomingMessage() Utility class for reading the message written to a SideChannel. Values must be read in the order they were written. __init__ | __init__(buffer: bytes, offset: int = 0) Create a new IncomingMessage from the bytes. read_bool | read_bool(default_value: bool = False) -> bool Read a boolean value from the message buffer. Arguments : default_value : Default value to use if the end of the message is reached. Returns : The value read from the message, or the default value if the end was reached. read_int32 | read_int32(default_value: int = 0) -> int Read an integer value from the message buffer. Arguments : default_value : Default value to use if the end of the message is reached. Returns : The value read from the message, or the default value if the end was reached. read_float32 | read_float32(default_value: float = 0.0) -> float Read a float value from the message buffer. Arguments : default_value : Default value to use if the end of the message is reached. Returns : The value read from the message, or the default value if the end was reached. read_float32_list | read_float32_list(default_value: List[float] = None) -> List[float] Read a list of float values from the message buffer. Arguments : default_value : Default value to use if the end of the message is reached. Returns : The value read from the message, or the default value if the end was reached. read_string | read_string(default_value: str = \"\") -> str Read a string value from the message buffer. Arguments : default_value : Default value to use if the end of the message is reached. Returns : The value read from the message, or the default value if the end was reached. get_raw_bytes | get_raw_bytes() -> bytes Get a copy of the internal bytes used by the message. mlagents_envs.side_channel.float_properties_channel FloatPropertiesChannel Objects class FloatPropertiesChannel(SideChannel) This is the SideChannel for float properties shared with Unity. You can modify the float properties of an environment with the commands set_property, get_property and list_properties. on_message_received | on_message_received(msg: IncomingMessage) -> None Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel. set_property | set_property(key: str, value: float) -> None Sets a property in the Unity Environment. Arguments : key : The string identifier of the property. value : The float value of the property. get_property | get_property(key: str) -> Optional[float] Gets a property in the Unity Environment. If the property was not found, will return None. Arguments : key : The string identifier of the property. Returns : The float value of the property or None. list_properties | list_properties() -> List[str] Returns a list of all the string identifiers of the properties currently present in the Unity Environment. get_property_dict_copy | get_property_dict_copy() -> Dict[str, float] Returns a copy of the float properties. Returns : mlagents_envs.side_channel.environment_parameters_channel EnvironmentParametersChannel Objects class EnvironmentParametersChannel(SideChannel) This is the SideChannel for sending environment parameters to Unity. You can send parameters to an environment with the command set_float_parameter. set_float_parameter | set_float_parameter(key: str, value: float) -> None Sets a float environment parameter in the Unity Environment. Arguments : key : The string identifier of the parameter. value : The float value of the parameter. set_uniform_sampler_parameters | set_uniform_sampler_parameters(key: str, min_value: float, max_value: float, seed: int) -> None Sets a uniform environment parameter sampler. Arguments : key : The string identifier of the parameter. min_value : The minimum of the sampling distribution. max_value : The maximum of the sampling distribution. seed : The random seed to initialize the sampler. set_gaussian_sampler_parameters | set_gaussian_sampler_parameters(key: str, mean: float, st_dev: float, seed: int) -> None Sets a gaussian environment parameter sampler. Arguments : key : The string identifier of the parameter. mean : The mean of the sampling distribution. st_dev : The standard deviation of the sampling distribution. seed : The random seed to initialize the sampler. set_multirangeuniform_sampler_parameters | set_multirangeuniform_sampler_parameters(key: str, intervals: List[Tuple[float, float]], seed: int) -> None Sets a multirangeuniform environment parameter sampler. Arguments : key : The string identifier of the parameter. intervals : The lists of min and max that define each uniform distribution. seed : The random seed to initialize the sampler. mlagents_envs.side_channel.side_channel SideChannel Objects class SideChannel(ABC) The side channel just get access to a bytes buffer that will be shared between C# and Python. For example, We will create a specific side channel for properties that will be a list of string (fixed size) to float number, that can be modified by both C# and Python. All side channels are passed to the Env object at construction. queue_message_to_send | queue_message_to_send(msg: OutgoingMessage) -> None Queues a message to be sent by the environment at the next call to step. on_message_received | @abstractmethod | on_message_received(msg: IncomingMessage) -> None Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel. channel_id | @property | channel_id() -> uuid.UUID Returns : The type of side channel used. Will influence how the data is processed in the environment.","title":"LLAPI Documentation"},{"location":"Python-LLAPI-Documentation/#table-of-contents","text":"mlagents_envs.base_env DecisionStep DecisionSteps agent_id_to_index __getitem__ empty TerminalStep TerminalSteps agent_id_to_index __getitem__ empty ActionTuple discrete_dtype ActionSpec is_discrete is_continuous discrete_size empty_action random_action create_continuous create_discrete DimensionProperty UNSPECIFIED NONE TRANSLATIONAL_EQUIVARIANCE VARIABLE_SIZE ObservationType DEFAULT GOAL_SIGNAL ObservationSpec BehaviorSpec BaseEnv step reset close behavior_specs set_actions set_action_for_agent get_steps mlagents_envs.environment UnityEnvironment __init__ close mlagents_envs.registry mlagents_envs.registry.unity_env_registry UnityEnvRegistry register register_from_yaml clear __getitem__ mlagents_envs.side_channel mlagents_envs.side_channel.raw_bytes_channel RawBytesChannel on_message_received get_and_clear_received_messages send_raw_data mlagents_envs.side_channel.outgoing_message OutgoingMessage __init__ write_bool write_int32 write_float32 write_float32_list write_string set_raw_bytes mlagents_envs.side_channel.engine_configuration_channel EngineConfigurationChannel on_message_received set_configuration_parameters set_configuration mlagents_envs.side_channel.side_channel_manager SideChannelManager process_side_channel_message generate_side_channel_messages mlagents_envs.side_channel.stats_side_channel StatsSideChannel on_message_received get_and_reset_stats mlagents_envs.side_channel.incoming_message IncomingMessage __init__ read_bool read_int32 read_float32 read_float32_list read_string get_raw_bytes mlagents_envs.side_channel.float_properties_channel FloatPropertiesChannel on_message_received set_property get_property list_properties get_property_dict_copy mlagents_envs.side_channel.environment_parameters_channel EnvironmentParametersChannel set_float_parameter set_uniform_sampler_parameters set_gaussian_sampler_parameters set_multirangeuniform_sampler_parameters mlagents_envs.side_channel.side_channel SideChannel queue_message_to_send on_message_received channel_id","title":"Table of Contents"},{"location":"Python-LLAPI-Documentation/#mlagents_envsbase_env","text":"Python Environment API for the ML-Agents Toolkit The aim of this API is to expose Agents evolving in a simulation to perform reinforcement learning on. This API supports multi-agent scenarios and groups similar Agents (same observations, actions spaces and behavior) together. These groups of Agents are identified by their BehaviorName. For performance reasons, the data of each group of agents is processed in a batched manner. Agents are identified by a unique AgentId identifier that allows tracking of Agents across simulation steps. Note that there is no guarantee that the number or order of the Agents in the state will be consistent across simulation steps. A simulation steps corresponds to moving the simulation forward until at least one agent in the simulation sends its observations to Python again. Since Agents can request decisions at different frequencies, a simulation step does not necessarily correspond to a fixed simulation time increment.","title":"mlagents_envs.base_env"},{"location":"Python-LLAPI-Documentation/#decisionstep-objects","text":"class DecisionStep(NamedTuple) Contains the data a single Agent collected since the last simulation step. - obs is a list of numpy arrays observations collected by the agent. - reward is a float. Corresponds to the rewards collected by the agent since the last simulation step. - agent_id is an int and an unique identifier for the corresponding Agent. - action_mask is an optional list of one dimensional array of booleans. Only available when using multi-discrete actions. Each array corresponds to an action branch. Each array contains a mask for each action of the branch. If true, the action is not available for the agent during this simulation step.","title":"DecisionStep Objects"},{"location":"Python-LLAPI-Documentation/#decisionsteps-objects","text":"class DecisionSteps(Mapping) Contains the data a batch of similar Agents collected since the last simulation step. Note that all Agents do not necessarily have new information to send at each simulation step. Therefore, the ordering of agents and the batch size of the DecisionSteps are not fixed across simulation steps. - obs is a list of numpy arrays observations collected by the batch of agent. Each obs has one extra dimension compared to DecisionStep: the first dimension of the array corresponds to the batch size of the batch. - reward is a float vector of length batch size. Corresponds to the rewards collected by each agent since the last simulation step. - agent_id is an int vector of length batch size containing unique identifier for the corresponding Agent. This is used to track Agents across simulation steps. - action_mask is an optional list of two dimensional array of booleans. Only available when using multi-discrete actions. Each array corresponds to an action branch. The first dimension of each array is the batch size and the second contains a mask for each action of the branch. If true, the action is not available for the agent during this simulation step.","title":"DecisionSteps Objects"},{"location":"Python-LLAPI-Documentation/#agent_id_to_index","text":"| @property | agent_id_to_index() -> Dict[AgentId, int] Returns : A Dict that maps agent_id to the index of those agents in this DecisionSteps.","title":"agent_id_to_index"},{"location":"Python-LLAPI-Documentation/#__getitem__","text":"| __getitem__(agent_id: AgentId) -> DecisionStep returns the DecisionStep for a specific agent. Arguments : agent_id : The id of the agent Returns : The DecisionStep","title":"__getitem__"},{"location":"Python-LLAPI-Documentation/#empty","text":"| @staticmethod | empty(spec: \"BehaviorSpec\") -> \"DecisionSteps\" Returns an empty DecisionSteps. Arguments : spec : The BehaviorSpec for the DecisionSteps","title":"empty"},{"location":"Python-LLAPI-Documentation/#terminalstep-objects","text":"class TerminalStep(NamedTuple) Contains the data a single Agent collected when its episode ended. - obs is a list of numpy arrays observations collected by the agent. - reward is a float. Corresponds to the rewards collected by the agent since the last simulation step. - interrupted is a bool. Is true if the Agent was interrupted since the last decision step. For example, if the Agent reached the maximum number of steps for the episode. - agent_id is an int and an unique identifier for the corresponding Agent.","title":"TerminalStep Objects"},{"location":"Python-LLAPI-Documentation/#terminalsteps-objects","text":"class TerminalSteps(Mapping) Contains the data a batch of Agents collected when their episode terminated. All Agents present in the TerminalSteps have ended their episode. - obs is a list of numpy arrays observations collected by the batch of agent. Each obs has one extra dimension compared to DecisionStep: the first dimension of the array corresponds to the batch size of the batch. - reward is a float vector of length batch size. Corresponds to the rewards collected by each agent since the last simulation step. - interrupted is an array of booleans of length batch size. Is true if the associated Agent was interrupted since the last decision step. For example, if the Agent reached the maximum number of steps for the episode. - agent_id is an int vector of length batch size containing unique identifier for the corresponding Agent. This is used to track Agents across simulation steps.","title":"TerminalSteps Objects"},{"location":"Python-LLAPI-Documentation/#agent_id_to_index_1","text":"| @property | agent_id_to_index() -> Dict[AgentId, int] Returns : A Dict that maps agent_id to the index of those agents in this TerminalSteps.","title":"agent_id_to_index"},{"location":"Python-LLAPI-Documentation/#__getitem___1","text":"| __getitem__(agent_id: AgentId) -> TerminalStep returns the TerminalStep for a specific agent. Arguments : agent_id : The id of the agent Returns : obs, reward, done, agent_id and optional action mask for a specific agent","title":"__getitem__"},{"location":"Python-LLAPI-Documentation/#empty_1","text":"| @staticmethod | empty(spec: \"BehaviorSpec\") -> \"TerminalSteps\" Returns an empty TerminalSteps. Arguments : spec : The BehaviorSpec for the TerminalSteps","title":"empty"},{"location":"Python-LLAPI-Documentation/#actiontuple-objects","text":"class ActionTuple(_ActionTupleBase) An object whose fields correspond to actions of different types. Continuous and discrete actions are numpy arrays of type float32 and int32, respectively and are type checked on construction. Dimensions are of (n_agents, continuous_size) and (n_agents, discrete_size), respectively. Note, this also holds when continuous or discrete size is zero.","title":"ActionTuple Objects"},{"location":"Python-LLAPI-Documentation/#discrete_dtype","text":"| @property | discrete_dtype() -> np.dtype The dtype of a discrete action.","title":"discrete_dtype"},{"location":"Python-LLAPI-Documentation/#actionspec-objects","text":"class ActionSpec(NamedTuple) A NamedTuple containing utility functions and information about the action spaces for a group of Agents under the same behavior. - num_continuous_actions is an int corresponding to the number of floats which constitute the action. - discrete_branch_sizes is a Tuple of int where each int corresponds to the number of discrete actions available to the agent on an independent action branch.","title":"ActionSpec Objects"},{"location":"Python-LLAPI-Documentation/#is_discrete","text":"| is_discrete() -> bool Returns true if this Behavior uses discrete actions","title":"is_discrete"},{"location":"Python-LLAPI-Documentation/#is_continuous","text":"| is_continuous() -> bool Returns true if this Behavior uses continuous actions","title":"is_continuous"},{"location":"Python-LLAPI-Documentation/#discrete_size","text":"| @property | discrete_size() -> int Returns a an int corresponding to the number of discrete branches.","title":"discrete_size"},{"location":"Python-LLAPI-Documentation/#empty_action","text":"| empty_action(n_agents: int) -> ActionTuple Generates ActionTuple corresponding to an empty action (all zeros) for a number of agents. Arguments : n_agents : The number of agents that will have actions generated","title":"empty_action"},{"location":"Python-LLAPI-Documentation/#random_action","text":"| random_action(n_agents: int) -> ActionTuple Generates ActionTuple corresponding to a random action (either discrete or continuous) for a number of agents. Arguments : n_agents : The number of agents that will have actions generated","title":"random_action"},{"location":"Python-LLAPI-Documentation/#create_continuous","text":"| @staticmethod | create_continuous(continuous_size: int) -> \"ActionSpec\" Creates an ActionSpec that is homogenously continuous","title":"create_continuous"},{"location":"Python-LLAPI-Documentation/#create_discrete","text":"| @staticmethod | create_discrete(discrete_branches: Tuple[int]) -> \"ActionSpec\" Creates an ActionSpec that is homogenously discrete","title":"create_discrete"},{"location":"Python-LLAPI-Documentation/#dimensionproperty-objects","text":"class DimensionProperty(IntFlag) The dimension property of a dimension of an observation.","title":"DimensionProperty Objects"},{"location":"Python-LLAPI-Documentation/#unspecified","text":"No properties specified.","title":"UNSPECIFIED"},{"location":"Python-LLAPI-Documentation/#none","text":"No Property of the observation in that dimension. Observation can be processed with Fully connected networks.","title":"NONE"},{"location":"Python-LLAPI-Documentation/#translational_equivariance","text":"Means it is suitable to do a convolution in this dimension.","title":"TRANSLATIONAL_EQUIVARIANCE"},{"location":"Python-LLAPI-Documentation/#variable_size","text":"Means that there can be a variable number of observations in this dimension. The observations are unordered.","title":"VARIABLE_SIZE"},{"location":"Python-LLAPI-Documentation/#observationtype-objects","text":"class ObservationType(Enum) An Enum which defines the type of information carried in the observation of the agent.","title":"ObservationType Objects"},{"location":"Python-LLAPI-Documentation/#default","text":"Observation information is generic.","title":"DEFAULT"},{"location":"Python-LLAPI-Documentation/#goal_signal","text":"Observation contains goal information for current task.","title":"GOAL_SIGNAL"},{"location":"Python-LLAPI-Documentation/#observationspec-objects","text":"class ObservationSpec(NamedTuple) A NamedTuple containing information about the observation of Agents. - shape is a Tuple of int : It corresponds to the shape of an observation's dimensions. - dimension_property is a Tuple of DimensionProperties flag, one flag for each dimension. - observation_type is an enum of ObservationType.","title":"ObservationSpec Objects"},{"location":"Python-LLAPI-Documentation/#behaviorspec-objects","text":"class BehaviorSpec(NamedTuple) A NamedTuple containing information about the observation and action spaces for a group of Agents under the same behavior. - observation_specs is a List of ObservationSpec NamedTuple containing information about the information of the Agent's observations such as their shapes. The order of the ObservationSpec is the same as the order of the observations of an agent. - action_spec is an ActionSpec NamedTuple.","title":"BehaviorSpec Objects"},{"location":"Python-LLAPI-Documentation/#baseenv-objects","text":"class BaseEnv(ABC)","title":"BaseEnv Objects"},{"location":"Python-LLAPI-Documentation/#step","text":"| @abstractmethod | step() -> None Signals the environment that it must move the simulation forward by one step.","title":"step"},{"location":"Python-LLAPI-Documentation/#reset","text":"| @abstractmethod | reset() -> None Signals the environment that it must reset the simulation.","title":"reset"},{"location":"Python-LLAPI-Documentation/#close","text":"| @abstractmethod | close() -> None Signals the environment that it must close.","title":"close"},{"location":"Python-LLAPI-Documentation/#behavior_specs","text":"| @property | @abstractmethod | behavior_specs() -> MappingType[str, BehaviorSpec] Returns a Mapping from behavior names to behavior specs. Agents grouped under the same behavior name have the same action and observation specs, and are expected to behave similarly in the environment. Note that new keys can be added to this mapping as new policies are instantiated.","title":"behavior_specs"},{"location":"Python-LLAPI-Documentation/#set_actions","text":"| @abstractmethod | set_actions(behavior_name: BehaviorName, action: ActionTuple) -> None Sets the action for all of the agents in the simulation for the next step. The Actions must be in the same order as the order received in the DecisionSteps. Arguments : behavior_name : The name of the behavior the agents are part of action : ActionTuple tuple of continuous and/or discrete action. Actions are np.arrays with dimensions (n_agents, continuous_size) and (n_agents, discrete_size), respectively.","title":"set_actions"},{"location":"Python-LLAPI-Documentation/#set_action_for_agent","text":"| @abstractmethod | set_action_for_agent(behavior_name: BehaviorName, agent_id: AgentId, action: ActionTuple) -> None Sets the action for one of the agents in the simulation for the next step. Arguments : behavior_name : The name of the behavior the agent is part of agent_id : The id of the agent the action is set for action : ActionTuple tuple of continuous and/or discrete action Actions are np.arrays with dimensions (1, continuous_size) and (1, discrete_size), respectively. Note, this initial dimensions of 1 is because this action is meant for a single agent.","title":"set_action_for_agent"},{"location":"Python-LLAPI-Documentation/#get_steps","text":"| @abstractmethod | get_steps(behavior_name: BehaviorName) -> Tuple[DecisionSteps, TerminalSteps] Retrieves the steps of the agents that requested a step in the simulation. Arguments : behavior_name : The name of the behavior the agents are part of Returns : A tuple containing : - A DecisionSteps NamedTuple containing the observations, the rewards, the agent ids and the action masks for the Agents of the specified behavior. These Agents need an action this step. - A TerminalSteps NamedTuple containing the observations, rewards, agent ids and interrupted flags of the agents that had their episode terminated last step.","title":"get_steps"},{"location":"Python-LLAPI-Documentation/#mlagents_envsenvironment","text":"","title":"mlagents_envs.environment"},{"location":"Python-LLAPI-Documentation/#unityenvironment-objects","text":"class UnityEnvironment(BaseEnv)","title":"UnityEnvironment Objects"},{"location":"Python-LLAPI-Documentation/#__init__","text":"| __init__(file_name: Optional[str] = None, worker_id: int = 0, base_port: Optional[int] = None, seed: int = 0, no_graphics: bool = False, timeout_wait: int = 60, additional_args: Optional[List[str]] = None, side_channels: Optional[List[SideChannel]] = None, log_folder: Optional[str] = None, num_areas: int = 1) Starts a new unity environment and establishes a connection with the environment. Notice: Currently communication between Unity and Python takes place over an open socket without authentication. Ensure that the network where training takes place is secure. :string file_name: Name of Unity environment binary. :int base_port: Baseline port number to connect to Unity environment over. worker_id increments over this. If no environment is specified (i.e. file_name is None), the DEFAULT_EDITOR_PORT will be used. :int worker_id: Offset from base_port. Used for training multiple environments simultaneously. :bool no_graphics: Whether to run the Unity simulator in no-graphics mode :int timeout_wait: Time (in seconds) to wait for connection from environment. :list args: Addition Unity command line arguments :list side_channels: Additional side channel for no-rl communication with Unity :str log_folder: Optional folder to write the Unity Player log file into. Requires absolute path.","title":"__init__"},{"location":"Python-LLAPI-Documentation/#close_1","text":"| close() Sends a shutdown signal to the unity environment, and closes the socket connection.","title":"close"},{"location":"Python-LLAPI-Documentation/#mlagents_envsregistry","text":"","title":"mlagents_envs.registry"},{"location":"Python-LLAPI-Documentation/#mlagents_envsregistryunity_env_registry","text":"","title":"mlagents_envs.registry.unity_env_registry"},{"location":"Python-LLAPI-Documentation/#unityenvregistry-objects","text":"class UnityEnvRegistry(Mapping)","title":"UnityEnvRegistry Objects"},{"location":"Python-LLAPI-Documentation/#unityenvregistry","text":"Provides a library of Unity environments that can be launched without the need of downloading the Unity Editor. The UnityEnvRegistry implements a Map, to access an entry of the Registry, use: registry = UnityEnvRegistry() entry = registry[<environment_identifyier>] An entry has the following properties : * identifier : Uniquely identifies this environment * expected_reward : Corresponds to the reward an agent must obtained for the task to be considered completed. * description : A human readable description of the environment. To launch a Unity environment from a registry entry, use the make method: registry = UnityEnvRegistry() env = registry[<environment_identifyier>].make()","title":"UnityEnvRegistry"},{"location":"Python-LLAPI-Documentation/#register","text":"| register(new_entry: BaseRegistryEntry) -> None Registers a new BaseRegistryEntry to the registry. The BaseRegistryEntry.identifier value will be used as indexing key. If two are more environments are registered under the same key, the most recentry added will replace the others.","title":"register"},{"location":"Python-LLAPI-Documentation/#register_from_yaml","text":"| register_from_yaml(path_to_yaml: str) -> None Registers the environments listed in a yaml file (either local or remote). Note that the entries are registered lazily: the registration will only happen when an environment is accessed. The yaml file must have the following format : environments: - <identifier of the first environment>: expected_reward: <expected reward of the environment> description: | <a multi line description of the environment> <continued multi line description> linux_url: <The url for the Linux executable zip file> darwin_url: <The url for the OSX executable zip file> win_url: <The url for the Windows executable zip file> - <identifier of the second environment>: expected_reward: <expected reward of the environment> description: | <a multi line description of the environment> <continued multi line description> linux_url: <The url for the Linux executable zip file> darwin_url: <The url for the OSX executable zip file> win_url: <The url for the Windows executable zip file> - ... Arguments : path_to_yaml : A local path or url to the yaml file","title":"register_from_yaml"},{"location":"Python-LLAPI-Documentation/#clear","text":"| clear() -> None Deletes all entries in the registry.","title":"clear"},{"location":"Python-LLAPI-Documentation/#__getitem___2","text":"| __getitem__(identifier: str) -> BaseRegistryEntry Returns the BaseRegistryEntry with the provided identifier. BaseRegistryEntry can then be used to make a Unity Environment. Arguments : identifier : The identifier of the BaseRegistryEntry Returns : The associated BaseRegistryEntry","title":"__getitem__"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channel","text":"","title":"mlagents_envs.side_channel"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelraw_bytes_channel","text":"","title":"mlagents_envs.side_channel.raw_bytes_channel"},{"location":"Python-LLAPI-Documentation/#rawbyteschannel-objects","text":"class RawBytesChannel(SideChannel) This is an example of what the SideChannel for raw bytes exchange would look like. Is meant to be used for general research purpose.","title":"RawBytesChannel Objects"},{"location":"Python-LLAPI-Documentation/#on_message_received","text":"| on_message_received(msg: IncomingMessage) -> None Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel.","title":"on_message_received"},{"location":"Python-LLAPI-Documentation/#get_and_clear_received_messages","text":"| get_and_clear_received_messages() -> List[bytes] returns a list of bytearray received from the environment.","title":"get_and_clear_received_messages"},{"location":"Python-LLAPI-Documentation/#send_raw_data","text":"| send_raw_data(data: bytearray) -> None Queues a message to be sent by the environment at the next call to step.","title":"send_raw_data"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channeloutgoing_message","text":"","title":"mlagents_envs.side_channel.outgoing_message"},{"location":"Python-LLAPI-Documentation/#outgoingmessage-objects","text":"class OutgoingMessage() Utility class for forming the message that is written to a SideChannel. All data is written in little-endian format using the struct module.","title":"OutgoingMessage Objects"},{"location":"Python-LLAPI-Documentation/#__init___1","text":"| __init__() Create an OutgoingMessage with an empty buffer.","title":"__init__"},{"location":"Python-LLAPI-Documentation/#write_bool","text":"| write_bool(b: bool) -> None Append a boolean value.","title":"write_bool"},{"location":"Python-LLAPI-Documentation/#write_int32","text":"| write_int32(i: int) -> None Append an integer value.","title":"write_int32"},{"location":"Python-LLAPI-Documentation/#write_float32","text":"| write_float32(f: float) -> None Append a float value. It will be truncated to 32-bit precision.","title":"write_float32"},{"location":"Python-LLAPI-Documentation/#write_float32_list","text":"| write_float32_list(float_list: List[float]) -> None Append a list of float values. They will be truncated to 32-bit precision.","title":"write_float32_list"},{"location":"Python-LLAPI-Documentation/#write_string","text":"| write_string(s: str) -> None Append a string value. Internally, it will be encoded to ascii, and the encoded length will also be written to the message.","title":"write_string"},{"location":"Python-LLAPI-Documentation/#set_raw_bytes","text":"| set_raw_bytes(buffer: bytearray) -> None Set the internal buffer to a new bytearray. This will overwrite any existing data. Arguments : buffer : Returns :","title":"set_raw_bytes"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelengine_configuration_channel","text":"","title":"mlagents_envs.side_channel.engine_configuration_channel"},{"location":"Python-LLAPI-Documentation/#engineconfigurationchannel-objects","text":"class EngineConfigurationChannel(SideChannel) This is the SideChannel for engine configuration exchange. The data in the engine configuration is as follows : - int width; - int height; - int qualityLevel; - float timeScale; - int targetFrameRate; - int captureFrameRate;","title":"EngineConfigurationChannel Objects"},{"location":"Python-LLAPI-Documentation/#on_message_received_1","text":"| on_message_received(msg: IncomingMessage) -> None Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel. Note that Python should never receive an engine configuration from Unity","title":"on_message_received"},{"location":"Python-LLAPI-Documentation/#set_configuration_parameters","text":"| set_configuration_parameters(width: Optional[int] = None, height: Optional[int] = None, quality_level: Optional[int] = None, time_scale: Optional[float] = None, target_frame_rate: Optional[int] = None, capture_frame_rate: Optional[int] = None) -> None Sets the engine configuration. Takes as input the configurations of the engine. Arguments : width : Defines the width of the display. (Must be set alongside height) height : Defines the height of the display. (Must be set alongside width) quality_level : Defines the quality level of the simulation. time_scale : Defines the multiplier for the deltatime in the simulation. If set to a higher value, time will pass faster in the simulation but the physics might break. target_frame_rate : Instructs simulation to try to render at a specified frame rate. capture_frame_rate : Instructs the simulation to consider time between updates to always be constant, regardless of the actual frame rate.","title":"set_configuration_parameters"},{"location":"Python-LLAPI-Documentation/#set_configuration","text":"| set_configuration(config: EngineConfig) -> None Sets the engine configuration. Takes as input an EngineConfig.","title":"set_configuration"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelside_channel_manager","text":"","title":"mlagents_envs.side_channel.side_channel_manager"},{"location":"Python-LLAPI-Documentation/#sidechannelmanager-objects","text":"class SideChannelManager()","title":"SideChannelManager Objects"},{"location":"Python-LLAPI-Documentation/#process_side_channel_message","text":"| process_side_channel_message(data: bytes) -> None Separates the data received from Python into individual messages for each registered side channel and calls on_message_received on them. Arguments : data : The packed message sent by Unity","title":"process_side_channel_message"},{"location":"Python-LLAPI-Documentation/#generate_side_channel_messages","text":"| generate_side_channel_messages() -> bytearray Gathers the messages that the registered side channels will send to Unity and combines them into a single message ready to be sent.","title":"generate_side_channel_messages"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelstats_side_channel","text":"","title":"mlagents_envs.side_channel.stats_side_channel"},{"location":"Python-LLAPI-Documentation/#statssidechannel-objects","text":"class StatsSideChannel(SideChannel) Side channel that receives (string, float) pairs from the environment, so that they can eventually be passed to a StatsReporter.","title":"StatsSideChannel Objects"},{"location":"Python-LLAPI-Documentation/#on_message_received_2","text":"| on_message_received(msg: IncomingMessage) -> None Receive the message from the environment, and save it for later retrieval. Arguments : msg : Returns :","title":"on_message_received"},{"location":"Python-LLAPI-Documentation/#get_and_reset_stats","text":"| get_and_reset_stats() -> EnvironmentStats Returns the current stats, and resets the internal storage of the stats. Returns :","title":"get_and_reset_stats"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelincoming_message","text":"","title":"mlagents_envs.side_channel.incoming_message"},{"location":"Python-LLAPI-Documentation/#incomingmessage-objects","text":"class IncomingMessage() Utility class for reading the message written to a SideChannel. Values must be read in the order they were written.","title":"IncomingMessage Objects"},{"location":"Python-LLAPI-Documentation/#__init___2","text":"| __init__(buffer: bytes, offset: int = 0) Create a new IncomingMessage from the bytes.","title":"__init__"},{"location":"Python-LLAPI-Documentation/#read_bool","text":"| read_bool(default_value: bool = False) -> bool Read a boolean value from the message buffer. Arguments : default_value : Default value to use if the end of the message is reached. Returns : The value read from the message, or the default value if the end was reached.","title":"read_bool"},{"location":"Python-LLAPI-Documentation/#read_int32","text":"| read_int32(default_value: int = 0) -> int Read an integer value from the message buffer. Arguments : default_value : Default value to use if the end of the message is reached. Returns : The value read from the message, or the default value if the end was reached.","title":"read_int32"},{"location":"Python-LLAPI-Documentation/#read_float32","text":"| read_float32(default_value: float = 0.0) -> float Read a float value from the message buffer. Arguments : default_value : Default value to use if the end of the message is reached. Returns : The value read from the message, or the default value if the end was reached.","title":"read_float32"},{"location":"Python-LLAPI-Documentation/#read_float32_list","text":"| read_float32_list(default_value: List[float] = None) -> List[float] Read a list of float values from the message buffer. Arguments : default_value : Default value to use if the end of the message is reached. Returns : The value read from the message, or the default value if the end was reached.","title":"read_float32_list"},{"location":"Python-LLAPI-Documentation/#read_string","text":"| read_string(default_value: str = \"\") -> str Read a string value from the message buffer. Arguments : default_value : Default value to use if the end of the message is reached. Returns : The value read from the message, or the default value if the end was reached.","title":"read_string"},{"location":"Python-LLAPI-Documentation/#get_raw_bytes","text":"| get_raw_bytes() -> bytes Get a copy of the internal bytes used by the message.","title":"get_raw_bytes"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelfloat_properties_channel","text":"","title":"mlagents_envs.side_channel.float_properties_channel"},{"location":"Python-LLAPI-Documentation/#floatpropertieschannel-objects","text":"class FloatPropertiesChannel(SideChannel) This is the SideChannel for float properties shared with Unity. You can modify the float properties of an environment with the commands set_property, get_property and list_properties.","title":"FloatPropertiesChannel Objects"},{"location":"Python-LLAPI-Documentation/#on_message_received_3","text":"| on_message_received(msg: IncomingMessage) -> None Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel.","title":"on_message_received"},{"location":"Python-LLAPI-Documentation/#set_property","text":"| set_property(key: str, value: float) -> None Sets a property in the Unity Environment. Arguments : key : The string identifier of the property. value : The float value of the property.","title":"set_property"},{"location":"Python-LLAPI-Documentation/#get_property","text":"| get_property(key: str) -> Optional[float] Gets a property in the Unity Environment. If the property was not found, will return None. Arguments : key : The string identifier of the property. Returns : The float value of the property or None.","title":"get_property"},{"location":"Python-LLAPI-Documentation/#list_properties","text":"| list_properties() -> List[str] Returns a list of all the string identifiers of the properties currently present in the Unity Environment.","title":"list_properties"},{"location":"Python-LLAPI-Documentation/#get_property_dict_copy","text":"| get_property_dict_copy() -> Dict[str, float] Returns a copy of the float properties. Returns :","title":"get_property_dict_copy"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelenvironment_parameters_channel","text":"","title":"mlagents_envs.side_channel.environment_parameters_channel"},{"location":"Python-LLAPI-Documentation/#environmentparameterschannel-objects","text":"class EnvironmentParametersChannel(SideChannel) This is the SideChannel for sending environment parameters to Unity. You can send parameters to an environment with the command set_float_parameter.","title":"EnvironmentParametersChannel Objects"},{"location":"Python-LLAPI-Documentation/#set_float_parameter","text":"| set_float_parameter(key: str, value: float) -> None Sets a float environment parameter in the Unity Environment. Arguments : key : The string identifier of the parameter. value : The float value of the parameter.","title":"set_float_parameter"},{"location":"Python-LLAPI-Documentation/#set_uniform_sampler_parameters","text":"| set_uniform_sampler_parameters(key: str, min_value: float, max_value: float, seed: int) -> None Sets a uniform environment parameter sampler. Arguments : key : The string identifier of the parameter. min_value : The minimum of the sampling distribution. max_value : The maximum of the sampling distribution. seed : The random seed to initialize the sampler.","title":"set_uniform_sampler_parameters"},{"location":"Python-LLAPI-Documentation/#set_gaussian_sampler_parameters","text":"| set_gaussian_sampler_parameters(key: str, mean: float, st_dev: float, seed: int) -> None Sets a gaussian environment parameter sampler. Arguments : key : The string identifier of the parameter. mean : The mean of the sampling distribution. st_dev : The standard deviation of the sampling distribution. seed : The random seed to initialize the sampler.","title":"set_gaussian_sampler_parameters"},{"location":"Python-LLAPI-Documentation/#set_multirangeuniform_sampler_parameters","text":"| set_multirangeuniform_sampler_parameters(key: str, intervals: List[Tuple[float, float]], seed: int) -> None Sets a multirangeuniform environment parameter sampler. Arguments : key : The string identifier of the parameter. intervals : The lists of min and max that define each uniform distribution. seed : The random seed to initialize the sampler.","title":"set_multirangeuniform_sampler_parameters"},{"location":"Python-LLAPI-Documentation/#mlagents_envsside_channelside_channel","text":"","title":"mlagents_envs.side_channel.side_channel"},{"location":"Python-LLAPI-Documentation/#sidechannel-objects","text":"class SideChannel(ABC) The side channel just get access to a bytes buffer that will be shared between C# and Python. For example, We will create a specific side channel for properties that will be a list of string (fixed size) to float number, that can be modified by both C# and Python. All side channels are passed to the Env object at construction.","title":"SideChannel Objects"},{"location":"Python-LLAPI-Documentation/#queue_message_to_send","text":"| queue_message_to_send(msg: OutgoingMessage) -> None Queues a message to be sent by the environment at the next call to step.","title":"queue_message_to_send"},{"location":"Python-LLAPI-Documentation/#on_message_received_4","text":"| @abstractmethod | on_message_received(msg: IncomingMessage) -> None Is called by the environment to the side channel. Can be called multiple times per step if multiple messages are meant for that SideChannel.","title":"on_message_received"},{"location":"Python-LLAPI-Documentation/#channel_id","text":"| @property | channel_id() -> uuid.UUID Returns : The type of side channel used. Will influence how the data is processed in the environment.","title":"channel_id"},{"location":"Python-LLAPI/","text":"Unity ML-Agents Python Low Level API The mlagents Python package contains two components: a low level API which allows you to interact directly with a Unity Environment ( mlagents_envs ) and an entry point to train ( mlagents-learn ) which allows you to train agents in Unity Environments using our implementations of reinforcement learning or imitation learning. This document describes how to use the mlagents_envs API. For information on using mlagents-learn , see here . For Python Low Level API documentation, see here . The Python Low Level API can be used to interact directly with your Unity learning environment. As such, it can serve as the basis for developing and evaluating new learning algorithms. mlagents_envs The ML-Agents Toolkit Low Level API is a Python API for controlling the simulation loop of an environment or game built with Unity. This API is used by the training algorithms inside the ML-Agent Toolkit, but you can also write your own Python programs using this API. The key objects in the Python API include: UnityEnvironment \u2014 the main interface between the Unity application and your code. Use UnityEnvironment to start and control a simulation or training session. BehaviorName - is a string that identifies a behavior in the simulation. AgentId - is an int that serves as unique identifier for Agents in the simulation. DecisionSteps \u2014 contains the data from Agents belonging to the same \"Behavior\" in the simulation, such as observations and rewards. Only Agents that requested a decision since the last call to env.step() are in the DecisionSteps object. TerminalSteps \u2014 contains the data from Agents belonging to the same \"Behavior\" in the simulation, such as observations and rewards. Only Agents whose episode ended since the last call to env.step() are in the TerminalSteps object. BehaviorSpec \u2014 describes the shape of the observation data inside DecisionSteps and TerminalSteps as well as the expected action shapes. These classes are all defined in the base_env script. An Agent \"Behavior\" is a group of Agents identified by a BehaviorName that share the same observations and action types (described in their BehaviorSpec ). You can think about Agent Behavior as a group of agents that will share the same policy. All Agents with the same behavior have the same goal and reward signals. To communicate with an Agent in a Unity environment from a Python program, the Agent in the simulation must have Behavior Parameters set to communicate. You must set the Behavior Type to Default and give it a Behavior Name . Notice: Currently communication between Unity and Python takes place over an open socket without authentication. As such, please make sure that the network where training takes place is secure. This will be addressed in a future release. Loading a Unity Environment Python-side communication happens through UnityEnvironment which is located in environment.py . To load a Unity environment from a built binary file, put the file in the same directory as envs . For example, if the filename of your Unity environment is 3DBall , in python, run: from mlagents_envs.environment import UnityEnvironment # This is a non-blocking call that only loads the environment. env = UnityEnvironment(file_name=\"3DBall\", seed=1, side_channels=[]) # Start interacting with the environment. env.reset() behavior_names = env.behavior_specs.keys() ... NOTE: Please read Interacting with a Unity Environment to read more about how you can interact with the Unity environment from Python. file_name is the name of the environment binary (located in the root directory of the python project). worker_id indicates which port to use for communication with the environment. For use in parallel training regimes such as A3C. seed indicates the seed to use when generating random numbers during the training process. In environments which are stochastic, setting the seed enables reproducible experimentation by ensuring that the environment and trainers utilize the same random seed. side_channels provides a way to exchange data with the Unity simulation that is not related to the reinforcement learning loop. For example: configurations or properties. More on them in the Side Channels doc. If you want to directly interact with the Editor, you need to use file_name=None , then press the Play button in the Editor when the message \"Start training by pressing the Play button in the Unity Editor\" is displayed on the screen Interacting with a Unity Environment The BaseEnv interface A BaseEnv has the following methods: Reset : env.reset() Sends a signal to reset the environment. Returns None. Step : env.step() Sends a signal to step the environment. Returns None. Note that a \"step\" for Python does not correspond to either Unity Update nor FixedUpdate . When step() or reset() is called, the Unity simulation will move forward until an Agent in the simulation needs a input from Python to act. Close : env.close() Sends a shutdown signal to the environment and terminates the communication. Behavior Specs : env.behavior_specs Returns a Mapping of BehaviorName to BehaviorSpec objects (read only). A BehaviorSpec contains the observation shapes and the ActionSpec (which defines the action shape). Note that the BehaviorSpec for a specific group is fixed throughout the simulation. The number of entries in the Mapping can change over time in the simulation if new Agent behaviors are created in the simulation. Get Steps : env.get_steps(behavior_name: str) Returns a tuple DecisionSteps, TerminalSteps corresponding to the behavior_name given as input. The DecisionSteps contains information about the state of the agents that need an action this step and have the behavior behavior_name. The TerminalSteps contains information about the state of the agents whose episode ended and have the behavior behavior_name. Both DecisionSteps and TerminalSteps contain information such as the observations, the rewards and the agent identifiers. DecisionSteps also contains action masks for the next action while TerminalSteps contains the reason for termination (did the Agent reach its maximum step and was interrupted). The data is in np.array of which the first dimension is always the number of agents note that the number of agents is not guaranteed to remain constant during the simulation and it is not unusual to have either DecisionSteps or TerminalSteps contain no Agents at all. Set Actions : env.set_actions(behavior_name: str, action: ActionTuple) Sets the actions for a whole agent group. action is an ActionTuple , which is made up of a 2D np.array of dtype=np.int32 for discrete actions, and dtype=np.float32 for continuous actions. The first dimension of np.array in the tuple is the number of agents that requested a decision since the last call to env.step() . The second dimension is the number of discrete or continuous actions for the corresponding array. Set Action for Agent : env.set_action_for_agent(agent_group: str, agent_id: int, action: ActionTuple) Sets the action for a specific Agent in an agent group. agent_group is the name of the group the Agent belongs to and agent_id is the integer identifier of the Agent. action is an ActionTuple as described above. Note: If no action is provided for an agent group between two calls to env.step() then the default action will be all zeros. DecisionSteps and DecisionStep DecisionSteps (with s ) contains information about a whole batch of Agents while DecisionStep (no s ) only contains information about a single Agent. A DecisionSteps has the following fields : obs is a list of numpy arrays observations collected by the group of agent. The first dimension of the array corresponds to the batch size of the group (number of agents requesting a decision since the last call to env.step() ). reward is a float vector of length batch size. Corresponds to the rewards collected by each agent since the last simulation step. agent_id is an int vector of length batch size containing unique identifier for the corresponding Agent. This is used to track Agents across simulation steps. action_mask is an optional list of two dimensional arrays of booleans which is only available when using multi-discrete actions. Each array corresponds to an action branch. The first dimension of each array is the batch size and the second contains a mask for each action of the branch. If true, the action is not available for the agent during this simulation step. It also has the two following methods: len(DecisionSteps) Returns the number of agents requesting a decision since the last call to env.step() . DecisionSteps[agent_id] Returns a DecisionStep for the Agent with the agent_id unique identifier. A DecisionStep has the following fields: obs is a list of numpy arrays observations collected by the agent. (Each array has one less dimension than the arrays in DecisionSteps ) reward is a float. Corresponds to the rewards collected by the agent since the last simulation step. agent_id is an int and an unique identifier for the corresponding Agent. action_mask is an optional list of one dimensional arrays of booleans which is only available when using multi-discrete actions. Each array corresponds to an action branch. Each array contains a mask for each action of the branch. If true, the action is not available for the agent during this simulation step. TerminalSteps and TerminalStep Similarly to DecisionSteps and DecisionStep , TerminalSteps (with s ) contains information about a whole batch of Agents while TerminalStep (no s ) only contains information about a single Agent. A TerminalSteps has the following fields : obs is a list of numpy arrays observations collected by the group of agent. The first dimension of the array corresponds to the batch size of the group (number of agents requesting a decision since the last call to env.step() ). reward is a float vector of length batch size. Corresponds to the rewards collected by each agent since the last simulation step. agent_id is an int vector of length batch size containing unique identifier for the corresponding Agent. This is used to track Agents across simulation steps. interrupted is an array of booleans of length batch size. Is true if the associated Agent was interrupted since the last decision step. For example, if the Agent reached the maximum number of steps for the episode. It also has the two following methods: len(TerminalSteps) Returns the number of agents requesting a decision since the last call to env.step() . TerminalSteps[agent_id] Returns a TerminalStep for the Agent with the agent_id unique identifier. A TerminalStep has the following fields: obs is a list of numpy arrays observations collected by the agent. (Each array has one less dimension than the arrays in TerminalSteps ) reward is a float. Corresponds to the rewards collected by the agent since the last simulation step. agent_id is an int and an unique identifier for the corresponding Agent. interrupted is a bool. Is true if the Agent was interrupted since the last decision step. For example, if the Agent reached the maximum number of steps for the episode. BehaviorSpec A BehaviorSpec has the following fields : observation_specs is a List of ObservationSpec objects : Each ObservationSpec corresponds to an observation's properties: shape is a tuple of ints that corresponds to the shape of the observation (without the number of agents dimension). dimension_property is a tuple of flags containing extra information about how the data should be processed in the corresponding dimension. observation_type is an enum corresponding to what type of observation is generating the data (i.e., default, goal, etc). Note that the ObservationSpec have the same ordering as the ordering of observations in the DecisionSteps, DecisionStep, TerminalSteps and TerminalStep. action_spec is an ActionSpec namedtuple that defines the number and types of actions for the Agent. An ActionSpec has the following fields and properties: - continuous_size is the number of floats that constitute the continuous actions. - discrete_size is the number of branches (the number of independent actions) that constitute the multi-discrete actions. - discrete_branches is a Tuple of ints. Each int corresponds to the number of different options for each branch of the action. For example: In a game direction input (no movement, left, right) and jump input (no jump, jump) there will be two branches (direction and jump), the first one with 3 options and the second with 2 options. ( discrete_size = 2 and discrete_action_branches = (3,2,) ) Communicating additional information with the Environment In addition to the means of communicating between Unity and python described above, we also provide methods for sharing agent-agnostic information. These additional methods are referred to as side channels. ML-Agents includes two ready-made side channels, described below. It is also possible to create custom side channels to communicate any additional data between a Unity environment and Python. Instructions for creating custom side channels can be found here . Side channels exist as separate classes which are instantiated, and then passed as list to the side_channels argument of the constructor of the UnityEnvironment class. channel = MyChannel() env = UnityEnvironment(side_channels = [channel]) Note : A side channel will only send/receive messages when env.step or env.reset() is called. EngineConfigurationChannel The EngineConfiguration side channel allows you to modify the time-scale, resolution, and graphics quality of the environment. This can be useful for adjusting the environment to perform better during training, or be more interpretable during inference. EngineConfigurationChannel has two methods : set_configuration_parameters which takes the following arguments: width : Defines the width of the display. (Must be set alongside height) height : Defines the height of the display. (Must be set alongside width) quality_level : Defines the quality level of the simulation. time_scale : Defines the multiplier for the deltatime in the simulation. If set to a higher value, time will pass faster in the simulation but the physics may perform unpredictably. target_frame_rate : Instructs simulation to try to render at a specified frame rate. capture_frame_rate Instructs the simulation to consider time between updates to always be constant, regardless of the actual frame rate. set_configuration with argument config which is an EngineConfig NamedTuple object. For example, the following code would adjust the time-scale of the simulation to be 2x realtime. from mlagents_envs.environment import UnityEnvironment from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel channel = EngineConfigurationChannel() env = UnityEnvironment(side_channels=[channel]) channel.set_configuration_parameters(time_scale = 2.0) i = env.reset() ... EnvironmentParameters The EnvironmentParameters will allow you to get and set pre-defined numerical values in the environment. This can be useful for adjusting environment-specific settings, or for reading non-agent related information from the environment. You can call get_property and set_property on the side channel to read and write properties. EnvironmentParametersChannel has one methods: set_float_parameter Sets a float parameter in the Unity Environment. key: The string identifier of the property. value: The float value of the property. from mlagents_envs.environment import UnityEnvironment from mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel channel = EnvironmentParametersChannel() env = UnityEnvironment(side_channels=[channel]) channel.set_float_parameter(\"parameter_1\", 2.0) i = env.reset() ... Once a property has been modified in Python, you can access it in C# after the next call to step as follows: var envParameters = Academy.Instance.EnvironmentParameters; float property1 = envParameters.GetWithDefault(\"parameter_1\", 0.0f); Custom side channels For information on how to make custom side channels for sending additional data types, see the documentation here .","title":"Getting started with the LLAPI"},{"location":"Python-LLAPI/#unity-ml-agents-python-low-level-api","text":"The mlagents Python package contains two components: a low level API which allows you to interact directly with a Unity Environment ( mlagents_envs ) and an entry point to train ( mlagents-learn ) which allows you to train agents in Unity Environments using our implementations of reinforcement learning or imitation learning. This document describes how to use the mlagents_envs API. For information on using mlagents-learn , see here . For Python Low Level API documentation, see here . The Python Low Level API can be used to interact directly with your Unity learning environment. As such, it can serve as the basis for developing and evaluating new learning algorithms.","title":"Unity ML-Agents Python Low Level API"},{"location":"Python-LLAPI/#mlagents_envs","text":"The ML-Agents Toolkit Low Level API is a Python API for controlling the simulation loop of an environment or game built with Unity. This API is used by the training algorithms inside the ML-Agent Toolkit, but you can also write your own Python programs using this API. The key objects in the Python API include: UnityEnvironment \u2014 the main interface between the Unity application and your code. Use UnityEnvironment to start and control a simulation or training session. BehaviorName - is a string that identifies a behavior in the simulation. AgentId - is an int that serves as unique identifier for Agents in the simulation. DecisionSteps \u2014 contains the data from Agents belonging to the same \"Behavior\" in the simulation, such as observations and rewards. Only Agents that requested a decision since the last call to env.step() are in the DecisionSteps object. TerminalSteps \u2014 contains the data from Agents belonging to the same \"Behavior\" in the simulation, such as observations and rewards. Only Agents whose episode ended since the last call to env.step() are in the TerminalSteps object. BehaviorSpec \u2014 describes the shape of the observation data inside DecisionSteps and TerminalSteps as well as the expected action shapes. These classes are all defined in the base_env script. An Agent \"Behavior\" is a group of Agents identified by a BehaviorName that share the same observations and action types (described in their BehaviorSpec ). You can think about Agent Behavior as a group of agents that will share the same policy. All Agents with the same behavior have the same goal and reward signals. To communicate with an Agent in a Unity environment from a Python program, the Agent in the simulation must have Behavior Parameters set to communicate. You must set the Behavior Type to Default and give it a Behavior Name . Notice: Currently communication between Unity and Python takes place over an open socket without authentication. As such, please make sure that the network where training takes place is secure. This will be addressed in a future release.","title":"mlagents_envs"},{"location":"Python-LLAPI/#loading-a-unity-environment","text":"Python-side communication happens through UnityEnvironment which is located in environment.py . To load a Unity environment from a built binary file, put the file in the same directory as envs . For example, if the filename of your Unity environment is 3DBall , in python, run: from mlagents_envs.environment import UnityEnvironment # This is a non-blocking call that only loads the environment. env = UnityEnvironment(file_name=\"3DBall\", seed=1, side_channels=[]) # Start interacting with the environment. env.reset() behavior_names = env.behavior_specs.keys() ... NOTE: Please read Interacting with a Unity Environment to read more about how you can interact with the Unity environment from Python. file_name is the name of the environment binary (located in the root directory of the python project). worker_id indicates which port to use for communication with the environment. For use in parallel training regimes such as A3C. seed indicates the seed to use when generating random numbers during the training process. In environments which are stochastic, setting the seed enables reproducible experimentation by ensuring that the environment and trainers utilize the same random seed. side_channels provides a way to exchange data with the Unity simulation that is not related to the reinforcement learning loop. For example: configurations or properties. More on them in the Side Channels doc. If you want to directly interact with the Editor, you need to use file_name=None , then press the Play button in the Editor when the message \"Start training by pressing the Play button in the Unity Editor\" is displayed on the screen","title":"Loading a Unity Environment"},{"location":"Python-LLAPI/#interacting-with-a-unity-environment","text":"","title":"Interacting with a Unity Environment"},{"location":"Python-LLAPI/#the-baseenv-interface","text":"A BaseEnv has the following methods: Reset : env.reset() Sends a signal to reset the environment. Returns None. Step : env.step() Sends a signal to step the environment. Returns None. Note that a \"step\" for Python does not correspond to either Unity Update nor FixedUpdate . When step() or reset() is called, the Unity simulation will move forward until an Agent in the simulation needs a input from Python to act. Close : env.close() Sends a shutdown signal to the environment and terminates the communication. Behavior Specs : env.behavior_specs Returns a Mapping of BehaviorName to BehaviorSpec objects (read only). A BehaviorSpec contains the observation shapes and the ActionSpec (which defines the action shape). Note that the BehaviorSpec for a specific group is fixed throughout the simulation. The number of entries in the Mapping can change over time in the simulation if new Agent behaviors are created in the simulation. Get Steps : env.get_steps(behavior_name: str) Returns a tuple DecisionSteps, TerminalSteps corresponding to the behavior_name given as input. The DecisionSteps contains information about the state of the agents that need an action this step and have the behavior behavior_name. The TerminalSteps contains information about the state of the agents whose episode ended and have the behavior behavior_name. Both DecisionSteps and TerminalSteps contain information such as the observations, the rewards and the agent identifiers. DecisionSteps also contains action masks for the next action while TerminalSteps contains the reason for termination (did the Agent reach its maximum step and was interrupted). The data is in np.array of which the first dimension is always the number of agents note that the number of agents is not guaranteed to remain constant during the simulation and it is not unusual to have either DecisionSteps or TerminalSteps contain no Agents at all. Set Actions : env.set_actions(behavior_name: str, action: ActionTuple) Sets the actions for a whole agent group. action is an ActionTuple , which is made up of a 2D np.array of dtype=np.int32 for discrete actions, and dtype=np.float32 for continuous actions. The first dimension of np.array in the tuple is the number of agents that requested a decision since the last call to env.step() . The second dimension is the number of discrete or continuous actions for the corresponding array. Set Action for Agent : env.set_action_for_agent(agent_group: str, agent_id: int, action: ActionTuple) Sets the action for a specific Agent in an agent group. agent_group is the name of the group the Agent belongs to and agent_id is the integer identifier of the Agent. action is an ActionTuple as described above. Note: If no action is provided for an agent group between two calls to env.step() then the default action will be all zeros.","title":"The BaseEnv interface"},{"location":"Python-LLAPI/#decisionsteps-and-decisionstep","text":"DecisionSteps (with s ) contains information about a whole batch of Agents while DecisionStep (no s ) only contains information about a single Agent. A DecisionSteps has the following fields : obs is a list of numpy arrays observations collected by the group of agent. The first dimension of the array corresponds to the batch size of the group (number of agents requesting a decision since the last call to env.step() ). reward is a float vector of length batch size. Corresponds to the rewards collected by each agent since the last simulation step. agent_id is an int vector of length batch size containing unique identifier for the corresponding Agent. This is used to track Agents across simulation steps. action_mask is an optional list of two dimensional arrays of booleans which is only available when using multi-discrete actions. Each array corresponds to an action branch. The first dimension of each array is the batch size and the second contains a mask for each action of the branch. If true, the action is not available for the agent during this simulation step. It also has the two following methods: len(DecisionSteps) Returns the number of agents requesting a decision since the last call to env.step() . DecisionSteps[agent_id] Returns a DecisionStep for the Agent with the agent_id unique identifier. A DecisionStep has the following fields: obs is a list of numpy arrays observations collected by the agent. (Each array has one less dimension than the arrays in DecisionSteps ) reward is a float. Corresponds to the rewards collected by the agent since the last simulation step. agent_id is an int and an unique identifier for the corresponding Agent. action_mask is an optional list of one dimensional arrays of booleans which is only available when using multi-discrete actions. Each array corresponds to an action branch. Each array contains a mask for each action of the branch. If true, the action is not available for the agent during this simulation step.","title":"DecisionSteps and DecisionStep"},{"location":"Python-LLAPI/#terminalsteps-and-terminalstep","text":"Similarly to DecisionSteps and DecisionStep , TerminalSteps (with s ) contains information about a whole batch of Agents while TerminalStep (no s ) only contains information about a single Agent. A TerminalSteps has the following fields : obs is a list of numpy arrays observations collected by the group of agent. The first dimension of the array corresponds to the batch size of the group (number of agents requesting a decision since the last call to env.step() ). reward is a float vector of length batch size. Corresponds to the rewards collected by each agent since the last simulation step. agent_id is an int vector of length batch size containing unique identifier for the corresponding Agent. This is used to track Agents across simulation steps. interrupted is an array of booleans of length batch size. Is true if the associated Agent was interrupted since the last decision step. For example, if the Agent reached the maximum number of steps for the episode. It also has the two following methods: len(TerminalSteps) Returns the number of agents requesting a decision since the last call to env.step() . TerminalSteps[agent_id] Returns a TerminalStep for the Agent with the agent_id unique identifier. A TerminalStep has the following fields: obs is a list of numpy arrays observations collected by the agent. (Each array has one less dimension than the arrays in TerminalSteps ) reward is a float. Corresponds to the rewards collected by the agent since the last simulation step. agent_id is an int and an unique identifier for the corresponding Agent. interrupted is a bool. Is true if the Agent was interrupted since the last decision step. For example, if the Agent reached the maximum number of steps for the episode.","title":"TerminalSteps and TerminalStep"},{"location":"Python-LLAPI/#behaviorspec","text":"A BehaviorSpec has the following fields : observation_specs is a List of ObservationSpec objects : Each ObservationSpec corresponds to an observation's properties: shape is a tuple of ints that corresponds to the shape of the observation (without the number of agents dimension). dimension_property is a tuple of flags containing extra information about how the data should be processed in the corresponding dimension. observation_type is an enum corresponding to what type of observation is generating the data (i.e., default, goal, etc). Note that the ObservationSpec have the same ordering as the ordering of observations in the DecisionSteps, DecisionStep, TerminalSteps and TerminalStep. action_spec is an ActionSpec namedtuple that defines the number and types of actions for the Agent. An ActionSpec has the following fields and properties: - continuous_size is the number of floats that constitute the continuous actions. - discrete_size is the number of branches (the number of independent actions) that constitute the multi-discrete actions. - discrete_branches is a Tuple of ints. Each int corresponds to the number of different options for each branch of the action. For example: In a game direction input (no movement, left, right) and jump input (no jump, jump) there will be two branches (direction and jump), the first one with 3 options and the second with 2 options. ( discrete_size = 2 and discrete_action_branches = (3,2,) )","title":"BehaviorSpec"},{"location":"Python-LLAPI/#communicating-additional-information-with-the-environment","text":"In addition to the means of communicating between Unity and python described above, we also provide methods for sharing agent-agnostic information. These additional methods are referred to as side channels. ML-Agents includes two ready-made side channels, described below. It is also possible to create custom side channels to communicate any additional data between a Unity environment and Python. Instructions for creating custom side channels can be found here . Side channels exist as separate classes which are instantiated, and then passed as list to the side_channels argument of the constructor of the UnityEnvironment class. channel = MyChannel() env = UnityEnvironment(side_channels = [channel]) Note : A side channel will only send/receive messages when env.step or env.reset() is called.","title":"Communicating additional information with the Environment"},{"location":"Python-LLAPI/#engineconfigurationchannel","text":"The EngineConfiguration side channel allows you to modify the time-scale, resolution, and graphics quality of the environment. This can be useful for adjusting the environment to perform better during training, or be more interpretable during inference. EngineConfigurationChannel has two methods : set_configuration_parameters which takes the following arguments: width : Defines the width of the display. (Must be set alongside height) height : Defines the height of the display. (Must be set alongside width) quality_level : Defines the quality level of the simulation. time_scale : Defines the multiplier for the deltatime in the simulation. If set to a higher value, time will pass faster in the simulation but the physics may perform unpredictably. target_frame_rate : Instructs simulation to try to render at a specified frame rate. capture_frame_rate Instructs the simulation to consider time between updates to always be constant, regardless of the actual frame rate. set_configuration with argument config which is an EngineConfig NamedTuple object. For example, the following code would adjust the time-scale of the simulation to be 2x realtime. from mlagents_envs.environment import UnityEnvironment from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel channel = EngineConfigurationChannel() env = UnityEnvironment(side_channels=[channel]) channel.set_configuration_parameters(time_scale = 2.0) i = env.reset() ...","title":"EngineConfigurationChannel"},{"location":"Python-LLAPI/#environmentparameters","text":"The EnvironmentParameters will allow you to get and set pre-defined numerical values in the environment. This can be useful for adjusting environment-specific settings, or for reading non-agent related information from the environment. You can call get_property and set_property on the side channel to read and write properties. EnvironmentParametersChannel has one methods: set_float_parameter Sets a float parameter in the Unity Environment. key: The string identifier of the property. value: The float value of the property. from mlagents_envs.environment import UnityEnvironment from mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel channel = EnvironmentParametersChannel() env = UnityEnvironment(side_channels=[channel]) channel.set_float_parameter(\"parameter_1\", 2.0) i = env.reset() ... Once a property has been modified in Python, you can access it in C# after the next call to step as follows: var envParameters = Academy.Instance.EnvironmentParameters; float property1 = envParameters.GetWithDefault(\"parameter_1\", 0.0f);","title":"EnvironmentParameters"},{"location":"Python-LLAPI/#custom-side-channels","text":"For information on how to make custom side channels for sending additional data types, see the documentation here .","title":"Custom side channels"},{"location":"Python-PettingZoo-API-Documentation/","text":"Table of Contents mlagents_envs.envs.pettingzoo_env_factory PettingZooEnvFactory env mlagents_envs.envs.unity_aec_env UnityAECEnv __init__ step observe last mlagents_envs.envs.unity_parallel_env UnityParallelEnv __init__ reset mlagents_envs.envs.unity_pettingzoo_base_env UnityPettingzooBaseEnv observation_spaces observation_space action_spaces action_space side_channel reset seed render close mlagents_envs.envs.pettingzoo_env_factory PettingZooEnvFactory Objects class PettingZooEnvFactory() env | env(seed: Optional[int] = None, **kwargs: Union[List, int, bool, None]) -> UnityAECEnv Creates the environment with env_id from unity's default_registry and wraps it in a UnityToPettingZooWrapper Arguments : seed : The seed for the action spaces of the agents. kwargs : Any argument accepted by UnityEnvironment class except file_name mlagents_envs.envs.unity_aec_env UnityAECEnv Objects class UnityAECEnv(UnityPettingzooBaseEnv, AECEnv) Unity AEC (PettingZoo) environment wrapper. __init__ | __init__(env: BaseEnv, seed: Optional[int] = None) Initializes a Unity AEC environment wrapper. Arguments : env : The UnityEnvironment that is being wrapped. seed : The seed for the action spaces of the agents. step | step(action: Any) -> None Sets the action of the active agent and get the observation, reward, done and info of the next agent. Arguments : action : The action for the active agent observe | observe(agent_id) Returns the observation an agent currently can make. last() calls this function. last | last(observe=True) returns observation, cumulative reward, done, info for the current agent (specified by self.agent_selection) mlagents_envs.envs.unity_parallel_env UnityParallelEnv Objects class UnityParallelEnv(UnityPettingzooBaseEnv, ParallelEnv) Unity Parallel (PettingZoo) environment wrapper. __init__ | __init__(env: BaseEnv, seed: Optional[int] = None) Initializes a Unity Parallel environment wrapper. Arguments : env : The UnityEnvironment that is being wrapped. seed : The seed for the action spaces of the agents. reset | reset() -> Dict[str, Any] Resets the environment. mlagents_envs.envs.unity_pettingzoo_base_env UnityPettingzooBaseEnv Objects class UnityPettingzooBaseEnv() Unity Petting Zoo base environment. observation_spaces | @property | observation_spaces() -> Dict[str, spaces.Space] Return the observation spaces of all the agents. observation_space | observation_space(agent: str) -> Optional[spaces.Space] The observation space of the current agent. action_spaces | @property | action_spaces() -> Dict[str, spaces.Space] Return the action spaces of all the agents. action_space | action_space(agent: str) -> Optional[spaces.Space] The action space of the current agent. side_channel | @property | side_channel() -> Dict[str, Any] The side channels of the environment. You can access the side channels of an environment with env.side_channel[<name-of-channel>] . reset | reset() Resets the environment. seed | seed(seed=None) Reseeds the environment (making the resulting environment deterministic). reset() must be called after seed() , and before step() . render | render(mode=\"human\") NOT SUPPORTED. Displays a rendered frame from the environment, if supported. Alternate render modes in the default environments are 'rgb_array' which returns a numpy array and is supported by all environments outside of classic, and 'ansi' which returns the strings printed (specific to classic environments). close | close() -> None Close the environment.","title":"Petting Zoo Documentation"},{"location":"Python-PettingZoo-API-Documentation/#table-of-contents","text":"mlagents_envs.envs.pettingzoo_env_factory PettingZooEnvFactory env mlagents_envs.envs.unity_aec_env UnityAECEnv __init__ step observe last mlagents_envs.envs.unity_parallel_env UnityParallelEnv __init__ reset mlagents_envs.envs.unity_pettingzoo_base_env UnityPettingzooBaseEnv observation_spaces observation_space action_spaces action_space side_channel reset seed render close","title":"Table of Contents"},{"location":"Python-PettingZoo-API-Documentation/#mlagents_envsenvspettingzoo_env_factory","text":"","title":"mlagents_envs.envs.pettingzoo_env_factory"},{"location":"Python-PettingZoo-API-Documentation/#pettingzooenvfactory-objects","text":"class PettingZooEnvFactory()","title":"PettingZooEnvFactory Objects"},{"location":"Python-PettingZoo-API-Documentation/#env","text":"| env(seed: Optional[int] = None, **kwargs: Union[List, int, bool, None]) -> UnityAECEnv Creates the environment with env_id from unity's default_registry and wraps it in a UnityToPettingZooWrapper Arguments : seed : The seed for the action spaces of the agents. kwargs : Any argument accepted by UnityEnvironment class except file_name","title":"env"},{"location":"Python-PettingZoo-API-Documentation/#mlagents_envsenvsunity_aec_env","text":"","title":"mlagents_envs.envs.unity_aec_env"},{"location":"Python-PettingZoo-API-Documentation/#unityaecenv-objects","text":"class UnityAECEnv(UnityPettingzooBaseEnv, AECEnv) Unity AEC (PettingZoo) environment wrapper.","title":"UnityAECEnv Objects"},{"location":"Python-PettingZoo-API-Documentation/#__init__","text":"| __init__(env: BaseEnv, seed: Optional[int] = None) Initializes a Unity AEC environment wrapper. Arguments : env : The UnityEnvironment that is being wrapped. seed : The seed for the action spaces of the agents.","title":"__init__"},{"location":"Python-PettingZoo-API-Documentation/#step","text":"| step(action: Any) -> None Sets the action of the active agent and get the observation, reward, done and info of the next agent. Arguments : action : The action for the active agent","title":"step"},{"location":"Python-PettingZoo-API-Documentation/#observe","text":"| observe(agent_id) Returns the observation an agent currently can make. last() calls this function.","title":"observe"},{"location":"Python-PettingZoo-API-Documentation/#last","text":"| last(observe=True) returns observation, cumulative reward, done, info for the current agent (specified by self.agent_selection)","title":"last"},{"location":"Python-PettingZoo-API-Documentation/#mlagents_envsenvsunity_parallel_env","text":"","title":"mlagents_envs.envs.unity_parallel_env"},{"location":"Python-PettingZoo-API-Documentation/#unityparallelenv-objects","text":"class UnityParallelEnv(UnityPettingzooBaseEnv, ParallelEnv) Unity Parallel (PettingZoo) environment wrapper.","title":"UnityParallelEnv Objects"},{"location":"Python-PettingZoo-API-Documentation/#__init___1","text":"| __init__(env: BaseEnv, seed: Optional[int] = None) Initializes a Unity Parallel environment wrapper. Arguments : env : The UnityEnvironment that is being wrapped. seed : The seed for the action spaces of the agents.","title":"__init__"},{"location":"Python-PettingZoo-API-Documentation/#reset","text":"| reset() -> Dict[str, Any] Resets the environment.","title":"reset"},{"location":"Python-PettingZoo-API-Documentation/#mlagents_envsenvsunity_pettingzoo_base_env","text":"","title":"mlagents_envs.envs.unity_pettingzoo_base_env"},{"location":"Python-PettingZoo-API-Documentation/#unitypettingzoobaseenv-objects","text":"class UnityPettingzooBaseEnv() Unity Petting Zoo base environment.","title":"UnityPettingzooBaseEnv Objects"},{"location":"Python-PettingZoo-API-Documentation/#observation_spaces","text":"| @property | observation_spaces() -> Dict[str, spaces.Space] Return the observation spaces of all the agents.","title":"observation_spaces"},{"location":"Python-PettingZoo-API-Documentation/#observation_space","text":"| observation_space(agent: str) -> Optional[spaces.Space] The observation space of the current agent.","title":"observation_space"},{"location":"Python-PettingZoo-API-Documentation/#action_spaces","text":"| @property | action_spaces() -> Dict[str, spaces.Space] Return the action spaces of all the agents.","title":"action_spaces"},{"location":"Python-PettingZoo-API-Documentation/#action_space","text":"| action_space(agent: str) -> Optional[spaces.Space] The action space of the current agent.","title":"action_space"},{"location":"Python-PettingZoo-API-Documentation/#side_channel","text":"| @property | side_channel() -> Dict[str, Any] The side channels of the environment. You can access the side channels of an environment with env.side_channel[<name-of-channel>] .","title":"side_channel"},{"location":"Python-PettingZoo-API-Documentation/#reset_1","text":"| reset() Resets the environment.","title":"reset"},{"location":"Python-PettingZoo-API-Documentation/#seed","text":"| seed(seed=None) Reseeds the environment (making the resulting environment deterministic). reset() must be called after seed() , and before step() .","title":"seed"},{"location":"Python-PettingZoo-API-Documentation/#render","text":"| render(mode=\"human\") NOT SUPPORTED. Displays a rendered frame from the environment, if supported. Alternate render modes in the default environments are 'rgb_array' which returns a numpy array and is supported by all environments outside of classic, and 'ansi' which returns the strings printed (specific to classic environments).","title":"render"},{"location":"Python-PettingZoo-API-Documentation/#close","text":"| close() -> None Close the environment.","title":"close"},{"location":"Python-PettingZoo-API/","text":"Unity ML-Agents PettingZoo Wrapper With the increasing interest in multi-agent training with a gym-like API, we provide a PettingZoo Wrapper around the Petting Zoo API . Our wrapper provides interfaces on top of our UnityEnvironment class, which is the default way of interfacing with a Unity environment via Python. Installation and Examples [Colab] PettingZoo Wrapper Example This colab notebook demonstrates the example usage of the wrapper, including installation, basic usages, and an example with our Striker vs Goalie environment which is a multi-agents environment with multiple different behavior names. API interface This wrapper is compatible with PettingZoo API. Please check out PettingZoo API page for more details. Here's an example of interacting with wrapped environment: unity_env = UnityEnvironment(\"StrikersVsGoalie\") env = UnityToPettingZooWrapper(unity_env) env.reset() for agent in env.agent_iter(): observation, reward, done, info = env.last() action = policy(observation, agent) env.step(action) Notes The wrapper is compatible with PettingZoo API interface but works in a little bit different way under the hood. Instead of stepping the environment in every env.step(action) , our environment will store the action, and will only perform environment stepping when all the agents requesting for actions in the current step have been assigned an action. This is for performance consideration that the communication between Unity and python is more efficient when data are sent in batches. Since the actions are stored with no environment, some additional API might behave in unexpected way. Specifically, env.reward should return the instant reward in that particular step, but you would only see those reward when an actual environment step is performed. It's recommended that you follow the API definition for training (access rewards from env.last() instead of env.reward ) and the underlying mechanism shouldn't affect training results. Advanced features in PettingZoo like Parallel API is not guaranteed to work with this wrapper. The environments will automatically reset when it's done, so env.agent_iter(max_step) will keep going on until the specified max step is reached (default: 2**63 ). There is no need to call env.reset() except for the very beginning of instantiating an environment.","title":"Getting started with the PettingZoo API"},{"location":"Python-PettingZoo-API/#unity-ml-agents-pettingzoo-wrapper","text":"With the increasing interest in multi-agent training with a gym-like API, we provide a PettingZoo Wrapper around the Petting Zoo API . Our wrapper provides interfaces on top of our UnityEnvironment class, which is the default way of interfacing with a Unity environment via Python.","title":"Unity ML-Agents PettingZoo Wrapper"},{"location":"Python-PettingZoo-API/#installation-and-examples","text":"[Colab] PettingZoo Wrapper Example This colab notebook demonstrates the example usage of the wrapper, including installation, basic usages, and an example with our Striker vs Goalie environment which is a multi-agents environment with multiple different behavior names.","title":"Installation and Examples"},{"location":"Python-PettingZoo-API/#api-interface","text":"This wrapper is compatible with PettingZoo API. Please check out PettingZoo API page for more details. Here's an example of interacting with wrapped environment: unity_env = UnityEnvironment(\"StrikersVsGoalie\") env = UnityToPettingZooWrapper(unity_env) env.reset() for agent in env.agent_iter(): observation, reward, done, info = env.last() action = policy(observation, agent) env.step(action)","title":"API interface"},{"location":"Python-PettingZoo-API/#notes","text":"The wrapper is compatible with PettingZoo API interface but works in a little bit different way under the hood. Instead of stepping the environment in every env.step(action) , our environment will store the action, and will only perform environment stepping when all the agents requesting for actions in the current step have been assigned an action. This is for performance consideration that the communication between Unity and python is more efficient when data are sent in batches. Since the actions are stored with no environment, some additional API might behave in unexpected way. Specifically, env.reward should return the instant reward in that particular step, but you would only see those reward when an actual environment step is performed. It's recommended that you follow the API definition for training (access rewards from env.last() instead of env.reward ) and the underlying mechanism shouldn't affect training results. Advanced features in PettingZoo like Parallel API is not guaranteed to work with this wrapper. The environments will automatically reset when it's done, so env.agent_iter(max_step) will keep going on until the specified max step is reached (default: 2**63 ). There is no need to call env.reset() except for the very beginning of instantiating an environment.","title":"Notes"},{"location":"Training-Configuration-File/","text":"Training Configuration File Table of Contents Common Trainer Configurations Trainer-specific Configurations PPO-specific Configurations SAC-specific Configurations Reward Signals Extrinsic Rewards Curiosity Intrinsic Reward GAIL Intrinsic Reward RND Intrinsic Reward Reward Signal Settings for SAC Behavioral Cloning Memory-enhanced Agents using Recurrent Neural Networks Self-Play Note on Reward Signals Note on Swap Steps Common Trainer Configurations One of the first decisions you need to make regarding your training run is which trainer to use: PPO, SAC, or POCA. There are some training configurations that are common to both trainers (which we review now) and others that depend on the choice of the trainer (which we review on subsequent sections). Setting Description trainer_type (default = ppo ) The type of trainer to use: ppo , sac , or poca . summary_freq (default = 50000 ) Number of experiences that needs to be collected before generating and displaying training statistics. This determines the granularity of the graphs in Tensorboard. time_horizon (default = 64 ) How many steps of experience to collect per-agent before adding it to the experience buffer. When this limit is reached before the end of an episode, a value estimate is used to predict the overall expected reward from the agent's current state. As such, this parameter trades off between a less biased, but higher variance estimate (long time horizon) and more biased, but less varied estimate (short time horizon). In cases where there are frequent rewards within an episode, or episodes are prohibitively large, a smaller number can be more ideal. This number should be large enough to capture all the important behavior within a sequence of an agent's actions. Typical range: 32 - 2048 max_steps (default = 500000 ) Total number of steps (i.e., observation collected and action taken) that must be taken in the environment (or across all environments if using multiple in parallel) before ending the training process. If you have multiple agents with the same behavior name within your environment, all steps taken by those agents will contribute to the same max_steps count. Typical range: 5e5 - 1e7 keep_checkpoints (default = 5 ) The maximum number of model checkpoints to keep. Checkpoints are saved after the number of steps specified by the checkpoint_interval option. Once the maximum number of checkpoints has been reached, the oldest checkpoint is deleted when saving a new checkpoint. checkpoint_interval (default = 500000 ) The number of experiences collected between each checkpoint by the trainer. A maximum of keep_checkpoints checkpoints are saved before old ones are deleted. Each checkpoint saves the .onnx files in results/ folder. init_path (default = None) Initialize trainer from a previously saved model. Note that the prior run should have used the same trainer configurations as the current run, and have been saved with the same version of ML-Agents. You can provide either the file name or the full path to the checkpoint, e.g. {checkpoint_name.pt} or ./models/{run-id}/{behavior_name}/{checkpoint_name.pt} . This option is provided in case you want to initialize different behaviors from different runs or initialize from an older checkpoint; in most cases, it is sufficient to use the --initialize-from CLI parameter to initialize all models from the same run. threaded (default = false ) Allow environments to step while updating the model. This might result in a training speedup, especially when using SAC. For best performance, leave setting to false when using self-play. hyperparameters -> learning_rate (default = 3e-4 ) Initial learning rate for gradient descent. Corresponds to the strength of each gradient descent update step. This should typically be decreased if training is unstable, and the reward does not consistently increase. Typical range: 1e-5 - 1e-3 hyperparameters -> batch_size Number of experiences in each iteration of gradient descent. This should always be multiple times smaller than buffer_size . If you are using continuous actions, this value should be large (on the order of 1000s). If you are using only discrete actions, this value should be smaller (on the order of 10s). Typical range: (Continuous - PPO): 512 - 5120 ; (Continuous - SAC): 128 - 1024 ; (Discrete, PPO & SAC): 32 - 512 . hyperparameters -> buffer_size (default = 10240 for PPO and 50000 for SAC) PPO: Number of experiences to collect before updating the policy model. Corresponds to how many experiences should be collected before we do any learning or updating of the model. This should be multiple times larger than batch_size . Typically a larger buffer_size corresponds to more stable training updates. SAC: The max size of the experience buffer - on the order of thousands of times longer than your episodes, so that SAC can learn from old as well as new experiences. Typical range: PPO: 2048 - 409600 ; SAC: 50000 - 1000000 hyperparameters -> learning_rate_schedule (default = linear for PPO and constant for SAC) Determines how learning rate changes over time. For PPO, we recommend decaying learning rate until max_steps so learning converges more stably. However, for some cases (e.g. training for an unknown amount of time) this feature can be disabled. For SAC, we recommend holding learning rate constant so that the agent can continue to learn until its Q function converges naturally. linear decays the learning_rate linearly, reaching 0 at max_steps, while constant keeps the learning rate constant for the entire training run. network_settings -> hidden_units (default = 128 ) Number of units in the hidden layers of the neural network. Correspond to how many units are in each fully connected layer of the neural network. For simple problems where the correct action is a straightforward combination of the observation inputs, this should be small. For problems where the action is a very complex interaction between the observation variables, this should be larger. Typical range: 32 - 512 network_settings -> num_layers (default = 2 ) The number of hidden layers in the neural network. Corresponds to how many hidden layers are present after the observation input, or after the CNN encoding of the visual observation. For simple problems, fewer layers are likely to train faster and more efficiently. More layers may be necessary for more complex control problems. Typical range: 1 - 3 network_settings -> normalize (default = false ) Whether normalization is applied to the vector observation inputs. This normalization is based on the running average and variance of the vector observation. Normalization can be helpful in cases with complex continuous control problems, but may be harmful with simpler discrete control problems. network_settings -> vis_encode_type (default = simple ) Encoder type for encoding visual observations. simple (default) uses a simple encoder which consists of two convolutional layers, nature_cnn uses the CNN implementation proposed by Mnih et al. , consisting of three convolutional layers, and resnet uses the IMPALA Resnet consisting of three stacked layers, each with two residual blocks, making a much larger network than the other two. match3 is a smaller CNN ( Gudmundsoon et al. ) that can capture more granular spatial relationships and is optimized for board games. fully_connected uses a single fully connected dense layer as encoder without any convolutional layers. Due to the size of convolution kernel, there is a minimum observation size limitation that each encoder type can handle - simple : 20x20, nature_cnn : 36x36, resnet : 15 x 15, match3 : 5x5. fully_connected doesn't have convolutional layers and thus no size limits, but since it has less representation power it should be reserved for very small inputs. Note that using the match3 CNN with very large visual input might result in a huge observation encoding and thus potentially slow down training or cause memory issues. network_settings -> conditioning_type (default = hyper ) Conditioning type for the policy using goal observations. none treats the goal observations as regular observations, hyper (default) uses a HyperNetwork with goal observations as input to generate some of the weights of the policy. Note that when using hyper the number of parameters of the network increases greatly. Therefore, it is recommended to reduce the number of hidden_units when using this conditioning_type Trainer-specific Configurations Depending on your choice of a trainer, there are additional trainer-specific configurations. We present them below in two separate tables, but keep in mind that you only need to include the configurations for the trainer selected (i.e. the trainer setting above). PPO-specific Configurations Setting Description hyperparameters -> beta (default = 5.0e-3 ) Strength of the entropy regularization, which makes the policy \"more random.\" This ensures that agents properly explore the action space during training. Increasing this will ensure more random actions are taken. This should be adjusted such that the entropy (measurable from TensorBoard) slowly decreases alongside increases in reward. If entropy drops too quickly, increase beta. If entropy drops too slowly, decrease beta . Typical range: 1e-4 - 1e-2 hyperparameters -> epsilon (default = 0.2 ) Influences how rapidly the policy can evolve during training. Corresponds to the acceptable threshold of divergence between the old and new policies during gradient descent updating. Setting this value small will result in more stable updates, but will also slow the training process. Typical range: 0.1 - 0.3 hyperparameters -> beta_schedule (default = learning_rate_schedule ) Determines how beta changes over time. linear decays beta linearly, reaching 0 at max_steps, while constant keeps beta constant for the entire training run. If not explicitly set, the default beta schedule will be set to hyperparameters -> learning_rate_schedule . hyperparameters -> epsilon_schedule (default = learning_rate_schedule ) Determines how epsilon changes over time (PPO only). linear decays epsilon linearly, reaching 0 at max_steps, while constant keeps the epsilon constant for the entire training run. If not explicitly set, the default epsilon schedule will be set to hyperparameters -> learning_rate_schedule . hyperparameters -> lambd (default = 0.95 ) Regularization parameter (lambda) used when calculating the Generalized Advantage Estimate ( GAE ). This can be thought of as how much the agent relies on its current value estimate when calculating an updated value estimate. Low values correspond to relying more on the current value estimate (which can be high bias), and high values correspond to relying more on the actual rewards received in the environment (which can be high variance). The parameter provides a trade-off between the two, and the right value can lead to a more stable training process. Typical range: 0.9 - 0.95 hyperparameters -> num_epoch (default = 3 ) Number of passes to make through the experience buffer when performing gradient descent optimization.The larger the batch_size, the larger it is acceptable to make this. Decreasing this will ensure more stable updates, at the cost of slower learning. Typical range: 3 - 10 SAC-specific Configurations Setting Description hyperparameters -> buffer_init_steps (default = 0 ) Number of experiences to collect into the buffer before updating the policy model. As the untrained policy is fairly random, pre-filling the buffer with random actions is useful for exploration. Typically, at least several episodes of experiences should be pre-filled. Typical range: 1000 - 10000 hyperparameters -> init_entcoef (default = 1.0 ) How much the agent should explore in the beginning of training. Corresponds to the initial entropy coefficient set at the beginning of training. In SAC, the agent is incentivized to make its actions entropic to facilitate better exploration. The entropy coefficient weighs the true reward with a bonus entropy reward. The entropy coefficient is automatically adjusted to a preset target entropy, so the init_entcoef only corresponds to the starting value of the entropy bonus. Increase init_entcoef to explore more in the beginning, decrease to converge to a solution faster. Typical range: (Continuous): 0.5 - 1.0 ; (Discrete): 0.05 - 0.5 hyperparameters -> save_replay_buffer (default = false ) Whether to save and load the experience replay buffer as well as the model when quitting and re-starting training. This may help resumes go more smoothly, as the experiences collected won't be wiped. Note that replay buffers can be very large, and will take up a considerable amount of disk space. For that reason, we disable this feature by default. hyperparameters -> tau (default = 0.005 ) How aggressively to update the target network used for bootstrapping value estimation in SAC. Corresponds to the magnitude of the target Q update during the SAC model update. In SAC, there are two neural networks: the target and the policy. The target network is used to bootstrap the policy's estimate of the future rewards at a given state, and is fixed while the policy is being updated. This target is then slowly updated according to tau. Typically, this value should be left at 0.005. For simple problems, increasing tau to 0.01 might reduce the time it takes to learn, at the cost of stability. Typical range: 0.005 - 0.01 hyperparameters -> steps_per_update (default = 1 ) Average ratio of agent steps (actions) taken to updates made of the agent's policy. In SAC, a single \"update\" corresponds to grabbing a batch of size batch_size from the experience replay buffer, and using this mini batch to update the models. Note that it is not guaranteed that after exactly steps_per_update steps an update will be made, only that the ratio will hold true over many steps. Typically, steps_per_update should be greater than or equal to 1. Note that setting steps_per_update lower will improve sample efficiency (reduce the number of steps required to train) but increase the CPU time spent performing updates. For most environments where steps are fairly fast (e.g. our example environments) steps_per_update equal to the number of agents in the scene is a good balance. For slow environments (steps take 0.1 seconds or more) reducing steps_per_update may improve training speed. We can also change steps_per_update to lower than 1 to update more often than once per step, though this will usually result in a slowdown unless the environment is very slow. Typical range: 1 - 20 hyperparameters -> reward_signal_num_update (default = steps_per_update ) Number of steps per mini batch sampled and used for updating the reward signals. By default, we update the reward signals once every time the main policy is updated. However, to imitate the training procedure in certain imitation learning papers (e.g. Kostrikov et. al , Blond\u00e9 et. al ), we may want to update the reward signal (GAIL) M times for every update of the policy. We can change steps_per_update of SAC to N, as well as reward_signal_steps_per_update under reward_signals to N / M to accomplish this. By default, reward_signal_steps_per_update is set to steps_per_update . MA-POCA-specific Configurations MA-POCA uses the same configurations as PPO, and there are no additional POCA-specific parameters. NOTE : Reward signals other than Extrinsic Rewards have not been extensively tested with MA-POCA, though they can still be added and used for training on a your-mileage-may-vary basis. Reward Signals The reward_signals section enables the specification of settings for both extrinsic (i.e. environment-based) and intrinsic reward signals (e.g. curiosity and GAIL). Each reward signal should define at least two parameters, strength and gamma , in addition to any class-specific hyperparameters. Note that to remove a reward signal, you should delete its entry entirely from reward_signals . At least one reward signal should be left defined at all times. Provide the following configurations to design the reward signal for your training run. Extrinsic Rewards Enable these settings to ensure that your training run incorporates your environment-based reward signal: Setting Description extrinsic -> strength (default = 1.0 ) Factor by which to multiply the reward given by the environment. Typical ranges will vary depending on the reward signal. Typical range: 1.00 extrinsic -> gamma (default = 0.99 ) Discount factor for future rewards coming from the environment. This can be thought of as how far into the future the agent should care about possible rewards. In situations when the agent should be acting in the present in order to prepare for rewards in the distant future, this value should be large. In cases when rewards are more immediate, it can be smaller. Must be strictly smaller than 1. Typical range: 0.8 - 0.995 Curiosity Intrinsic Reward To enable curiosity, provide these settings: Setting Description curiosity -> strength (default = 1.0 ) Magnitude of the curiosity reward generated by the intrinsic curiosity module. This should be scaled in order to ensure it is large enough to not be overwhelmed by extrinsic reward signals in the environment. Likewise it should not be too large to overwhelm the extrinsic reward signal. Typical range: 0.001 - 0.1 curiosity -> gamma (default = 0.99 ) Discount factor for future rewards. Typical range: 0.8 - 0.995 curiosity -> network_settings Please see the documentation for network_settings under Common Trainer Configurations . The network specs used by the intrinsic curiosity model. The value should of hidden_units should be small enough to encourage the ICM to compress the original observation, but also not too small to prevent it from learning to differentiate between expected and actual observations. Typical range: 64 - 256 curiosity -> learning_rate (default = 3e-4 ) Learning rate used to update the intrinsic curiosity module. This should typically be decreased if training is unstable, and the curiosity loss is unstable. Typical range: 1e-5 - 1e-3 GAIL Intrinsic Reward To enable GAIL (assuming you have recorded demonstrations), provide these settings: Setting Description gail -> strength (default = 1.0 ) Factor by which to multiply the raw reward. Note that when using GAIL with an Extrinsic Signal, this value should be set lower if your demonstrations are suboptimal (e.g. from a human), so that a trained agent will focus on receiving extrinsic rewards instead of exactly copying the demonstrations. Keep the strength below about 0.1 in those cases. Typical range: 0.01 - 1.0 gail -> gamma (default = 0.99 ) Discount factor for future rewards. Typical range: 0.8 - 0.9 gail -> demo_path (Required, no default) The path to your .demo file or directory of .demo files. gail -> network_settings Please see the documentation for network_settings under Common Trainer Configurations . The network specs for the GAIL discriminator. The value of hidden_units should be small enough to encourage the discriminator to compress the original observation, but also not too small to prevent it from learning to differentiate between demonstrated and actual behavior. Dramatically increasing this size will also negatively affect training times. Typical range: 64 - 256 gail -> learning_rate (Optional, default = 3e-4 ) Learning rate used to update the discriminator. This should typically be decreased if training is unstable, and the GAIL loss is unstable. Typical range: 1e-5 - 1e-3 gail -> use_actions (default = false ) Determines whether the discriminator should discriminate based on both observations and actions, or just observations. Set to True if you want the agent to mimic the actions from the demonstrations, and False if you'd rather have the agent visit the same states as in the demonstrations but with possibly different actions. Setting to False is more likely to be stable, especially with imperfect demonstrations, but may learn slower. gail -> use_vail (default = false ) Enables a variational bottleneck within the GAIL discriminator. This forces the discriminator to learn a more general representation and reduces its tendency to be \"too good\" at discriminating, making learning more stable. However, it does increase training time. Enable this if you notice your imitation learning is unstable, or unable to learn the task at hand. RND Intrinsic Reward Random Network Distillation (RND) is only available for the PyTorch trainers. To enable RND, provide these settings: Setting Description rnd -> strength (default = 1.0 ) Magnitude of the curiosity reward generated by the intrinsic rnd module. This should be scaled in order to ensure it is large enough to not be overwhelmed by extrinsic reward signals in the environment. Likewise it should not be too large to overwhelm the extrinsic reward signal. Typical range: 0.001 - 0.01 rnd -> gamma (default = 0.99 ) Discount factor for future rewards. Typical range: 0.8 - 0.995 rnd -> network_settings Please see the documentation for network_settings under Common Trainer Configurations . The network specs for the RND model. curiosity -> learning_rate (default = 3e-4 ) Learning rate used to update the RND module. This should be large enough for the RND module to quickly learn the state representation, but small enough to allow for stable learning. Typical range: 1e-5 - 1e-3 Behavioral Cloning To enable Behavioral Cloning as a pre-training option (assuming you have recorded demonstrations), provide the following configurations under the behavior_cloning section: Setting Description demo_path (Required, no default) The path to your .demo file or directory of .demo files. strength (default = 1.0 ) Learning rate of the imitation relative to the learning rate of PPO, and roughly corresponds to how strongly we allow BC to influence the policy. Typical range: 0.1 - 0.5 steps (default = 0 ) During BC, it is often desirable to stop using demonstrations after the agent has \"seen\" rewards, and allow it to optimize past the available demonstrations and/or generalize outside of the provided demonstrations. steps corresponds to the training steps over which BC is active. The learning rate of BC will anneal over the steps. Set the steps to 0 for constant imitation over the entire training run. batch_size (default = batch_size of trainer) Number of demonstration experiences used for one iteration of a gradient descent update. If not specified, it will default to the batch_size of the trainer. Typical range: (Continuous): 512 - 5120 ; (Discrete): 32 - 512 num_epoch (default = num_epoch of trainer) Number of passes through the experience buffer during gradient descent. If not specified, it will default to the number of epochs set for PPO. Typical range: 3 - 10 samples_per_update (default = 0 ) Maximum number of samples to use during each imitation update. You may want to lower this if your demonstration dataset is very large to avoid overfitting the policy on demonstrations. Set to 0 to train over all of the demonstrations at each update step. Typical range: buffer_size Memory-enhanced Agents using Recurrent Neural Networks You can enable your agents to use memory by adding a memory section under network_settings , and setting memory_size and sequence_length : Setting Description network_settings -> memory -> memory_size (default = 128 ) Size of the memory an agent must keep. In order to use a LSTM, training requires a sequence of experiences instead of single experiences. Corresponds to the size of the array of floating point numbers used to store the hidden state of the recurrent neural network of the policy. This value must be a multiple of 2, and should scale with the amount of information you expect the agent will need to remember in order to successfully complete the task. Typical range: 32 - 256 network_settings -> memory -> sequence_length (default = 64 ) Defines how long the sequences of experiences must be while training. Note that if this number is too small, the agent will not be able to remember things over longer periods of time. If this number is too large, the neural network will take longer to train. Typical range: 4 - 128 A few considerations when deciding to use memory: LSTM does not work well with continuous actions. Please use discrete actions for better results. Adding a recurrent layer increases the complexity of the neural network, it is recommended to decrease num_layers when using recurrent. It is required that memory_size be divisible by 2. Self-Play Training with self-play adds additional confounding factors to the usual issues faced by reinforcement learning. In general, the tradeoff is between the skill level and generality of the final policy and the stability of learning. Training against a set of slowly or unchanging adversaries with low diversity results in a more stable learning process than training against a set of quickly changing adversaries with high diversity. With this context, this guide discusses the exposed self-play hyperparameters and intuitions for tuning them. If your environment contains multiple agents that are divided into teams, you can leverage our self-play training option by providing these configurations for each Behavior: Setting Description save_steps (default = 20000 ) Number of trainer steps between snapshots. For example, if save_steps=10000 then a snapshot of the current policy will be saved every 10000 trainer steps. Note, trainer steps are counted per agent. For more information, please see the migration doc after v0.13. A larger value of save_steps will yield a set of opponents that cover a wider range of skill levels and possibly play styles since the policy receives more training. As a result, the agent trains against a wider variety of opponents. Learning a policy to defeat more diverse opponents is a harder problem and so may require more overall training steps but also may lead to more general and robust policy at the end of training. This value is also dependent on how intrinsically difficult the environment is for the agent. Typical range: 10000 - 100000 team_change (default = 5 * save_steps ) Number of trainer_steps between switching the learning team. This is the number of trainer steps the teams associated with a specific ghost trainer will train before a different team becomes the new learning team. It is possible that, in asymmetric games, opposing teams require fewer trainer steps to make similar performance gains. This enables users to train a more complicated team of agents for more trainer steps than a simpler team of agents per team switch. A larger value of team-change will allow the agent to train longer against it's opponents. The longer an agent trains against the same set of opponents the more able it will be to defeat them. However, training against them for too long may result in overfitting to the particular opponent strategies and so the agent may fail against the next batch of opponents. The value of team-change will determine how many snapshots of the agent's policy are saved to be used as opponents for the other team. So, we recommend setting this value as a function of the save_steps parameter discussed previously. Typical range: 4x-10x where x= save_steps swap_steps (default = 10000 ) Number of ghost steps (not trainer steps) between swapping the opponents policy with a different snapshot. A 'ghost step' refers to a step taken by an agent that is following a fixed policy and not learning . The reason for this distinction is that in asymmetric games, we may have teams with an unequal number of agents e.g. a 2v1 scenario like our Strikers Vs Goalie example environment. The team with two agents collects twice as many agent steps per environment step as the team with one agent. Thus, these two values will need to be distinct to ensure that the same number of trainer steps corresponds to the same number of opponent swaps for each team. The formula for swap_steps if a user desires x swaps of a team with num_agents agents against an opponent team with num_opponent_agents agents during team-change total steps is: (num_agents / num_opponent_agents) * (team_change / x) Typical range: 10000 - 100000 play_against_latest_model_ratio (default = 0.5 ) Probability an agent will play against the latest opponent policy. With probability 1 - play_against_latest_model_ratio , the agent will play against a snapshot of its opponent from a past iteration. A larger value of play_against_latest_model_ratio indicates that an agent will be playing against the current opponent more often. Since the agent is updating it's policy, the opponent will be different from iteration to iteration. This can lead to an unstable learning environment, but poses the agent with an auto-curricula of more increasingly challenging situations which may lead to a stronger final policy. Typical range: 0.0 - 1.0 window (default = 10 ) Size of the sliding window of past snapshots from which the agent's opponents are sampled. For example, a window size of 5 will save the last 5 snapshots taken. Each time a new snapshot is taken, the oldest is discarded. A larger value of window means that an agent's pool of opponents will contain a larger diversity of behaviors since it will contain policies from earlier in the training run. Like in the save_steps hyperparameter, the agent trains against a wider variety of opponents. Learning a policy to defeat more diverse opponents is a harder problem and so may require more overall training steps but also may lead to more general and robust policy at the end of training. Typical range: 5 - 30 Note on Reward Signals We make the assumption that the final reward in a trajectory corresponds to the outcome of an episode. A final reward of +1 indicates winning, -1 indicates losing and 0 indicates a draw. The ELO calculation (discussed below) depends on this final reward being either +1, 0, -1. The reward signal should still be used as described in the documentation for the other trainers. However, we encourage users to be a bit more conservative when shaping reward functions due to the instability and non-stationarity of learning in adversarial games. Specifically, we encourage users to begin with the simplest possible reward function (+1 winning, -1 losing) and to allow for more iterations of training to compensate for the sparsity of reward. Note on Swap Steps As an example, in a 2v1 scenario, if we want the swap to occur x=4 times during team-change=200000 steps, the swap_steps for the team of one agent is: swap_steps = (1 / 2) * (200000 / 4) = 25000 The swap_steps for the team of two agents is: swap_steps = (2 / 1) * (200000 / 4) = 100000 Note, with equal team sizes, the first term is equal to 1 and swap_steps can be calculated by just dividing the total steps by the desired number of swaps. A larger value of swap_steps means that an agent will play against the same fixed opponent for a longer number of training iterations. This results in a more stable training scenario, but leaves the agent open to the risk of overfitting it's behavior for this particular opponent. Thus, when a new opponent is swapped, the agent may lose more often than expected.","title":"Training Configuration File"},{"location":"Training-Configuration-File/#training-configuration-file","text":"Table of Contents Common Trainer Configurations Trainer-specific Configurations PPO-specific Configurations SAC-specific Configurations Reward Signals Extrinsic Rewards Curiosity Intrinsic Reward GAIL Intrinsic Reward RND Intrinsic Reward Reward Signal Settings for SAC Behavioral Cloning Memory-enhanced Agents using Recurrent Neural Networks Self-Play Note on Reward Signals Note on Swap Steps","title":"Training Configuration File"},{"location":"Training-Configuration-File/#common-trainer-configurations","text":"One of the first decisions you need to make regarding your training run is which trainer to use: PPO, SAC, or POCA. There are some training configurations that are common to both trainers (which we review now) and others that depend on the choice of the trainer (which we review on subsequent sections). Setting Description trainer_type (default = ppo ) The type of trainer to use: ppo , sac , or poca . summary_freq (default = 50000 ) Number of experiences that needs to be collected before generating and displaying training statistics. This determines the granularity of the graphs in Tensorboard. time_horizon (default = 64 ) How many steps of experience to collect per-agent before adding it to the experience buffer. When this limit is reached before the end of an episode, a value estimate is used to predict the overall expected reward from the agent's current state. As such, this parameter trades off between a less biased, but higher variance estimate (long time horizon) and more biased, but less varied estimate (short time horizon). In cases where there are frequent rewards within an episode, or episodes are prohibitively large, a smaller number can be more ideal. This number should be large enough to capture all the important behavior within a sequence of an agent's actions. Typical range: 32 - 2048 max_steps (default = 500000 ) Total number of steps (i.e., observation collected and action taken) that must be taken in the environment (or across all environments if using multiple in parallel) before ending the training process. If you have multiple agents with the same behavior name within your environment, all steps taken by those agents will contribute to the same max_steps count. Typical range: 5e5 - 1e7 keep_checkpoints (default = 5 ) The maximum number of model checkpoints to keep. Checkpoints are saved after the number of steps specified by the checkpoint_interval option. Once the maximum number of checkpoints has been reached, the oldest checkpoint is deleted when saving a new checkpoint. checkpoint_interval (default = 500000 ) The number of experiences collected between each checkpoint by the trainer. A maximum of keep_checkpoints checkpoints are saved before old ones are deleted. Each checkpoint saves the .onnx files in results/ folder. init_path (default = None) Initialize trainer from a previously saved model. Note that the prior run should have used the same trainer configurations as the current run, and have been saved with the same version of ML-Agents. You can provide either the file name or the full path to the checkpoint, e.g. {checkpoint_name.pt} or ./models/{run-id}/{behavior_name}/{checkpoint_name.pt} . This option is provided in case you want to initialize different behaviors from different runs or initialize from an older checkpoint; in most cases, it is sufficient to use the --initialize-from CLI parameter to initialize all models from the same run. threaded (default = false ) Allow environments to step while updating the model. This might result in a training speedup, especially when using SAC. For best performance, leave setting to false when using self-play. hyperparameters -> learning_rate (default = 3e-4 ) Initial learning rate for gradient descent. Corresponds to the strength of each gradient descent update step. This should typically be decreased if training is unstable, and the reward does not consistently increase. Typical range: 1e-5 - 1e-3 hyperparameters -> batch_size Number of experiences in each iteration of gradient descent. This should always be multiple times smaller than buffer_size . If you are using continuous actions, this value should be large (on the order of 1000s). If you are using only discrete actions, this value should be smaller (on the order of 10s). Typical range: (Continuous - PPO): 512 - 5120 ; (Continuous - SAC): 128 - 1024 ; (Discrete, PPO & SAC): 32 - 512 . hyperparameters -> buffer_size (default = 10240 for PPO and 50000 for SAC) PPO: Number of experiences to collect before updating the policy model. Corresponds to how many experiences should be collected before we do any learning or updating of the model. This should be multiple times larger than batch_size . Typically a larger buffer_size corresponds to more stable training updates. SAC: The max size of the experience buffer - on the order of thousands of times longer than your episodes, so that SAC can learn from old as well as new experiences. Typical range: PPO: 2048 - 409600 ; SAC: 50000 - 1000000 hyperparameters -> learning_rate_schedule (default = linear for PPO and constant for SAC) Determines how learning rate changes over time. For PPO, we recommend decaying learning rate until max_steps so learning converges more stably. However, for some cases (e.g. training for an unknown amount of time) this feature can be disabled. For SAC, we recommend holding learning rate constant so that the agent can continue to learn until its Q function converges naturally. linear decays the learning_rate linearly, reaching 0 at max_steps, while constant keeps the learning rate constant for the entire training run. network_settings -> hidden_units (default = 128 ) Number of units in the hidden layers of the neural network. Correspond to how many units are in each fully connected layer of the neural network. For simple problems where the correct action is a straightforward combination of the observation inputs, this should be small. For problems where the action is a very complex interaction between the observation variables, this should be larger. Typical range: 32 - 512 network_settings -> num_layers (default = 2 ) The number of hidden layers in the neural network. Corresponds to how many hidden layers are present after the observation input, or after the CNN encoding of the visual observation. For simple problems, fewer layers are likely to train faster and more efficiently. More layers may be necessary for more complex control problems. Typical range: 1 - 3 network_settings -> normalize (default = false ) Whether normalization is applied to the vector observation inputs. This normalization is based on the running average and variance of the vector observation. Normalization can be helpful in cases with complex continuous control problems, but may be harmful with simpler discrete control problems. network_settings -> vis_encode_type (default = simple ) Encoder type for encoding visual observations. simple (default) uses a simple encoder which consists of two convolutional layers, nature_cnn uses the CNN implementation proposed by Mnih et al. , consisting of three convolutional layers, and resnet uses the IMPALA Resnet consisting of three stacked layers, each with two residual blocks, making a much larger network than the other two. match3 is a smaller CNN ( Gudmundsoon et al. ) that can capture more granular spatial relationships and is optimized for board games. fully_connected uses a single fully connected dense layer as encoder without any convolutional layers. Due to the size of convolution kernel, there is a minimum observation size limitation that each encoder type can handle - simple : 20x20, nature_cnn : 36x36, resnet : 15 x 15, match3 : 5x5. fully_connected doesn't have convolutional layers and thus no size limits, but since it has less representation power it should be reserved for very small inputs. Note that using the match3 CNN with very large visual input might result in a huge observation encoding and thus potentially slow down training or cause memory issues. network_settings -> conditioning_type (default = hyper ) Conditioning type for the policy using goal observations. none treats the goal observations as regular observations, hyper (default) uses a HyperNetwork with goal observations as input to generate some of the weights of the policy. Note that when using hyper the number of parameters of the network increases greatly. Therefore, it is recommended to reduce the number of hidden_units when using this conditioning_type","title":"Common Trainer Configurations"},{"location":"Training-Configuration-File/#trainer-specific-configurations","text":"Depending on your choice of a trainer, there are additional trainer-specific configurations. We present them below in two separate tables, but keep in mind that you only need to include the configurations for the trainer selected (i.e. the trainer setting above).","title":"Trainer-specific Configurations"},{"location":"Training-Configuration-File/#ppo-specific-configurations","text":"Setting Description hyperparameters -> beta (default = 5.0e-3 ) Strength of the entropy regularization, which makes the policy \"more random.\" This ensures that agents properly explore the action space during training. Increasing this will ensure more random actions are taken. This should be adjusted such that the entropy (measurable from TensorBoard) slowly decreases alongside increases in reward. If entropy drops too quickly, increase beta. If entropy drops too slowly, decrease beta . Typical range: 1e-4 - 1e-2 hyperparameters -> epsilon (default = 0.2 ) Influences how rapidly the policy can evolve during training. Corresponds to the acceptable threshold of divergence between the old and new policies during gradient descent updating. Setting this value small will result in more stable updates, but will also slow the training process. Typical range: 0.1 - 0.3 hyperparameters -> beta_schedule (default = learning_rate_schedule ) Determines how beta changes over time. linear decays beta linearly, reaching 0 at max_steps, while constant keeps beta constant for the entire training run. If not explicitly set, the default beta schedule will be set to hyperparameters -> learning_rate_schedule . hyperparameters -> epsilon_schedule (default = learning_rate_schedule ) Determines how epsilon changes over time (PPO only). linear decays epsilon linearly, reaching 0 at max_steps, while constant keeps the epsilon constant for the entire training run. If not explicitly set, the default epsilon schedule will be set to hyperparameters -> learning_rate_schedule . hyperparameters -> lambd (default = 0.95 ) Regularization parameter (lambda) used when calculating the Generalized Advantage Estimate ( GAE ). This can be thought of as how much the agent relies on its current value estimate when calculating an updated value estimate. Low values correspond to relying more on the current value estimate (which can be high bias), and high values correspond to relying more on the actual rewards received in the environment (which can be high variance). The parameter provides a trade-off between the two, and the right value can lead to a more stable training process. Typical range: 0.9 - 0.95 hyperparameters -> num_epoch (default = 3 ) Number of passes to make through the experience buffer when performing gradient descent optimization.The larger the batch_size, the larger it is acceptable to make this. Decreasing this will ensure more stable updates, at the cost of slower learning. Typical range: 3 - 10","title":"PPO-specific Configurations"},{"location":"Training-Configuration-File/#sac-specific-configurations","text":"Setting Description hyperparameters -> buffer_init_steps (default = 0 ) Number of experiences to collect into the buffer before updating the policy model. As the untrained policy is fairly random, pre-filling the buffer with random actions is useful for exploration. Typically, at least several episodes of experiences should be pre-filled. Typical range: 1000 - 10000 hyperparameters -> init_entcoef (default = 1.0 ) How much the agent should explore in the beginning of training. Corresponds to the initial entropy coefficient set at the beginning of training. In SAC, the agent is incentivized to make its actions entropic to facilitate better exploration. The entropy coefficient weighs the true reward with a bonus entropy reward. The entropy coefficient is automatically adjusted to a preset target entropy, so the init_entcoef only corresponds to the starting value of the entropy bonus. Increase init_entcoef to explore more in the beginning, decrease to converge to a solution faster. Typical range: (Continuous): 0.5 - 1.0 ; (Discrete): 0.05 - 0.5 hyperparameters -> save_replay_buffer (default = false ) Whether to save and load the experience replay buffer as well as the model when quitting and re-starting training. This may help resumes go more smoothly, as the experiences collected won't be wiped. Note that replay buffers can be very large, and will take up a considerable amount of disk space. For that reason, we disable this feature by default. hyperparameters -> tau (default = 0.005 ) How aggressively to update the target network used for bootstrapping value estimation in SAC. Corresponds to the magnitude of the target Q update during the SAC model update. In SAC, there are two neural networks: the target and the policy. The target network is used to bootstrap the policy's estimate of the future rewards at a given state, and is fixed while the policy is being updated. This target is then slowly updated according to tau. Typically, this value should be left at 0.005. For simple problems, increasing tau to 0.01 might reduce the time it takes to learn, at the cost of stability. Typical range: 0.005 - 0.01 hyperparameters -> steps_per_update (default = 1 ) Average ratio of agent steps (actions) taken to updates made of the agent's policy. In SAC, a single \"update\" corresponds to grabbing a batch of size batch_size from the experience replay buffer, and using this mini batch to update the models. Note that it is not guaranteed that after exactly steps_per_update steps an update will be made, only that the ratio will hold true over many steps. Typically, steps_per_update should be greater than or equal to 1. Note that setting steps_per_update lower will improve sample efficiency (reduce the number of steps required to train) but increase the CPU time spent performing updates. For most environments where steps are fairly fast (e.g. our example environments) steps_per_update equal to the number of agents in the scene is a good balance. For slow environments (steps take 0.1 seconds or more) reducing steps_per_update may improve training speed. We can also change steps_per_update to lower than 1 to update more often than once per step, though this will usually result in a slowdown unless the environment is very slow. Typical range: 1 - 20 hyperparameters -> reward_signal_num_update (default = steps_per_update ) Number of steps per mini batch sampled and used for updating the reward signals. By default, we update the reward signals once every time the main policy is updated. However, to imitate the training procedure in certain imitation learning papers (e.g. Kostrikov et. al , Blond\u00e9 et. al ), we may want to update the reward signal (GAIL) M times for every update of the policy. We can change steps_per_update of SAC to N, as well as reward_signal_steps_per_update under reward_signals to N / M to accomplish this. By default, reward_signal_steps_per_update is set to steps_per_update .","title":"SAC-specific Configurations"},{"location":"Training-Configuration-File/#ma-poca-specific-configurations","text":"MA-POCA uses the same configurations as PPO, and there are no additional POCA-specific parameters. NOTE : Reward signals other than Extrinsic Rewards have not been extensively tested with MA-POCA, though they can still be added and used for training on a your-mileage-may-vary basis.","title":"MA-POCA-specific Configurations"},{"location":"Training-Configuration-File/#reward-signals","text":"The reward_signals section enables the specification of settings for both extrinsic (i.e. environment-based) and intrinsic reward signals (e.g. curiosity and GAIL). Each reward signal should define at least two parameters, strength and gamma , in addition to any class-specific hyperparameters. Note that to remove a reward signal, you should delete its entry entirely from reward_signals . At least one reward signal should be left defined at all times. Provide the following configurations to design the reward signal for your training run.","title":"Reward Signals"},{"location":"Training-Configuration-File/#extrinsic-rewards","text":"Enable these settings to ensure that your training run incorporates your environment-based reward signal: Setting Description extrinsic -> strength (default = 1.0 ) Factor by which to multiply the reward given by the environment. Typical ranges will vary depending on the reward signal. Typical range: 1.00 extrinsic -> gamma (default = 0.99 ) Discount factor for future rewards coming from the environment. This can be thought of as how far into the future the agent should care about possible rewards. In situations when the agent should be acting in the present in order to prepare for rewards in the distant future, this value should be large. In cases when rewards are more immediate, it can be smaller. Must be strictly smaller than 1. Typical range: 0.8 - 0.995","title":"Extrinsic Rewards"},{"location":"Training-Configuration-File/#curiosity-intrinsic-reward","text":"To enable curiosity, provide these settings: Setting Description curiosity -> strength (default = 1.0 ) Magnitude of the curiosity reward generated by the intrinsic curiosity module. This should be scaled in order to ensure it is large enough to not be overwhelmed by extrinsic reward signals in the environment. Likewise it should not be too large to overwhelm the extrinsic reward signal. Typical range: 0.001 - 0.1 curiosity -> gamma (default = 0.99 ) Discount factor for future rewards. Typical range: 0.8 - 0.995 curiosity -> network_settings Please see the documentation for network_settings under Common Trainer Configurations . The network specs used by the intrinsic curiosity model. The value should of hidden_units should be small enough to encourage the ICM to compress the original observation, but also not too small to prevent it from learning to differentiate between expected and actual observations. Typical range: 64 - 256 curiosity -> learning_rate (default = 3e-4 ) Learning rate used to update the intrinsic curiosity module. This should typically be decreased if training is unstable, and the curiosity loss is unstable. Typical range: 1e-5 - 1e-3","title":"Curiosity Intrinsic Reward"},{"location":"Training-Configuration-File/#gail-intrinsic-reward","text":"To enable GAIL (assuming you have recorded demonstrations), provide these settings: Setting Description gail -> strength (default = 1.0 ) Factor by which to multiply the raw reward. Note that when using GAIL with an Extrinsic Signal, this value should be set lower if your demonstrations are suboptimal (e.g. from a human), so that a trained agent will focus on receiving extrinsic rewards instead of exactly copying the demonstrations. Keep the strength below about 0.1 in those cases. Typical range: 0.01 - 1.0 gail -> gamma (default = 0.99 ) Discount factor for future rewards. Typical range: 0.8 - 0.9 gail -> demo_path (Required, no default) The path to your .demo file or directory of .demo files. gail -> network_settings Please see the documentation for network_settings under Common Trainer Configurations . The network specs for the GAIL discriminator. The value of hidden_units should be small enough to encourage the discriminator to compress the original observation, but also not too small to prevent it from learning to differentiate between demonstrated and actual behavior. Dramatically increasing this size will also negatively affect training times. Typical range: 64 - 256 gail -> learning_rate (Optional, default = 3e-4 ) Learning rate used to update the discriminator. This should typically be decreased if training is unstable, and the GAIL loss is unstable. Typical range: 1e-5 - 1e-3 gail -> use_actions (default = false ) Determines whether the discriminator should discriminate based on both observations and actions, or just observations. Set to True if you want the agent to mimic the actions from the demonstrations, and False if you'd rather have the agent visit the same states as in the demonstrations but with possibly different actions. Setting to False is more likely to be stable, especially with imperfect demonstrations, but may learn slower. gail -> use_vail (default = false ) Enables a variational bottleneck within the GAIL discriminator. This forces the discriminator to learn a more general representation and reduces its tendency to be \"too good\" at discriminating, making learning more stable. However, it does increase training time. Enable this if you notice your imitation learning is unstable, or unable to learn the task at hand.","title":"GAIL Intrinsic Reward"},{"location":"Training-Configuration-File/#rnd-intrinsic-reward","text":"Random Network Distillation (RND) is only available for the PyTorch trainers. To enable RND, provide these settings: Setting Description rnd -> strength (default = 1.0 ) Magnitude of the curiosity reward generated by the intrinsic rnd module. This should be scaled in order to ensure it is large enough to not be overwhelmed by extrinsic reward signals in the environment. Likewise it should not be too large to overwhelm the extrinsic reward signal. Typical range: 0.001 - 0.01 rnd -> gamma (default = 0.99 ) Discount factor for future rewards. Typical range: 0.8 - 0.995 rnd -> network_settings Please see the documentation for network_settings under Common Trainer Configurations . The network specs for the RND model. curiosity -> learning_rate (default = 3e-4 ) Learning rate used to update the RND module. This should be large enough for the RND module to quickly learn the state representation, but small enough to allow for stable learning. Typical range: 1e-5 - 1e-3","title":"RND Intrinsic Reward"},{"location":"Training-Configuration-File/#behavioral-cloning","text":"To enable Behavioral Cloning as a pre-training option (assuming you have recorded demonstrations), provide the following configurations under the behavior_cloning section: Setting Description demo_path (Required, no default) The path to your .demo file or directory of .demo files. strength (default = 1.0 ) Learning rate of the imitation relative to the learning rate of PPO, and roughly corresponds to how strongly we allow BC to influence the policy. Typical range: 0.1 - 0.5 steps (default = 0 ) During BC, it is often desirable to stop using demonstrations after the agent has \"seen\" rewards, and allow it to optimize past the available demonstrations and/or generalize outside of the provided demonstrations. steps corresponds to the training steps over which BC is active. The learning rate of BC will anneal over the steps. Set the steps to 0 for constant imitation over the entire training run. batch_size (default = batch_size of trainer) Number of demonstration experiences used for one iteration of a gradient descent update. If not specified, it will default to the batch_size of the trainer. Typical range: (Continuous): 512 - 5120 ; (Discrete): 32 - 512 num_epoch (default = num_epoch of trainer) Number of passes through the experience buffer during gradient descent. If not specified, it will default to the number of epochs set for PPO. Typical range: 3 - 10 samples_per_update (default = 0 ) Maximum number of samples to use during each imitation update. You may want to lower this if your demonstration dataset is very large to avoid overfitting the policy on demonstrations. Set to 0 to train over all of the demonstrations at each update step. Typical range: buffer_size","title":"Behavioral Cloning"},{"location":"Training-Configuration-File/#memory-enhanced-agents-using-recurrent-neural-networks","text":"You can enable your agents to use memory by adding a memory section under network_settings , and setting memory_size and sequence_length : Setting Description network_settings -> memory -> memory_size (default = 128 ) Size of the memory an agent must keep. In order to use a LSTM, training requires a sequence of experiences instead of single experiences. Corresponds to the size of the array of floating point numbers used to store the hidden state of the recurrent neural network of the policy. This value must be a multiple of 2, and should scale with the amount of information you expect the agent will need to remember in order to successfully complete the task. Typical range: 32 - 256 network_settings -> memory -> sequence_length (default = 64 ) Defines how long the sequences of experiences must be while training. Note that if this number is too small, the agent will not be able to remember things over longer periods of time. If this number is too large, the neural network will take longer to train. Typical range: 4 - 128 A few considerations when deciding to use memory: LSTM does not work well with continuous actions. Please use discrete actions for better results. Adding a recurrent layer increases the complexity of the neural network, it is recommended to decrease num_layers when using recurrent. It is required that memory_size be divisible by 2.","title":"Memory-enhanced Agents using Recurrent Neural Networks"},{"location":"Training-Configuration-File/#self-play","text":"Training with self-play adds additional confounding factors to the usual issues faced by reinforcement learning. In general, the tradeoff is between the skill level and generality of the final policy and the stability of learning. Training against a set of slowly or unchanging adversaries with low diversity results in a more stable learning process than training against a set of quickly changing adversaries with high diversity. With this context, this guide discusses the exposed self-play hyperparameters and intuitions for tuning them. If your environment contains multiple agents that are divided into teams, you can leverage our self-play training option by providing these configurations for each Behavior: Setting Description save_steps (default = 20000 ) Number of trainer steps between snapshots. For example, if save_steps=10000 then a snapshot of the current policy will be saved every 10000 trainer steps. Note, trainer steps are counted per agent. For more information, please see the migration doc after v0.13. A larger value of save_steps will yield a set of opponents that cover a wider range of skill levels and possibly play styles since the policy receives more training. As a result, the agent trains against a wider variety of opponents. Learning a policy to defeat more diverse opponents is a harder problem and so may require more overall training steps but also may lead to more general and robust policy at the end of training. This value is also dependent on how intrinsically difficult the environment is for the agent. Typical range: 10000 - 100000 team_change (default = 5 * save_steps ) Number of trainer_steps between switching the learning team. This is the number of trainer steps the teams associated with a specific ghost trainer will train before a different team becomes the new learning team. It is possible that, in asymmetric games, opposing teams require fewer trainer steps to make similar performance gains. This enables users to train a more complicated team of agents for more trainer steps than a simpler team of agents per team switch. A larger value of team-change will allow the agent to train longer against it's opponents. The longer an agent trains against the same set of opponents the more able it will be to defeat them. However, training against them for too long may result in overfitting to the particular opponent strategies and so the agent may fail against the next batch of opponents. The value of team-change will determine how many snapshots of the agent's policy are saved to be used as opponents for the other team. So, we recommend setting this value as a function of the save_steps parameter discussed previously. Typical range: 4x-10x where x= save_steps swap_steps (default = 10000 ) Number of ghost steps (not trainer steps) between swapping the opponents policy with a different snapshot. A 'ghost step' refers to a step taken by an agent that is following a fixed policy and not learning . The reason for this distinction is that in asymmetric games, we may have teams with an unequal number of agents e.g. a 2v1 scenario like our Strikers Vs Goalie example environment. The team with two agents collects twice as many agent steps per environment step as the team with one agent. Thus, these two values will need to be distinct to ensure that the same number of trainer steps corresponds to the same number of opponent swaps for each team. The formula for swap_steps if a user desires x swaps of a team with num_agents agents against an opponent team with num_opponent_agents agents during team-change total steps is: (num_agents / num_opponent_agents) * (team_change / x) Typical range: 10000 - 100000 play_against_latest_model_ratio (default = 0.5 ) Probability an agent will play against the latest opponent policy. With probability 1 - play_against_latest_model_ratio , the agent will play against a snapshot of its opponent from a past iteration. A larger value of play_against_latest_model_ratio indicates that an agent will be playing against the current opponent more often. Since the agent is updating it's policy, the opponent will be different from iteration to iteration. This can lead to an unstable learning environment, but poses the agent with an auto-curricula of more increasingly challenging situations which may lead to a stronger final policy. Typical range: 0.0 - 1.0 window (default = 10 ) Size of the sliding window of past snapshots from which the agent's opponents are sampled. For example, a window size of 5 will save the last 5 snapshots taken. Each time a new snapshot is taken, the oldest is discarded. A larger value of window means that an agent's pool of opponents will contain a larger diversity of behaviors since it will contain policies from earlier in the training run. Like in the save_steps hyperparameter, the agent trains against a wider variety of opponents. Learning a policy to defeat more diverse opponents is a harder problem and so may require more overall training steps but also may lead to more general and robust policy at the end of training. Typical range: 5 - 30","title":"Self-Play"},{"location":"Training-Configuration-File/#note-on-reward-signals","text":"We make the assumption that the final reward in a trajectory corresponds to the outcome of an episode. A final reward of +1 indicates winning, -1 indicates losing and 0 indicates a draw. The ELO calculation (discussed below) depends on this final reward being either +1, 0, -1. The reward signal should still be used as described in the documentation for the other trainers. However, we encourage users to be a bit more conservative when shaping reward functions due to the instability and non-stationarity of learning in adversarial games. Specifically, we encourage users to begin with the simplest possible reward function (+1 winning, -1 losing) and to allow for more iterations of training to compensate for the sparsity of reward.","title":"Note on Reward Signals"},{"location":"Training-Configuration-File/#note-on-swap-steps","text":"As an example, in a 2v1 scenario, if we want the swap to occur x=4 times during team-change=200000 steps, the swap_steps for the team of one agent is: swap_steps = (1 / 2) * (200000 / 4) = 25000 The swap_steps for the team of two agents is: swap_steps = (2 / 1) * (200000 / 4) = 100000 Note, with equal team sizes, the first term is equal to 1 and swap_steps can be calculated by just dividing the total steps by the desired number of swaps. A larger value of swap_steps means that an agent will play against the same fixed opponent for a longer number of training iterations. This results in a more stable training scenario, but leaves the agent open to the risk of overfitting it's behavior for this particular opponent. Thus, when a new opponent is swapped, the agent may lose more often than expected.","title":"Note on Swap Steps"},{"location":"Training-ML-Agents/","text":"Training ML-Agents Table of Contents Training ML-Agents Training with mlagents-learn Starting Training Observing Training Stopping and Resuming Training Loading an Existing Model Training Configurations Adding CLI Arguments to the Training Configuration file Environment settings Engine settings Checkpoint settings Torch settings: Behavior Configurations Default Behavior Settings Environment Parameters Environment Parameter Randomization Supported Sampler Types Training with Environment Parameter Randomization Curriculum Training with a Curriculum Training Using Concurrent Unity Instances For a broad overview of reinforcement learning, imitation learning and all the training scenarios, methods and options within the ML-Agents Toolkit, see ML-Agents Toolkit Overview . Once your learning environment has been created and is ready for training, the next step is to initiate a training run. Training in the ML-Agents Toolkit is powered by a dedicated Python package, mlagents . This package exposes a command mlagents-learn that is the single entry point for all training workflows (e.g. reinforcement leaning, imitation learning, curriculum learning). Its implementation can be found at ml-agents/mlagents/trainers/learn.py . Training with mlagents-learn Starting Training mlagents-learn is the main training utility provided by the ML-Agents Toolkit. It accepts a number of CLI options in addition to a YAML configuration file that contains all the configurations and hyperparameters to be used during training. The set of configurations and hyperparameters to include in this file depend on the agents in your environment and the specific training method you wish to utilize. Keep in mind that the hyperparameter values can have a big impact on the training performance (i.e. your agent's ability to learn a policy that solves the task). In this page, we will review all the hyperparameters for all training methods and provide guidelines and advice on their values. To view a description of all the CLI options accepted by mlagents-learn , use the --help : mlagents-learn --help The basic command for training is: mlagents-learn <trainer-config-file> --env=<env_name> --run-id=<run-identifier> where <trainer-config-file> is the file path of the trainer configuration YAML. This contains all the hyperparameter values. We offer a detailed guide on the structure of this file and the meaning of the hyperparameters (and advice on how to set them) in the dedicated Training Configurations section below. <env_name> (Optional) is the name (including path) of your Unity executable containing the agents to be trained. If <env_name> is not passed, the training will happen in the Editor. Press the Play button in Unity when the message \"Start training by pressing the Play button in the Unity Editor\" is displayed on the screen. <run-identifier> is a unique name you can use to identify the results of your training runs. See the Getting Started Guide for a sample execution of the mlagents-learn command. Observing Training Regardless of which training methods, configurations or hyperparameters you provide, the training process will always generate three artifacts, all found in the results/<run-identifier> folder: Summaries: these are training metrics that are updated throughout the training process. They are helpful to monitor your training performance and may help inform how to update your hyperparameter values. See Using TensorBoard for more details on how to visualize the training metrics. Models: these contain the model checkpoints that are updated throughout training and the final model file ( .onnx ). This final model file is generated once either when training completes or is interrupted. Timers file (under results/<run-identifier>/run_logs ): this contains aggregated metrics on your training process, including time spent on specific code blocks. See Profiling in Python for more information on the timers generated. These artifacts are updated throughout the training process and finalized when training is completed or is interrupted. Stopping and Resuming Training To interrupt training and save the current progress, hit Ctrl+C once and wait for the model(s) to be saved out. To resume a previously interrupted or completed training run, use the --resume flag and make sure to specify the previously used run ID. If you would like to re-run a previously interrupted or completed training run and re-use the same run ID (in this case, overwriting the previously generated artifacts), then use the --force flag. Loading an Existing Model You can also use this mode to run inference of an already-trained model in Python by using both the --resume and --inference flags. Note that if you want to run inference in Unity, you should use the Unity Inference Engine . Additionally, if the network architecture changes, you may still load an existing model, but ML-Agents will only load the parts of the model it can load and ignore all others. For instance, if you add a new reward signal, the existing model will load but the new reward signal will be initialized from scratch. If you have a model with a visual encoder (CNN) but change the hidden_units , the CNN will be loaded but the body of the network will be initialized from scratch. Alternatively, you might want to start a new training run but initialize it using an already-trained model. You may want to do this, for instance, if your environment changed and you want a new model, but the old behavior is still better than random. You can do this by specifying --initialize-from=<run-identifier> , where <run-identifier> is the old run ID. Training Configurations The Unity ML-Agents Toolkit provides a wide range of training scenarios, methods and options. As such, specific training runs may require different training configurations and may generate different artifacts and TensorBoard statistics. This section offers a detailed guide into how to manage the different training set-ups withing the toolkit. More specifically, this section offers a detailed guide on the command-line flags for mlagents-learn that control the training configurations: <trainer-config-file> : defines the training hyperparameters for each Behavior in the scene, and the set-ups for the environment parameters (Curriculum Learning and Environment Parameter Randomization) It is important to highlight that successfully training a Behavior in the ML-Agents Toolkit involves tuning the training hyperparameters and configuration. This guide contains some best practices for tuning the training process when the default parameters don't seem to be giving the level of performance you would like. We provide sample configuration files for our example environments in the config/ directory. The config/ppo/3DBall.yaml was used to train the 3D Balance Ball in the Getting Started guide. That configuration file uses the PPO trainer, but we also have configuration files for SAC and GAIL. Additionally, the set of configurations you provide depend on the training functionalities you use (see ML-Agents Toolkit Overview for a description of all the training functionalities). Each functionality you add typically has its own training configurations. For instance: Use PPO or SAC? Use Recurrent Neural Networks for adding memory to your agents? Use the intrinsic curiosity module? Ignore the environment reward signal? Pre-train using behavioral cloning? (Assuming you have recorded demonstrations.) Include the GAIL intrinsic reward signals? (Assuming you have recorded demonstrations.) Use self-play? (Assuming your environment includes multiple agents.) The trainer config file, <trainer-config-file> , determines the features you will use during training, and the answers to the above questions will dictate its contents. The rest of this guide breaks down the different sub-sections of the trainer config file and explains the possible settings for each. If you need a list of all the trainer configurations, please see Training Configuration File . NOTE: The configuration file format has been changed between 0.17.0 and 0.18.0 and between 0.18.0 and onwards. To convert an old set of configuration files (trainer config, curriculum, and sampler files) to the new format, a script has been provided. Run python -m mlagents.trainers.upgrade_config -h in your console to see the script's usage. Adding CLI Arguments to the Training Configuration file Additionally, within the training configuration YAML file, you can also add the CLI arguments (such as --num-envs ). Reminder that a detailed description of all the CLI arguments can be found by using the help utility: mlagents-learn --help These additional CLI arguments are grouped into environment, engine, checkpoint and torch. The available settings and example values are shown below. Environment settings env_settings: env_path: FoodCollector env_args: null base_port: 5005 num_envs: 1 seed: -1 max_lifetime_restarts: 10 restarts_rate_limit_n: 1 restarts_rate_limit_period_s: 60 Engine settings engine_settings: width: 84 height: 84 quality_level: 5 time_scale: 20 target_frame_rate: -1 capture_frame_rate: 60 no_graphics: false Checkpoint settings checkpoint_settings: run_id: foodtorch initialize_from: null load_model: false resume: false force: true train_model: false inference: false Torch settings: torch_settings: device: cpu Behavior Configurations The primary section of the trainer config file is a set of configurations for each Behavior in your scene. These are defined under the sub-section behaviors in your trainer config file. Some of the configurations are required while others are optional. To help us get started, below is a sample file that includes all the possible settings if we're using a PPO trainer with all the possible training functionalities enabled (memory, behavioral cloning, curiosity, GAIL and self-play). You will notice that curriculum and environment parameter randomization settings are not part of the behaviors configuration, but in their own section called environment_parameters . behaviors: BehaviorPPO: trainer_type: ppo hyperparameters: # Hyperparameters common to PPO and SAC batch_size: 1024 buffer_size: 10240 learning_rate: 3.0e-4 learning_rate_schedule: linear # PPO-specific hyperparameters # Replaces the \"PPO-specific hyperparameters\" section above beta: 5.0e-3 beta_schedule: constant epsilon: 0.2 epsilon_schedule: linear lambd: 0.95 num_epoch: 3 # Configuration of the neural network (common to PPO/SAC) network_settings: vis_encode_type: simple normalize: false hidden_units: 128 num_layers: 2 # memory memory: sequence_length: 64 memory_size: 256 # Trainer configurations common to all trainers max_steps: 5.0e5 time_horizon: 64 summary_freq: 10000 keep_checkpoints: 5 checkpoint_interval: 50000 threaded: false init_path: null # behavior cloning behavioral_cloning: demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo strength: 0.5 steps: 150000 batch_size: 512 num_epoch: 3 samples_per_update: 0 reward_signals: # environment reward (default) extrinsic: strength: 1.0 gamma: 0.99 # curiosity module curiosity: strength: 0.02 gamma: 0.99 encoding_size: 256 learning_rate: 3.0e-4 # GAIL gail: strength: 0.01 gamma: 0.99 encoding_size: 128 demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo learning_rate: 3.0e-4 use_actions: false use_vail: false # self-play self_play: window: 10 play_against_latest_model_ratio: 0.5 save_steps: 50000 swap_steps: 2000 team_change: 100000 Here is an equivalent file if we use an SAC trainer instead. Notice that the configurations for the additional functionalities (memory, behavioral cloning, curiosity and self-play) remain unchanged. behaviors: BehaviorSAC: trainer_type: sac # Trainer configs common to PPO/SAC (excluding reward signals) # same as PPO config # SAC-specific configs (replaces the hyperparameters section above) hyperparameters: # Hyperparameters common to PPO and SAC # Same as PPO config # SAC-specific hyperparameters # Replaces the \"PPO-specific hyperparameters\" section above buffer_init_steps: 0 tau: 0.005 steps_per_update: 10.0 save_replay_buffer: false init_entcoef: 0.5 reward_signal_steps_per_update: 10.0 # Configuration of the neural network (common to PPO/SAC) network_settings: # Same as PPO config # Trainer configurations common to all trainers # <Same as PPO config> # pre-training using behavior cloning behavioral_cloning: # same as PPO config reward_signals: # environment reward extrinsic: # same as PPO config # curiosity module curiosity: # same as PPO config # GAIL gail: # same as PPO config # self-play self_play: # same as PPO config We now break apart the components of the configuration file and describe what each of these parameters mean and provide guidelines on how to set them. See Training Configuration File for a detailed description of all the configurations listed above, along with their defaults. Unless otherwise specified, omitting a configuration will revert it to its default. Default Behavior Settings In some cases, you may want to specify a set of default configurations for your Behaviors. This may be useful, for instance, if your Behavior names are generated procedurally by the environment and not known before runtime, or if you have many Behaviors with very similar settings. To specify a default configuraton, insert a default_settings section in your YAML. This section should be formatted exactly like a configuration for a Behavior. default_settings: # < Same as Behavior configuration > behaviors: # < Same as above > Behaviors found in the environment that aren't specified in the YAML will now use the default_settings , and unspecified settings in behavior configurations will default to the values in default_settings if specified there. Environment Parameters In order to control the EnvironmentParameters in the Unity simulation during training, you need to add a section called environment_parameters . For example you can set the value of an EnvironmentParameter called my_environment_parameter to 3.0 with the following code : behaviors: BehaviorY: # < Same as above > # Add this section environment_parameters: my_environment_parameter: 3.0 Inside the Unity simulation, you can access your Environment Parameters by doing : Academy.Instance.EnvironmentParameters.GetWithDefault(\"my_environment_parameter\", 0.0f); Environment Parameter Randomization To enable environment parameter randomization, you need to edit the environment_parameters section of your training configuration yaml file. Instead of providing a single float value for your environment parameter, you can specify a sampler instead. Here is an example with three environment parameters called mass , length and scale : behaviors: BehaviorY: # < Same as above > # Add this section environment_parameters: mass: sampler_type: uniform sampler_parameters: min_value: 0.5 max_value: 10 length: sampler_type: multirangeuniform sampler_parameters: intervals: [[7, 10], [15, 20]] scale: sampler_type: gaussian sampler_parameters: mean: 2 st_dev: .3 Setting Description sampler_type A string identifier for the type of sampler to use for this Environment Parameter . sampler_parameters The parameters for a given sampler_type . Samplers of different types can have different sampler_parameters Supported Sampler Types Below is a list of the sampler_type values supported by the toolkit. uniform - Uniform sampler Uniformly samples a single float value from a range with a given minimum and maximum value (inclusive). parameters - min_value , max_value gaussian - Gaussian sampler Samples a single float value from a normal distribution with a given mean and standard deviation. parameters - mean , st_dev multirange_uniform - Multirange uniform sampler First, samples an interval from a set of intervals in proportion to relative length of the intervals. Then, uniformly samples a single float value from the sampled interval (inclusive). This sampler can take an arbitrary number of intervals in a list in the following format: [[ interval_1_min , interval_1_max ], [ interval_2_min , interval_2_max ], ...] parameters - intervals The implementation of the samplers can be found in the Samplers.cs file . Training with Environment Parameter Randomization After the sampler configuration is defined, we proceed by launching mlagents-learn and specify trainer configuration with parameter randomization enabled. For example, if we wanted to train the 3D ball agent with parameter randomization, we would run mlagents-learn config/ppo/3DBall_randomize.yaml --run-id=3D-Ball-randomize We can observe progress and metrics via TensorBoard. Curriculum To enable curriculum learning, you need to add a curriculum sub-section to your environment parameter. Here is one example with the environment parameter my_environment_parameter : behaviors: BehaviorY: # < Same as above > # Add this section environment_parameters: my_environment_parameter: curriculum: - name: MyFirstLesson # The '-' is important as this is a list completion_criteria: measure: progress behavior: my_behavior signal_smoothing: true min_lesson_length: 100 threshold: 0.2 value: 0.0 - name: MySecondLesson # This is the start of the second lesson completion_criteria: measure: progress behavior: my_behavior signal_smoothing: true min_lesson_length: 100 threshold: 0.6 require_reset: true value: sampler_type: uniform sampler_parameters: min_value: 4.0 max_value: 7.0 - name: MyLastLesson value: 8.0 Note that this curriculum only applies to my_environment_parameter . The curriculum section contains a list of Lessons . In the example, the lessons are named MyFirstLesson , MySecondLesson and MyLastLesson . Each Lesson has 3 fields : name which is a user defined name for the lesson (The name of the lesson will be displayed in the console when the lesson changes) completion_criteria which determines what needs to happen in the simulation before the lesson can be considered complete. When that condition is met, the curriculum moves on to the next Lesson . Note that you do not need to specify a completion_criteria for the last Lesson value which is the value the environment parameter will take during the lesson. Note that this can be a float or a sampler. There are the different settings of the completion_criteria : Setting Description measure What to measure learning progress, and advancement in lessons by. reward uses a measure of received reward, progress uses the ratio of steps/max_steps, while Elo is available only for self-play situations and uses Elo score as a curriculum completion measure. behavior Specifies which behavior is being tracked. There can be multiple behaviors with different names, each at different points of training. This setting allows the curriculum to track only one of them. threshold Determines at what point in value of measure the lesson should be increased. min_lesson_length The minimum number of episodes that should be completed before the lesson can change. If measure is set to reward , the average cumulative reward of the last min_lesson_length episodes will be used to determine if the lesson should change. Must be nonnegative. Important : the average reward that is compared to the thresholds is different than the mean reward that is logged to the console. For example, if min_lesson_length is 100 , the lesson will increment after the average cumulative reward of the last 100 episodes exceeds the current threshold. The mean reward logged to the console is dictated by the summary_freq parameter defined above. signal_smoothing Whether to weight the current progress measure by previous values. require_reset Whether changing lesson requires the environment to reset (default: false) ##### Training with a Curriculum Once we have specified our metacurriculum and curricula, we can launch mlagents-learn to point to the config file containing our curricula and PPO will train using Curriculum Learning. For example, to train agents in the Wall Jump environment with curriculum learning, we can run: mlagents-learn config/ppo/WallJump_curriculum.yaml --run-id=wall-jump-curriculum We can then keep track of the current lessons and progresses via TensorBoard. If you've terminated the run, you can resume it using --resume and lesson progress will start off where it ended. Training Using Concurrent Unity Instances In order to run concurrent Unity instances during training, set the number of environment instances using the command line option --num-envs=<n> when you invoke mlagents-learn . Optionally, you can also set the --base-port , which is the starting port used for the concurrent Unity instances. Some considerations: Buffer Size - If you are having trouble getting an agent to train, even with multiple concurrent Unity instances, you could increase buffer_size in the trainer config file. A common practice is to multiply buffer_size by num-envs . Resource Constraints - Invoking concurrent Unity instances is constrained by the resources on the machine. Please use discretion when setting --num-envs=<n> . Result Variation Using Concurrent Unity Instances - If you keep all the hyperparameters the same, but change --num-envs=<n> , the results and model would likely change.","title":"Training ML-Agents"},{"location":"Training-ML-Agents/#training-ml-agents","text":"Table of Contents Training ML-Agents Training with mlagents-learn Starting Training Observing Training Stopping and Resuming Training Loading an Existing Model Training Configurations Adding CLI Arguments to the Training Configuration file Environment settings Engine settings Checkpoint settings Torch settings: Behavior Configurations Default Behavior Settings Environment Parameters Environment Parameter Randomization Supported Sampler Types Training with Environment Parameter Randomization Curriculum Training with a Curriculum Training Using Concurrent Unity Instances For a broad overview of reinforcement learning, imitation learning and all the training scenarios, methods and options within the ML-Agents Toolkit, see ML-Agents Toolkit Overview . Once your learning environment has been created and is ready for training, the next step is to initiate a training run. Training in the ML-Agents Toolkit is powered by a dedicated Python package, mlagents . This package exposes a command mlagents-learn that is the single entry point for all training workflows (e.g. reinforcement leaning, imitation learning, curriculum learning). Its implementation can be found at ml-agents/mlagents/trainers/learn.py .","title":"Training ML-Agents"},{"location":"Training-ML-Agents/#training-with-mlagents-learn","text":"","title":"Training with mlagents-learn"},{"location":"Training-ML-Agents/#starting-training","text":"mlagents-learn is the main training utility provided by the ML-Agents Toolkit. It accepts a number of CLI options in addition to a YAML configuration file that contains all the configurations and hyperparameters to be used during training. The set of configurations and hyperparameters to include in this file depend on the agents in your environment and the specific training method you wish to utilize. Keep in mind that the hyperparameter values can have a big impact on the training performance (i.e. your agent's ability to learn a policy that solves the task). In this page, we will review all the hyperparameters for all training methods and provide guidelines and advice on their values. To view a description of all the CLI options accepted by mlagents-learn , use the --help : mlagents-learn --help The basic command for training is: mlagents-learn <trainer-config-file> --env=<env_name> --run-id=<run-identifier> where <trainer-config-file> is the file path of the trainer configuration YAML. This contains all the hyperparameter values. We offer a detailed guide on the structure of this file and the meaning of the hyperparameters (and advice on how to set them) in the dedicated Training Configurations section below. <env_name> (Optional) is the name (including path) of your Unity executable containing the agents to be trained. If <env_name> is not passed, the training will happen in the Editor. Press the Play button in Unity when the message \"Start training by pressing the Play button in the Unity Editor\" is displayed on the screen. <run-identifier> is a unique name you can use to identify the results of your training runs. See the Getting Started Guide for a sample execution of the mlagents-learn command.","title":"Starting Training"},{"location":"Training-ML-Agents/#observing-training","text":"Regardless of which training methods, configurations or hyperparameters you provide, the training process will always generate three artifacts, all found in the results/<run-identifier> folder: Summaries: these are training metrics that are updated throughout the training process. They are helpful to monitor your training performance and may help inform how to update your hyperparameter values. See Using TensorBoard for more details on how to visualize the training metrics. Models: these contain the model checkpoints that are updated throughout training and the final model file ( .onnx ). This final model file is generated once either when training completes or is interrupted. Timers file (under results/<run-identifier>/run_logs ): this contains aggregated metrics on your training process, including time spent on specific code blocks. See Profiling in Python for more information on the timers generated. These artifacts are updated throughout the training process and finalized when training is completed or is interrupted.","title":"Observing Training"},{"location":"Training-ML-Agents/#stopping-and-resuming-training","text":"To interrupt training and save the current progress, hit Ctrl+C once and wait for the model(s) to be saved out. To resume a previously interrupted or completed training run, use the --resume flag and make sure to specify the previously used run ID. If you would like to re-run a previously interrupted or completed training run and re-use the same run ID (in this case, overwriting the previously generated artifacts), then use the --force flag.","title":"Stopping and Resuming Training"},{"location":"Training-ML-Agents/#loading-an-existing-model","text":"You can also use this mode to run inference of an already-trained model in Python by using both the --resume and --inference flags. Note that if you want to run inference in Unity, you should use the Unity Inference Engine . Additionally, if the network architecture changes, you may still load an existing model, but ML-Agents will only load the parts of the model it can load and ignore all others. For instance, if you add a new reward signal, the existing model will load but the new reward signal will be initialized from scratch. If you have a model with a visual encoder (CNN) but change the hidden_units , the CNN will be loaded but the body of the network will be initialized from scratch. Alternatively, you might want to start a new training run but initialize it using an already-trained model. You may want to do this, for instance, if your environment changed and you want a new model, but the old behavior is still better than random. You can do this by specifying --initialize-from=<run-identifier> , where <run-identifier> is the old run ID.","title":"Loading an Existing Model"},{"location":"Training-ML-Agents/#training-configurations","text":"The Unity ML-Agents Toolkit provides a wide range of training scenarios, methods and options. As such, specific training runs may require different training configurations and may generate different artifacts and TensorBoard statistics. This section offers a detailed guide into how to manage the different training set-ups withing the toolkit. More specifically, this section offers a detailed guide on the command-line flags for mlagents-learn that control the training configurations: <trainer-config-file> : defines the training hyperparameters for each Behavior in the scene, and the set-ups for the environment parameters (Curriculum Learning and Environment Parameter Randomization) It is important to highlight that successfully training a Behavior in the ML-Agents Toolkit involves tuning the training hyperparameters and configuration. This guide contains some best practices for tuning the training process when the default parameters don't seem to be giving the level of performance you would like. We provide sample configuration files for our example environments in the config/ directory. The config/ppo/3DBall.yaml was used to train the 3D Balance Ball in the Getting Started guide. That configuration file uses the PPO trainer, but we also have configuration files for SAC and GAIL. Additionally, the set of configurations you provide depend on the training functionalities you use (see ML-Agents Toolkit Overview for a description of all the training functionalities). Each functionality you add typically has its own training configurations. For instance: Use PPO or SAC? Use Recurrent Neural Networks for adding memory to your agents? Use the intrinsic curiosity module? Ignore the environment reward signal? Pre-train using behavioral cloning? (Assuming you have recorded demonstrations.) Include the GAIL intrinsic reward signals? (Assuming you have recorded demonstrations.) Use self-play? (Assuming your environment includes multiple agents.) The trainer config file, <trainer-config-file> , determines the features you will use during training, and the answers to the above questions will dictate its contents. The rest of this guide breaks down the different sub-sections of the trainer config file and explains the possible settings for each. If you need a list of all the trainer configurations, please see Training Configuration File . NOTE: The configuration file format has been changed between 0.17.0 and 0.18.0 and between 0.18.0 and onwards. To convert an old set of configuration files (trainer config, curriculum, and sampler files) to the new format, a script has been provided. Run python -m mlagents.trainers.upgrade_config -h in your console to see the script's usage.","title":"Training Configurations"},{"location":"Training-ML-Agents/#adding-cli-arguments-to-the-training-configuration-file","text":"Additionally, within the training configuration YAML file, you can also add the CLI arguments (such as --num-envs ). Reminder that a detailed description of all the CLI arguments can be found by using the help utility: mlagents-learn --help These additional CLI arguments are grouped into environment, engine, checkpoint and torch. The available settings and example values are shown below.","title":"Adding CLI Arguments to the Training Configuration file"},{"location":"Training-ML-Agents/#environment-settings","text":"env_settings: env_path: FoodCollector env_args: null base_port: 5005 num_envs: 1 seed: -1 max_lifetime_restarts: 10 restarts_rate_limit_n: 1 restarts_rate_limit_period_s: 60","title":"Environment settings"},{"location":"Training-ML-Agents/#engine-settings","text":"engine_settings: width: 84 height: 84 quality_level: 5 time_scale: 20 target_frame_rate: -1 capture_frame_rate: 60 no_graphics: false","title":"Engine settings"},{"location":"Training-ML-Agents/#checkpoint-settings","text":"checkpoint_settings: run_id: foodtorch initialize_from: null load_model: false resume: false force: true train_model: false inference: false","title":"Checkpoint settings"},{"location":"Training-ML-Agents/#torch-settings","text":"torch_settings: device: cpu","title":"Torch settings:"},{"location":"Training-ML-Agents/#behavior-configurations","text":"The primary section of the trainer config file is a set of configurations for each Behavior in your scene. These are defined under the sub-section behaviors in your trainer config file. Some of the configurations are required while others are optional. To help us get started, below is a sample file that includes all the possible settings if we're using a PPO trainer with all the possible training functionalities enabled (memory, behavioral cloning, curiosity, GAIL and self-play). You will notice that curriculum and environment parameter randomization settings are not part of the behaviors configuration, but in their own section called environment_parameters . behaviors: BehaviorPPO: trainer_type: ppo hyperparameters: # Hyperparameters common to PPO and SAC batch_size: 1024 buffer_size: 10240 learning_rate: 3.0e-4 learning_rate_schedule: linear # PPO-specific hyperparameters # Replaces the \"PPO-specific hyperparameters\" section above beta: 5.0e-3 beta_schedule: constant epsilon: 0.2 epsilon_schedule: linear lambd: 0.95 num_epoch: 3 # Configuration of the neural network (common to PPO/SAC) network_settings: vis_encode_type: simple normalize: false hidden_units: 128 num_layers: 2 # memory memory: sequence_length: 64 memory_size: 256 # Trainer configurations common to all trainers max_steps: 5.0e5 time_horizon: 64 summary_freq: 10000 keep_checkpoints: 5 checkpoint_interval: 50000 threaded: false init_path: null # behavior cloning behavioral_cloning: demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo strength: 0.5 steps: 150000 batch_size: 512 num_epoch: 3 samples_per_update: 0 reward_signals: # environment reward (default) extrinsic: strength: 1.0 gamma: 0.99 # curiosity module curiosity: strength: 0.02 gamma: 0.99 encoding_size: 256 learning_rate: 3.0e-4 # GAIL gail: strength: 0.01 gamma: 0.99 encoding_size: 128 demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo learning_rate: 3.0e-4 use_actions: false use_vail: false # self-play self_play: window: 10 play_against_latest_model_ratio: 0.5 save_steps: 50000 swap_steps: 2000 team_change: 100000 Here is an equivalent file if we use an SAC trainer instead. Notice that the configurations for the additional functionalities (memory, behavioral cloning, curiosity and self-play) remain unchanged. behaviors: BehaviorSAC: trainer_type: sac # Trainer configs common to PPO/SAC (excluding reward signals) # same as PPO config # SAC-specific configs (replaces the hyperparameters section above) hyperparameters: # Hyperparameters common to PPO and SAC # Same as PPO config # SAC-specific hyperparameters # Replaces the \"PPO-specific hyperparameters\" section above buffer_init_steps: 0 tau: 0.005 steps_per_update: 10.0 save_replay_buffer: false init_entcoef: 0.5 reward_signal_steps_per_update: 10.0 # Configuration of the neural network (common to PPO/SAC) network_settings: # Same as PPO config # Trainer configurations common to all trainers # <Same as PPO config> # pre-training using behavior cloning behavioral_cloning: # same as PPO config reward_signals: # environment reward extrinsic: # same as PPO config # curiosity module curiosity: # same as PPO config # GAIL gail: # same as PPO config # self-play self_play: # same as PPO config We now break apart the components of the configuration file and describe what each of these parameters mean and provide guidelines on how to set them. See Training Configuration File for a detailed description of all the configurations listed above, along with their defaults. Unless otherwise specified, omitting a configuration will revert it to its default.","title":"Behavior Configurations"},{"location":"Training-ML-Agents/#default-behavior-settings","text":"In some cases, you may want to specify a set of default configurations for your Behaviors. This may be useful, for instance, if your Behavior names are generated procedurally by the environment and not known before runtime, or if you have many Behaviors with very similar settings. To specify a default configuraton, insert a default_settings section in your YAML. This section should be formatted exactly like a configuration for a Behavior. default_settings: # < Same as Behavior configuration > behaviors: # < Same as above > Behaviors found in the environment that aren't specified in the YAML will now use the default_settings , and unspecified settings in behavior configurations will default to the values in default_settings if specified there.","title":"Default Behavior Settings"},{"location":"Training-ML-Agents/#environment-parameters","text":"In order to control the EnvironmentParameters in the Unity simulation during training, you need to add a section called environment_parameters . For example you can set the value of an EnvironmentParameter called my_environment_parameter to 3.0 with the following code : behaviors: BehaviorY: # < Same as above > # Add this section environment_parameters: my_environment_parameter: 3.0 Inside the Unity simulation, you can access your Environment Parameters by doing : Academy.Instance.EnvironmentParameters.GetWithDefault(\"my_environment_parameter\", 0.0f);","title":"Environment Parameters"},{"location":"Training-ML-Agents/#environment-parameter-randomization","text":"To enable environment parameter randomization, you need to edit the environment_parameters section of your training configuration yaml file. Instead of providing a single float value for your environment parameter, you can specify a sampler instead. Here is an example with three environment parameters called mass , length and scale : behaviors: BehaviorY: # < Same as above > # Add this section environment_parameters: mass: sampler_type: uniform sampler_parameters: min_value: 0.5 max_value: 10 length: sampler_type: multirangeuniform sampler_parameters: intervals: [[7, 10], [15, 20]] scale: sampler_type: gaussian sampler_parameters: mean: 2 st_dev: .3 Setting Description sampler_type A string identifier for the type of sampler to use for this Environment Parameter . sampler_parameters The parameters for a given sampler_type . Samplers of different types can have different sampler_parameters","title":"Environment Parameter Randomization"},{"location":"Training-ML-Agents/#supported-sampler-types","text":"Below is a list of the sampler_type values supported by the toolkit. uniform - Uniform sampler Uniformly samples a single float value from a range with a given minimum and maximum value (inclusive). parameters - min_value , max_value gaussian - Gaussian sampler Samples a single float value from a normal distribution with a given mean and standard deviation. parameters - mean , st_dev multirange_uniform - Multirange uniform sampler First, samples an interval from a set of intervals in proportion to relative length of the intervals. Then, uniformly samples a single float value from the sampled interval (inclusive). This sampler can take an arbitrary number of intervals in a list in the following format: [[ interval_1_min , interval_1_max ], [ interval_2_min , interval_2_max ], ...] parameters - intervals The implementation of the samplers can be found in the Samplers.cs file .","title":"Supported Sampler Types"},{"location":"Training-ML-Agents/#training-with-environment-parameter-randomization","text":"After the sampler configuration is defined, we proceed by launching mlagents-learn and specify trainer configuration with parameter randomization enabled. For example, if we wanted to train the 3D ball agent with parameter randomization, we would run mlagents-learn config/ppo/3DBall_randomize.yaml --run-id=3D-Ball-randomize We can observe progress and metrics via TensorBoard.","title":"Training with Environment Parameter Randomization"},{"location":"Training-ML-Agents/#curriculum","text":"To enable curriculum learning, you need to add a curriculum sub-section to your environment parameter. Here is one example with the environment parameter my_environment_parameter : behaviors: BehaviorY: # < Same as above > # Add this section environment_parameters: my_environment_parameter: curriculum: - name: MyFirstLesson # The '-' is important as this is a list completion_criteria: measure: progress behavior: my_behavior signal_smoothing: true min_lesson_length: 100 threshold: 0.2 value: 0.0 - name: MySecondLesson # This is the start of the second lesson completion_criteria: measure: progress behavior: my_behavior signal_smoothing: true min_lesson_length: 100 threshold: 0.6 require_reset: true value: sampler_type: uniform sampler_parameters: min_value: 4.0 max_value: 7.0 - name: MyLastLesson value: 8.0 Note that this curriculum only applies to my_environment_parameter . The curriculum section contains a list of Lessons . In the example, the lessons are named MyFirstLesson , MySecondLesson and MyLastLesson . Each Lesson has 3 fields : name which is a user defined name for the lesson (The name of the lesson will be displayed in the console when the lesson changes) completion_criteria which determines what needs to happen in the simulation before the lesson can be considered complete. When that condition is met, the curriculum moves on to the next Lesson . Note that you do not need to specify a completion_criteria for the last Lesson value which is the value the environment parameter will take during the lesson. Note that this can be a float or a sampler. There are the different settings of the completion_criteria : Setting Description measure What to measure learning progress, and advancement in lessons by. reward uses a measure of received reward, progress uses the ratio of steps/max_steps, while Elo is available only for self-play situations and uses Elo score as a curriculum completion measure. behavior Specifies which behavior is being tracked. There can be multiple behaviors with different names, each at different points of training. This setting allows the curriculum to track only one of them. threshold Determines at what point in value of measure the lesson should be increased. min_lesson_length The minimum number of episodes that should be completed before the lesson can change. If measure is set to reward , the average cumulative reward of the last min_lesson_length episodes will be used to determine if the lesson should change. Must be nonnegative. Important : the average reward that is compared to the thresholds is different than the mean reward that is logged to the console. For example, if min_lesson_length is 100 , the lesson will increment after the average cumulative reward of the last 100 episodes exceeds the current threshold. The mean reward logged to the console is dictated by the summary_freq parameter defined above. signal_smoothing Whether to weight the current progress measure by previous values. require_reset Whether changing lesson requires the environment to reset (default: false) ##### Training with a Curriculum Once we have specified our metacurriculum and curricula, we can launch mlagents-learn to point to the config file containing our curricula and PPO will train using Curriculum Learning. For example, to train agents in the Wall Jump environment with curriculum learning, we can run: mlagents-learn config/ppo/WallJump_curriculum.yaml --run-id=wall-jump-curriculum We can then keep track of the current lessons and progresses via TensorBoard. If you've terminated the run, you can resume it using --resume and lesson progress will start off where it ended.","title":"Curriculum"},{"location":"Training-ML-Agents/#training-using-concurrent-unity-instances","text":"In order to run concurrent Unity instances during training, set the number of environment instances using the command line option --num-envs=<n> when you invoke mlagents-learn . Optionally, you can also set the --base-port , which is the starting port used for the concurrent Unity instances. Some considerations: Buffer Size - If you are having trouble getting an agent to train, even with multiple concurrent Unity instances, you could increase buffer_size in the trainer config file. A common practice is to multiply buffer_size by num-envs . Resource Constraints - Invoking concurrent Unity instances is constrained by the resources on the machine. Please use discretion when setting --num-envs=<n> . Result Variation Using Concurrent Unity Instances - If you keep all the hyperparameters the same, but change --num-envs=<n> , the results and model would likely change.","title":"Training Using Concurrent Unity Instances"},{"location":"Training-Plugins/","text":"Customizing Training via Plugins ML-Agents provides support for running your own python implementations of specific interfaces during the training process. These interfaces are currently fairly limited, but will be expanded in the future. Note: Plugin interfaces should currently be considered \"in beta\", and they may change in future releases. How to Write Your Own Plugin This video explains the basics of how to create a plugin system using setuptools, and is the same approach that ML-Agents' plugin system is based on. The ml-agents-plugin-examples directory contains a reference implementation of each plugin interface, so it's a good starting point. setup.py If you don't already have a setup.py file for your python code, you'll need to add one. ml-agents-plugin-examples has a minimal example of this. In the call to setup() , you'll need to add to the entry_points dictionary for each plugin interface that you implement. The form of this is {entry point name}={plugin module}:{plugin function} . For example, in ml-agents-plugin-examples : entry_points={ ML_AGENTS_STATS_WRITER: [ \"example=mlagents_plugin_examples.example_stats_writer:get_example_stats_writer\" ] } ML_AGENTS_STATS_WRITER (which is a string constant, mlagents.stats_writer ) is the name of the plugin interface. This must be one of the provided interfaces ( see below ). example is the plugin implementation name. This can be anything. mlagents_plugin_examples.example_stats_writer is the plugin module. This points to the module where the plugin registration function is defined. get_example_stats_writer is the plugin registration function. This is called when running mlagents-learn . The arguments and expected return type for this are different for each plugin interface. Local Installation Once you've defined entry_points in your setup.py , you will need to run pip install -e [path to your plugin code] in the same python virtual environment that you have mlagents installed. Plugin Interfaces StatsWriter The StatsWriter class receives various information from the training process, such as the average Agent reward in each summary period. By default, we log this information to the console and write it to TensorBoard . Interface The StatsWriter.write_stats() method must be implemented in any derived classes. It takes a \"category\" parameter, which typically is the behavior name of the Agents being trained, and a dictionary of StatSummary values with string keys. Additionally, StatsWriter.on_add_stat() may be extended to register a callback handler for each stat emission. Registration The StatsWriter registration function takes a RunOptions argument and returns a list of StatsWriter s. An example implementation is provided in mlagents_plugin_examples","title":"Customizing Training via Plugins"},{"location":"Training-Plugins/#customizing-training-via-plugins","text":"ML-Agents provides support for running your own python implementations of specific interfaces during the training process. These interfaces are currently fairly limited, but will be expanded in the future. Note: Plugin interfaces should currently be considered \"in beta\", and they may change in future releases.","title":"Customizing Training via Plugins"},{"location":"Training-Plugins/#how-to-write-your-own-plugin","text":"This video explains the basics of how to create a plugin system using setuptools, and is the same approach that ML-Agents' plugin system is based on. The ml-agents-plugin-examples directory contains a reference implementation of each plugin interface, so it's a good starting point.","title":"How to Write Your Own Plugin"},{"location":"Training-Plugins/#setuppy","text":"If you don't already have a setup.py file for your python code, you'll need to add one. ml-agents-plugin-examples has a minimal example of this. In the call to setup() , you'll need to add to the entry_points dictionary for each plugin interface that you implement. The form of this is {entry point name}={plugin module}:{plugin function} . For example, in ml-agents-plugin-examples : entry_points={ ML_AGENTS_STATS_WRITER: [ \"example=mlagents_plugin_examples.example_stats_writer:get_example_stats_writer\" ] } ML_AGENTS_STATS_WRITER (which is a string constant, mlagents.stats_writer ) is the name of the plugin interface. This must be one of the provided interfaces ( see below ). example is the plugin implementation name. This can be anything. mlagents_plugin_examples.example_stats_writer is the plugin module. This points to the module where the plugin registration function is defined. get_example_stats_writer is the plugin registration function. This is called when running mlagents-learn . The arguments and expected return type for this are different for each plugin interface.","title":"setup.py"},{"location":"Training-Plugins/#local-installation","text":"Once you've defined entry_points in your setup.py , you will need to run pip install -e [path to your plugin code] in the same python virtual environment that you have mlagents installed.","title":"Local Installation"},{"location":"Training-Plugins/#plugin-interfaces","text":"","title":"Plugin Interfaces"},{"location":"Training-Plugins/#statswriter","text":"The StatsWriter class receives various information from the training process, such as the average Agent reward in each summary period. By default, we log this information to the console and write it to TensorBoard .","title":"StatsWriter"},{"location":"Training-Plugins/#interface","text":"The StatsWriter.write_stats() method must be implemented in any derived classes. It takes a \"category\" parameter, which typically is the behavior name of the Agents being trained, and a dictionary of StatSummary values with string keys. Additionally, StatsWriter.on_add_stat() may be extended to register a callback handler for each stat emission.","title":"Interface"},{"location":"Training-Plugins/#registration","text":"The StatsWriter registration function takes a RunOptions argument and returns a list of StatsWriter s. An example implementation is provided in mlagents_plugin_examples","title":"Registration"},{"location":"Training-on-Amazon-Web-Service/","text":"Training on Amazon Web Service :warning: Note: We no longer use this guide ourselves and so it may not work correctly. We've decided to keep it up just in case it is helpful to you. This page contains instructions for setting up an EC2 instance on Amazon Web Service for training ML-Agents environments. Pre-configured AMI We've prepared a pre-configured AMI for you with the ID: ami-016ff5559334f8619 in the us-east-1 region. It was created as a modification of Deep Learning AMI (Ubuntu) . The AMI has been tested with p2.xlarge instance. Furthermore, if you want to train without headless mode, you need to enable X Server. After launching your EC2 instance using the ami and ssh into it, run the following commands to enable it: # Start the X Server, press Enter to come to the command line $ sudo /usr/bin/X :0 & # Check if Xorg process is running # You will have a list of processes running on the GPU, Xorg should be in the # list, as shown below $ nvidia-smi # Thu Jun 14 20:27:26 2018 # +-----------------------------------------------------------------------------+ # | NVIDIA-SMI 390.67 Driver Version: 390.67 | # |-------------------------------+----------------------+----------------------+ # | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | # | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | # |===============================+======================+======================| # | 0 Tesla K80 On | 00000000:00:1E.0 Off | 0 | # | N/A 35C P8 31W / 149W | 9MiB / 11441MiB | 0% Default | # +-------------------------------+----------------------+----------------------+ # # +-----------------------------------------------------------------------------+ # | Processes: GPU Memory | # | GPU PID Type Process name Usage | # |=============================================================================| # | 0 2331 G /usr/lib/xorg/Xorg 8MiB | # +-----------------------------------------------------------------------------+ # Make the ubuntu use X Server for display $ export DISPLAY=:0 Configuring your own instance You could also choose to configure your own instance. To begin with, you will need an EC2 instance which contains the latest Nvidia drivers, CUDA9, and cuDNN. In this tutorial we used the Deep Learning AMI (Ubuntu) listed under AWS Marketplace with a p2.xlarge instance. Installing the ML-Agents Toolkit on the instance After launching your EC2 instance using the ami and ssh into it: Activate the python3 environment sh source activate python3 Clone the ML-Agents repo and install the required Python packages sh git clone --branch release_19 https://github.com/Unity-Technologies/ml-agents.git cd ml-agents/ml-agents/ pip3 install -e . Setting up X Server (optional) X Server setup is only necessary if you want to do training that requires visual observation input. Instructions here are adapted from this Medium post on running general Unity applications in the cloud. Current limitations of the Unity Engine require that a screen be available to render to when using visual observations. In order to make this possible when training on a remote server, a virtual screen is required. We can do this by installing Xorg and creating a virtual screen. Once installed and created, we can display the Unity environment in the virtual environment, and train as we would on a local machine. Ensure that headless mode is disabled when building linux executables which use visual observations. Install and setup Xorg: ```sh # Install Xorg $ sudo apt-get update $ sudo apt-get install -y xserver-xorg mesa-utils $ sudo nvidia-xconfig -a --use-display-device=None --virtual=1280x1024 # Get the BusID information $ nvidia-xconfig --query-gpu-info # Add the BusID information to your /etc/X11/xorg.conf file $ sudo sed -i 's/ BoardName \"Tesla K80\"/ BoardName \"Tesla K80\"\\n BusID \"0:30:0\"/g' /etc/X11/xorg.conf # Remove the Section \"Files\" from the /etc/X11/xorg.conf file # And remove two lines that contain Section \"Files\" and EndSection $ sudo vim /etc/X11/xorg.conf ``` Update and setup Nvidia driver: ```sh # Download and install the latest Nvidia driver for ubuntu # Please refer to http://download.nvidia.com/XFree86/Linux-#x86_64/latest.txt $ wget http://download.nvidia.com/XFree86/Linux-x86_64/390.87/NVIDIA-Linux-x86_64-390.87.run $ sudo /bin/bash ./NVIDIA-Linux-x86_64-390.87.run --accept-license --no-questions --ui=none # Disable Nouveau as it will clash with the Nvidia driver $ sudo echo 'blacklist nouveau' | sudo tee -a /etc/modprobe.d/blacklist.conf $ sudo echo 'options nouveau modeset=0' | sudo tee -a /etc/modprobe.d/blacklist.conf $ sudo echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf $ sudo update-initramfs -u ``` Restart the EC2 instance: ```sh sudo reboot now ``` Make sure there are no Xorg processes running: # Kill any possible running Xorg processes # Note that you might have to run this command multiple times depending on # how Xorg is configured. $ sudo killall Xorg # Check if there is any Xorg process left # You will have a list of processes running on the GPU, Xorg should not be in # the list, as shown below. $ nvidia-smi # Thu Jun 14 20:21:11 2018 # +-----------------------------------------------------------------------------+ # | NVIDIA-SMI 390.67 Driver Version: 390.67 | # |-------------------------------+----------------------+----------------------+ # | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | # | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | # |===============================+======================+======================| # | 0 Tesla K80 On | 00000000:00:1E.0 Off | 0 | # | N/A 37C P8 31W / 149W | 0MiB / 11441MiB | 0% Default | # +-------------------------------+----------------------+----------------------+ # # +-----------------------------------------------------------------------------+ # | Processes: GPU Memory | # | GPU PID Type Process name Usage | # |=============================================================================| # | No running processes found | # +-----------------------------------------------------------------------------+ Start X Server and make the ubuntu use X Server for display: ```console # Start the X Server, press Enter to come back to the command line $ sudo /usr/bin/X :0 & # Check if Xorg process is running # You will have a list of processes running on the GPU, Xorg should be in the list. $ nvidia-smi # Make the ubuntu use X Server for display $ export DISPLAY=:0 ``` Ensure the Xorg is correctly configured: ```sh # For more information on glxgears, see ftp://www.x.org/pub/X11R6.8.1/doc/glxgears.1.html. $ glxgears # If Xorg is configured correctly, you should see the following message # Running synchronized to the vertical refresh. The framerate should be # approximately the same as the monitor refresh rate. # 137296 frames in 5.0 seconds = 27459.053 FPS # 141674 frames in 5.0 seconds = 28334.779 FPS # 141490 frames in 5.0 seconds = 28297.875 FPS ``` Training on EC2 instance In the Unity Editor, load a project containing an ML-Agents environment (you can use one of the example environments if you have not created your own). Open the Build Settings window (menu: File > Build Settings). Select Linux as the Target Platform, and x86_64 as the target architecture (the default x86 currently does not work). Check Headless Mode if you have not setup the X Server. (If you do not use Headless Mode, you have to setup the X Server to enable training.) Click Build to build the Unity environment executable. Upload the executable to your EC2 instance within ml-agents folder. Change the permissions of the executable. sh chmod +x <your_env>.x86_64 (Without Headless Mode) Start X Server and use it for display: ```sh # Start the X Server, press Enter to come back to the command line $ sudo /usr/bin/X :0 & # Check if Xorg process is running # You will have a list of processes running on the GPU, Xorg should be in the list. $ nvidia-smi # Make the ubuntu use X Server for display $ export DISPLAY=:0 ``` Test the instance setup from Python using: ```python from mlagents_envs.environment import UnityEnvironment env = UnityEnvironment( ) ``` Where <your_env> corresponds to the path to your environment executable. You should receive a message confirming that the environment was loaded successfully. Train your models console mlagents-learn <trainer-config-file> --env=<your_env> --train FAQ The _Data folder hasn't been copied cover If you've built your Linux executable, but forget to copy over the corresponding _Data folder, you will see error message like the following: Set current directory to /home/ubuntu/ml-agents/ml-agents Found path: /home/ubuntu/ml-agents/ml-agents/3dball_linux.x86_64 no boot config - using default values (Filename: Line: 403) There is no data folder Unity Environment not responding If you didn't setup X Server or hasn't launched it properly, or your environment somehow crashes, or you haven't chmod +x your Unity Environment, all of these will cause connection between Unity and Python to fail. Then you will see something like this: Logging to /home/ubuntu/.config/unity3d/<Some_Path>/Player.log Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/home/ubuntu/ml-agents/ml-agents/mlagents_envs/environment.py\", line 63, in __init__ aca_params = self.send_academy_parameters(rl_init_parameters_in) File \"/home/ubuntu/ml-agents/ml-agents/mlagents_envs/environment.py\", line 489, in send_academy_parameters return self.communicator.initialize(inputs).rl_initialization_output File \"/home/ubuntu/ml-agents/ml-agents/mlagents_envs/rpc_communicator.py\", line 60, in initialize mlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that : The environment does not need user interaction to launch The environment and the Python interface have compatible versions. It would be also really helpful to check your /home/ubuntu/.config/unity3d/ /Player.log to see what happens with your Unity environment. Could not launch X Server When you execute: sudo /usr/bin/X :0 & You might see something like: X.Org X Server 1.18.4 ... (==) Log file: \"/var/log/Xorg.0.log\", Time: Thu Oct 11 21:10:38 2018 (==) Using config file: \"/etc/X11/xorg.conf\" (==) Using system config directory \"/usr/share/X11/xorg.conf.d\" (EE) Fatal server error: (EE) no screens found(EE) (EE) Please consult the X.Org Foundation support at http://wiki.x.org for help. (EE) Please also check the log file at \"/var/log/Xorg.0.log\" for additional information. (EE) (EE) Server terminated with error (1). Closing log file. And when you execute: nvidia-smi You might see something like: NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. This means the NVIDIA's driver needs to be updated. Refer to this section for more information.","title":"Training on Amazon Web Service"},{"location":"Training-on-Amazon-Web-Service/#training-on-amazon-web-service","text":":warning: Note: We no longer use this guide ourselves and so it may not work correctly. We've decided to keep it up just in case it is helpful to you. This page contains instructions for setting up an EC2 instance on Amazon Web Service for training ML-Agents environments.","title":"Training on Amazon Web Service"},{"location":"Training-on-Amazon-Web-Service/#pre-configured-ami","text":"We've prepared a pre-configured AMI for you with the ID: ami-016ff5559334f8619 in the us-east-1 region. It was created as a modification of Deep Learning AMI (Ubuntu) . The AMI has been tested with p2.xlarge instance. Furthermore, if you want to train without headless mode, you need to enable X Server. After launching your EC2 instance using the ami and ssh into it, run the following commands to enable it: # Start the X Server, press Enter to come to the command line $ sudo /usr/bin/X :0 & # Check if Xorg process is running # You will have a list of processes running on the GPU, Xorg should be in the # list, as shown below $ nvidia-smi # Thu Jun 14 20:27:26 2018 # +-----------------------------------------------------------------------------+ # | NVIDIA-SMI 390.67 Driver Version: 390.67 | # |-------------------------------+----------------------+----------------------+ # | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | # | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | # |===============================+======================+======================| # | 0 Tesla K80 On | 00000000:00:1E.0 Off | 0 | # | N/A 35C P8 31W / 149W | 9MiB / 11441MiB | 0% Default | # +-------------------------------+----------------------+----------------------+ # # +-----------------------------------------------------------------------------+ # | Processes: GPU Memory | # | GPU PID Type Process name Usage | # |=============================================================================| # | 0 2331 G /usr/lib/xorg/Xorg 8MiB | # +-----------------------------------------------------------------------------+ # Make the ubuntu use X Server for display $ export DISPLAY=:0","title":"Pre-configured AMI"},{"location":"Training-on-Amazon-Web-Service/#configuring-your-own-instance","text":"You could also choose to configure your own instance. To begin with, you will need an EC2 instance which contains the latest Nvidia drivers, CUDA9, and cuDNN. In this tutorial we used the Deep Learning AMI (Ubuntu) listed under AWS Marketplace with a p2.xlarge instance.","title":"Configuring your own instance"},{"location":"Training-on-Amazon-Web-Service/#installing-the-ml-agents-toolkit-on-the-instance","text":"After launching your EC2 instance using the ami and ssh into it: Activate the python3 environment sh source activate python3 Clone the ML-Agents repo and install the required Python packages sh git clone --branch release_19 https://github.com/Unity-Technologies/ml-agents.git cd ml-agents/ml-agents/ pip3 install -e .","title":"Installing the ML-Agents Toolkit on the instance"},{"location":"Training-on-Amazon-Web-Service/#setting-up-x-server-optional","text":"X Server setup is only necessary if you want to do training that requires visual observation input. Instructions here are adapted from this Medium post on running general Unity applications in the cloud. Current limitations of the Unity Engine require that a screen be available to render to when using visual observations. In order to make this possible when training on a remote server, a virtual screen is required. We can do this by installing Xorg and creating a virtual screen. Once installed and created, we can display the Unity environment in the virtual environment, and train as we would on a local machine. Ensure that headless mode is disabled when building linux executables which use visual observations.","title":"Setting up X Server (optional)"},{"location":"Training-on-Amazon-Web-Service/#install-and-setup-xorg","text":"```sh # Install Xorg $ sudo apt-get update $ sudo apt-get install -y xserver-xorg mesa-utils $ sudo nvidia-xconfig -a --use-display-device=None --virtual=1280x1024 # Get the BusID information $ nvidia-xconfig --query-gpu-info # Add the BusID information to your /etc/X11/xorg.conf file $ sudo sed -i 's/ BoardName \"Tesla K80\"/ BoardName \"Tesla K80\"\\n BusID \"0:30:0\"/g' /etc/X11/xorg.conf # Remove the Section \"Files\" from the /etc/X11/xorg.conf file # And remove two lines that contain Section \"Files\" and EndSection $ sudo vim /etc/X11/xorg.conf ```","title":"Install and setup Xorg:"},{"location":"Training-on-Amazon-Web-Service/#update-and-setup-nvidia-driver","text":"```sh # Download and install the latest Nvidia driver for ubuntu # Please refer to http://download.nvidia.com/XFree86/Linux-#x86_64/latest.txt $ wget http://download.nvidia.com/XFree86/Linux-x86_64/390.87/NVIDIA-Linux-x86_64-390.87.run $ sudo /bin/bash ./NVIDIA-Linux-x86_64-390.87.run --accept-license --no-questions --ui=none # Disable Nouveau as it will clash with the Nvidia driver $ sudo echo 'blacklist nouveau' | sudo tee -a /etc/modprobe.d/blacklist.conf $ sudo echo 'options nouveau modeset=0' | sudo tee -a /etc/modprobe.d/blacklist.conf $ sudo echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf $ sudo update-initramfs -u ```","title":"Update and setup Nvidia driver:"},{"location":"Training-on-Amazon-Web-Service/#restart-the-ec2-instance","text":"```sh sudo reboot now ```","title":"Restart the EC2 instance:"},{"location":"Training-on-Amazon-Web-Service/#make-sure-there-are-no-xorg-processes-running","text":"# Kill any possible running Xorg processes # Note that you might have to run this command multiple times depending on # how Xorg is configured. $ sudo killall Xorg # Check if there is any Xorg process left # You will have a list of processes running on the GPU, Xorg should not be in # the list, as shown below. $ nvidia-smi # Thu Jun 14 20:21:11 2018 # +-----------------------------------------------------------------------------+ # | NVIDIA-SMI 390.67 Driver Version: 390.67 | # |-------------------------------+----------------------+----------------------+ # | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | # | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | # |===============================+======================+======================| # | 0 Tesla K80 On | 00000000:00:1E.0 Off | 0 | # | N/A 37C P8 31W / 149W | 0MiB / 11441MiB | 0% Default | # +-------------------------------+----------------------+----------------------+ # # +-----------------------------------------------------------------------------+ # | Processes: GPU Memory | # | GPU PID Type Process name Usage | # |=============================================================================| # | No running processes found | # +-----------------------------------------------------------------------------+","title":"Make sure there are no Xorg processes running:"},{"location":"Training-on-Amazon-Web-Service/#start-x-server-and-make-the-ubuntu-use-x-server-for-display","text":"```console # Start the X Server, press Enter to come back to the command line $ sudo /usr/bin/X :0 & # Check if Xorg process is running # You will have a list of processes running on the GPU, Xorg should be in the list. $ nvidia-smi # Make the ubuntu use X Server for display $ export DISPLAY=:0 ```","title":"Start X Server and make the ubuntu use X Server for display:"},{"location":"Training-on-Amazon-Web-Service/#ensure-the-xorg-is-correctly-configured","text":"```sh # For more information on glxgears, see ftp://www.x.org/pub/X11R6.8.1/doc/glxgears.1.html. $ glxgears # If Xorg is configured correctly, you should see the following message # Running synchronized to the vertical refresh. The framerate should be # approximately the same as the monitor refresh rate. # 137296 frames in 5.0 seconds = 27459.053 FPS # 141674 frames in 5.0 seconds = 28334.779 FPS # 141490 frames in 5.0 seconds = 28297.875 FPS ```","title":"Ensure the Xorg is correctly configured:"},{"location":"Training-on-Amazon-Web-Service/#training-on-ec2-instance","text":"In the Unity Editor, load a project containing an ML-Agents environment (you can use one of the example environments if you have not created your own). Open the Build Settings window (menu: File > Build Settings). Select Linux as the Target Platform, and x86_64 as the target architecture (the default x86 currently does not work). Check Headless Mode if you have not setup the X Server. (If you do not use Headless Mode, you have to setup the X Server to enable training.) Click Build to build the Unity environment executable. Upload the executable to your EC2 instance within ml-agents folder. Change the permissions of the executable. sh chmod +x <your_env>.x86_64 (Without Headless Mode) Start X Server and use it for display: ```sh # Start the X Server, press Enter to come back to the command line $ sudo /usr/bin/X :0 & # Check if Xorg process is running # You will have a list of processes running on the GPU, Xorg should be in the list. $ nvidia-smi # Make the ubuntu use X Server for display $ export DISPLAY=:0 ``` Test the instance setup from Python using: ```python from mlagents_envs.environment import UnityEnvironment env = UnityEnvironment( ) ``` Where <your_env> corresponds to the path to your environment executable. You should receive a message confirming that the environment was loaded successfully. Train your models console mlagents-learn <trainer-config-file> --env=<your_env> --train","title":"Training on EC2 instance"},{"location":"Training-on-Amazon-Web-Service/#faq","text":"","title":"FAQ"},{"location":"Training-on-Amazon-Web-Service/#the-_data-folder-hasnt-been-copied-cover","text":"If you've built your Linux executable, but forget to copy over the corresponding _Data folder, you will see error message like the following: Set current directory to /home/ubuntu/ml-agents/ml-agents Found path: /home/ubuntu/ml-agents/ml-agents/3dball_linux.x86_64 no boot config - using default values (Filename: Line: 403) There is no data folder","title":"The _Data folder hasn't been copied cover"},{"location":"Training-on-Amazon-Web-Service/#unity-environment-not-responding","text":"If you didn't setup X Server or hasn't launched it properly, or your environment somehow crashes, or you haven't chmod +x your Unity Environment, all of these will cause connection between Unity and Python to fail. Then you will see something like this: Logging to /home/ubuntu/.config/unity3d/<Some_Path>/Player.log Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/home/ubuntu/ml-agents/ml-agents/mlagents_envs/environment.py\", line 63, in __init__ aca_params = self.send_academy_parameters(rl_init_parameters_in) File \"/home/ubuntu/ml-agents/ml-agents/mlagents_envs/environment.py\", line 489, in send_academy_parameters return self.communicator.initialize(inputs).rl_initialization_output File \"/home/ubuntu/ml-agents/ml-agents/mlagents_envs/rpc_communicator.py\", line 60, in initialize mlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that : The environment does not need user interaction to launch The environment and the Python interface have compatible versions. It would be also really helpful to check your /home/ubuntu/.config/unity3d/ /Player.log to see what happens with your Unity environment.","title":"Unity Environment not responding"},{"location":"Training-on-Amazon-Web-Service/#could-not-launch-x-server","text":"When you execute: sudo /usr/bin/X :0 & You might see something like: X.Org X Server 1.18.4 ... (==) Log file: \"/var/log/Xorg.0.log\", Time: Thu Oct 11 21:10:38 2018 (==) Using config file: \"/etc/X11/xorg.conf\" (==) Using system config directory \"/usr/share/X11/xorg.conf.d\" (EE) Fatal server error: (EE) no screens found(EE) (EE) Please consult the X.Org Foundation support at http://wiki.x.org for help. (EE) Please also check the log file at \"/var/log/Xorg.0.log\" for additional information. (EE) (EE) Server terminated with error (1). Closing log file. And when you execute: nvidia-smi You might see something like: NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. This means the NVIDIA's driver needs to be updated. Refer to this section for more information.","title":"Could not launch X Server"},{"location":"Training-on-Microsoft-Azure/","text":"Training on Microsoft Azure (works with ML-Agents Toolkit v0.3) :warning: Note: We no longer use this guide ourselves and so it may not work correctly. We've decided to keep it up just in case it is helpful to you. This page contains instructions for setting up training on Microsoft Azure through either Azure Container Instances or Virtual Machines. Non \"headless\" training has not yet been tested to verify support. Pre-Configured Azure Virtual Machine A pre-configured virtual machine image is available in the Azure Marketplace and is nearly completely ready for training. You can start by deploying the Data Science Virtual Machine for Linux (Ubuntu) into your Azure subscription. Note that, if you choose to deploy the image to an N-Series GPU optimized VM , training will, by default, run on the GPU. If you choose any other type of VM, training will run on the CPU. Configuring your own Instance Setting up your own instance requires a number of package installations. Please view the documentation for doing so here . Installing ML-Agents Move the ml-agents sub-folder of this ml-agents repo to the remote Azure instance, and set it as the working directory. Install the required packages: Torch: pip3 install torch==1.7.0 -f https://download.pytorch.org/whl/torch_stable.html and MLAgents: python -m pip install mlagents==0.28.0 Testing To verify that all steps worked correctly: In the Unity Editor, load a project containing an ML-Agents environment (you can use one of the example environments if you have not created your own). Open the Build Settings window (menu: File > Build Settings). Select Linux as the Target Platform, and x86_64 as the target architecture. Check Headless Mode. Click Build to build the Unity environment executable. Upload the resulting files to your Azure instance. Test the instance setup from Python using: from mlagents_envs.environment import UnityEnvironment env = UnityEnvironment(file_name=\"<your_env>\", seed=1, side_channels=[]) Where <your_env> corresponds to the path to your environment executable (i.e. /home/UserName/Build/yourFile ). You should receive a message confirming that the environment was loaded successfully. Note: When running your environment in headless mode, you must append --no-graphics to your mlagents-learn command, as it won't train otherwise. You can test this simply by aborting a training and check if it says \"Model Saved\" or \"Aborted\", or see if it generated the .onnx in the result folder. Running Training on your Virtual Machine To run your training on the VM: Move your built Unity application to your Virtual Machine. Set the directory where the ML-Agents Toolkit was installed to your working directory. Run the following command: mlagents-learn <trainer_config> --env=<your_app> --run-id=<run_id> --train Where <your_app> is the path to your app (i.e. ~/unity-volume/3DBallHeadless ) and <run_id> is an identifier you would like to identify your training run with. If you've selected to run on a N-Series VM with GPU support, you can verify that the GPU is being used by running nvidia-smi from the command line. Monitoring your Training Run with TensorBoard Once you have started training, you can use TensorBoard to observe the training . Start by opening the appropriate port for web traffic to connect to your VM . Note that you don't need to generate a new Network Security Group but instead, go to the Networking tab under Settings for your VM. As an example, you could use the following settings to open the Port with the following Inbound Rule settings: Source: Any Source Port Ranges: * Destination: Any Destination Port Ranges: 6006 Protocol: Any Action: Allow Priority: (Leave as default) Unless you started the training as a background process, connect to your VM from another terminal instance. Run the following command from your terminal tensorboard --logdir results --host 0.0.0.0 You should now be able to open a browser and navigate to <Your_VM_IP_Address>:6060 to view the TensorBoard report. Running on Azure Container Instances Azure Container Instances allow you to spin up a container, on demand, that will run your training and then be shut down. This ensures you aren't leaving a billable VM running when it isn't needed. Using ACI enables you to offload training of your models without needing to install Python and TensorFlow on your own computer. Custom Instances This page contains instructions for setting up a custom Virtual Machine on Microsoft Azure so you can running ML-Agents training in the cloud. Start by deploying an Azure VM with Ubuntu Linux (tests were done with 16.04 LTS). To use GPU support, use a N-Series VM. SSH into your VM. Start with the following commands to install the Nvidia driver: ```sh wget http://us.download.nvidia.com/tesla/375.66/nvidia-diag-driver-local-repo-ubuntu1604_375.66-1_amd64.deb sudo dpkg -i nvidia-diag-driver-local-repo-ubuntu1604_375.66-1_amd64.deb sudo apt-get update sudo apt-get install cuda-drivers sudo reboot ``` After a minute you should be able to reconnect to your VM and install the CUDA toolkit: ```sh wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.61-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu1604_8.0.61-1_amd64.deb sudo apt-get update sudo apt-get install cuda-8-0 ``` You'll next need to download cuDNN from the Nvidia developer site. This requires a registered account. Navigate to http://developer.nvidia.com and create an account and verify it. Download (to your own computer) cuDNN from this url . Copy the deb package to your VM: sh scp libcudnn6_6.0.21-1+cuda8.0_amd64.deb <VMUserName>@<VMIPAddress>:libcudnn6_6.0.21-1+cuda8.0_amd64.deb SSH back to your VM and execute the following: ```console sudo dpkg -i libcudnn6_6.0.21-1+cuda8.0_amd64.deb export LD_LIBRARY_PATH=/usr/local/cuda/lib64/:/usr/lib/x86_64-linux-gnu/:$LD_LIBRARY_PATH . ~/.profile sudo reboot ``` After a minute, you should be able to SSH back into your VM. After doing so, run the following: sh sudo apt install python-pip sudo apt install python3-pip At this point, you need to install TensorFlow. The version you install should be tied to if you are using GPU to train: sh pip3 install tensorflow-gpu==1.4.0 keras==2.0.6 Or CPU to train: sh pip3 install tensorflow==1.4.0 keras==2.0.6 You'll then need to install additional dependencies: sh pip3 install pillow pip3 install numpy","title":"Training on Microsoft Azure (works with ML-Agents Toolkit v0.3)"},{"location":"Training-on-Microsoft-Azure/#training-on-microsoft-azure-works-with-ml-agents-toolkit-v03","text":":warning: Note: We no longer use this guide ourselves and so it may not work correctly. We've decided to keep it up just in case it is helpful to you. This page contains instructions for setting up training on Microsoft Azure through either Azure Container Instances or Virtual Machines. Non \"headless\" training has not yet been tested to verify support.","title":"Training on Microsoft Azure (works with ML-Agents Toolkit v0.3)"},{"location":"Training-on-Microsoft-Azure/#pre-configured-azure-virtual-machine","text":"A pre-configured virtual machine image is available in the Azure Marketplace and is nearly completely ready for training. You can start by deploying the Data Science Virtual Machine for Linux (Ubuntu) into your Azure subscription. Note that, if you choose to deploy the image to an N-Series GPU optimized VM , training will, by default, run on the GPU. If you choose any other type of VM, training will run on the CPU.","title":"Pre-Configured Azure Virtual Machine"},{"location":"Training-on-Microsoft-Azure/#configuring-your-own-instance","text":"Setting up your own instance requires a number of package installations. Please view the documentation for doing so here .","title":"Configuring your own Instance"},{"location":"Training-on-Microsoft-Azure/#installing-ml-agents","text":"Move the ml-agents sub-folder of this ml-agents repo to the remote Azure instance, and set it as the working directory. Install the required packages: Torch: pip3 install torch==1.7.0 -f https://download.pytorch.org/whl/torch_stable.html and MLAgents: python -m pip install mlagents==0.28.0","title":"Installing ML-Agents"},{"location":"Training-on-Microsoft-Azure/#testing","text":"To verify that all steps worked correctly: In the Unity Editor, load a project containing an ML-Agents environment (you can use one of the example environments if you have not created your own). Open the Build Settings window (menu: File > Build Settings). Select Linux as the Target Platform, and x86_64 as the target architecture. Check Headless Mode. Click Build to build the Unity environment executable. Upload the resulting files to your Azure instance. Test the instance setup from Python using: from mlagents_envs.environment import UnityEnvironment env = UnityEnvironment(file_name=\"<your_env>\", seed=1, side_channels=[]) Where <your_env> corresponds to the path to your environment executable (i.e. /home/UserName/Build/yourFile ). You should receive a message confirming that the environment was loaded successfully. Note: When running your environment in headless mode, you must append --no-graphics to your mlagents-learn command, as it won't train otherwise. You can test this simply by aborting a training and check if it says \"Model Saved\" or \"Aborted\", or see if it generated the .onnx in the result folder.","title":"Testing"},{"location":"Training-on-Microsoft-Azure/#running-training-on-your-virtual-machine","text":"To run your training on the VM: Move your built Unity application to your Virtual Machine. Set the directory where the ML-Agents Toolkit was installed to your working directory. Run the following command: mlagents-learn <trainer_config> --env=<your_app> --run-id=<run_id> --train Where <your_app> is the path to your app (i.e. ~/unity-volume/3DBallHeadless ) and <run_id> is an identifier you would like to identify your training run with. If you've selected to run on a N-Series VM with GPU support, you can verify that the GPU is being used by running nvidia-smi from the command line.","title":"Running Training on your Virtual Machine"},{"location":"Training-on-Microsoft-Azure/#monitoring-your-training-run-with-tensorboard","text":"Once you have started training, you can use TensorBoard to observe the training . Start by opening the appropriate port for web traffic to connect to your VM . Note that you don't need to generate a new Network Security Group but instead, go to the Networking tab under Settings for your VM. As an example, you could use the following settings to open the Port with the following Inbound Rule settings: Source: Any Source Port Ranges: * Destination: Any Destination Port Ranges: 6006 Protocol: Any Action: Allow Priority: (Leave as default) Unless you started the training as a background process, connect to your VM from another terminal instance. Run the following command from your terminal tensorboard --logdir results --host 0.0.0.0 You should now be able to open a browser and navigate to <Your_VM_IP_Address>:6060 to view the TensorBoard report.","title":"Monitoring your Training Run with TensorBoard"},{"location":"Training-on-Microsoft-Azure/#running-on-azure-container-instances","text":"Azure Container Instances allow you to spin up a container, on demand, that will run your training and then be shut down. This ensures you aren't leaving a billable VM running when it isn't needed. Using ACI enables you to offload training of your models without needing to install Python and TensorFlow on your own computer.","title":"Running on Azure Container Instances"},{"location":"Training-on-Microsoft-Azure/#custom-instances","text":"This page contains instructions for setting up a custom Virtual Machine on Microsoft Azure so you can running ML-Agents training in the cloud. Start by deploying an Azure VM with Ubuntu Linux (tests were done with 16.04 LTS). To use GPU support, use a N-Series VM. SSH into your VM. Start with the following commands to install the Nvidia driver: ```sh wget http://us.download.nvidia.com/tesla/375.66/nvidia-diag-driver-local-repo-ubuntu1604_375.66-1_amd64.deb sudo dpkg -i nvidia-diag-driver-local-repo-ubuntu1604_375.66-1_amd64.deb sudo apt-get update sudo apt-get install cuda-drivers sudo reboot ``` After a minute you should be able to reconnect to your VM and install the CUDA toolkit: ```sh wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.61-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu1604_8.0.61-1_amd64.deb sudo apt-get update sudo apt-get install cuda-8-0 ``` You'll next need to download cuDNN from the Nvidia developer site. This requires a registered account. Navigate to http://developer.nvidia.com and create an account and verify it. Download (to your own computer) cuDNN from this url . Copy the deb package to your VM: sh scp libcudnn6_6.0.21-1+cuda8.0_amd64.deb <VMUserName>@<VMIPAddress>:libcudnn6_6.0.21-1+cuda8.0_amd64.deb SSH back to your VM and execute the following: ```console sudo dpkg -i libcudnn6_6.0.21-1+cuda8.0_amd64.deb export LD_LIBRARY_PATH=/usr/local/cuda/lib64/:/usr/lib/x86_64-linux-gnu/:$LD_LIBRARY_PATH . ~/.profile sudo reboot ``` After a minute, you should be able to SSH back into your VM. After doing so, run the following: sh sudo apt install python-pip sudo apt install python3-pip At this point, you need to install TensorFlow. The version you install should be tied to if you are using GPU to train: sh pip3 install tensorflow-gpu==1.4.0 keras==2.0.6 Or CPU to train: sh pip3 install tensorflow==1.4.0 keras==2.0.6 You'll then need to install additional dependencies: sh pip3 install pillow pip3 install numpy","title":"Custom Instances"},{"location":"Unity-Environment-Registry/","text":"Unity Environment Registry [Experimental] The Unity Environment Registry is a database of pre-built Unity environments that can be easily used without having to install the Unity Editor. It is a great way to get started with our UnityEnvironment API . Loading an Environment from the Registry To get started, you can access the default registry we provide with our Example Environments . The Unity Environment Registry implements a Mapping , therefore, you can access an entry with its identifier with the square brackets [ ] . Use the following code to list all of the environment identifiers present in the default registry: from mlagents_envs.registry import default_registry environment_names = list(default_registry.keys()) for name in environment_names: print(name) The make() method on a registry value will return a UnityEnvironment ready to be used. All arguments passed to the make method will be passed to the constructor of the UnityEnvironment as well. Refer to the documentation on the Python-API for more information about the arguments of the UnityEnvironment constructor. For example, the following code will create the environment under the identifier \"my-env\" , reset it, perform a few steps and finally close it: from mlagents_envs.registry import default_registry env = default_registry[\"my-env\"].make() env.reset() for _ in range(10): env.step() env.close() Create and share your own registry In order to share the UnityEnvironemnt you created, you must : - Create a Unity executable of your environment for each platform (Linux, OSX and/or Windows) - Place each executable in a zip compressed folder - Upload each zip file online to your preferred hosting platform - Create a yaml file that will contain the description and path to your environment - Upload the yaml file online The yaml file must have the following format : environments: - <environment-identifier>: expected_reward: <expected-reward-float> description: <description-of-the-environment> linux_url: <url-to-the-linux-zip-folder> darwin_url: <url-to-the-osx-zip-folder> win_url: <url-to-the-windows-zip-folder> additional_args: - <an-optional-list-of-command-line-arguments-for-the-executable> - ... Your users can now use your environment with the following code : from mlagents_envs.registry import UnityEnvRegistry registry = UnityEnvRegistry() registry.register_from_yaml(\"url-or-path-to-your-yaml-file\") Note : The \"url-or-path-to-your-yaml-file\" can be either a url or a local path.","title":"Unity Environment Registry [Experimental]"},{"location":"Unity-Environment-Registry/#unity-environment-registry-experimental","text":"The Unity Environment Registry is a database of pre-built Unity environments that can be easily used without having to install the Unity Editor. It is a great way to get started with our UnityEnvironment API .","title":"Unity Environment Registry [Experimental]"},{"location":"Unity-Environment-Registry/#loading-an-environment-from-the-registry","text":"To get started, you can access the default registry we provide with our Example Environments . The Unity Environment Registry implements a Mapping , therefore, you can access an entry with its identifier with the square brackets [ ] . Use the following code to list all of the environment identifiers present in the default registry: from mlagents_envs.registry import default_registry environment_names = list(default_registry.keys()) for name in environment_names: print(name) The make() method on a registry value will return a UnityEnvironment ready to be used. All arguments passed to the make method will be passed to the constructor of the UnityEnvironment as well. Refer to the documentation on the Python-API for more information about the arguments of the UnityEnvironment constructor. For example, the following code will create the environment under the identifier \"my-env\" , reset it, perform a few steps and finally close it: from mlagents_envs.registry import default_registry env = default_registry[\"my-env\"].make() env.reset() for _ in range(10): env.step() env.close()","title":"Loading an Environment from the Registry"},{"location":"Unity-Environment-Registry/#create-and-share-your-own-registry","text":"In order to share the UnityEnvironemnt you created, you must : - Create a Unity executable of your environment for each platform (Linux, OSX and/or Windows) - Place each executable in a zip compressed folder - Upload each zip file online to your preferred hosting platform - Create a yaml file that will contain the description and path to your environment - Upload the yaml file online The yaml file must have the following format : environments: - <environment-identifier>: expected_reward: <expected-reward-float> description: <description-of-the-environment> linux_url: <url-to-the-linux-zip-folder> darwin_url: <url-to-the-osx-zip-folder> win_url: <url-to-the-windows-zip-folder> additional_args: - <an-optional-list-of-command-line-arguments-for-the-executable> - ... Your users can now use your environment with the following code : from mlagents_envs.registry import UnityEnvRegistry registry = UnityEnvRegistry() registry.register_from_yaml(\"url-or-path-to-your-yaml-file\") Note : The \"url-or-path-to-your-yaml-file\" can be either a url or a local path.","title":"Create and share your own registry"},{"location":"Unity-Inference-Engine/","text":"Unity Inference Engine The ML-Agents Toolkit allows you to use pre-trained neural network models inside your Unity games. This support is possible thanks to the Unity Inference Engine (codenamed Barracuda). The Unity Inference Engine uses compute shaders to run the neural network within Unity. Supported devices See the Unity Inference Engine documentation for a list of the supported platforms . Scripting Backends : The Unity Inference Engine is generally faster with IL2CPP than with Mono for Standalone builds. In the Editor, It is not possible to use the Unity Inference Engine with GPU device selected when Editor Graphics Emulation is set to OpenGL(ES) 3.0 or 2.0 emulation . Also there might be non-fatal build time errors when target platform includes Graphics API that does not support Unity Compute Shaders . Using the Unity Inference Engine When using a model, drag the model file into the Model field in the Inspector of the Agent. Select the Inference Device : CPU or GPU you want to use for Inference. Note: For most of the models generated with the ML-Agents Toolkit, CPU will be faster than GPU. You should use the GPU only if you use the ResNet visual encoder or have a large number of agents with visual observations. Unsupported use cases Externally trained models The ML-Agents Toolkit only supports the models created with our trainers. Model loading expects certain conventions for constants and tensor names. While it is possible to construct a model that follows these conventions, we don't provide any additional help for this. More details can be found in TensorNames.cs and BarracudaModelParamLoader.cs . If you wish to run inference on an externally trained model, you should use Barracuda directly, instead of trying to run it through ML-Agents. Model inference outside of Unity We do not provide support for inference anywhere outside of Unity. The .onnx files produced by training use the open format ONNX; if you wish to convert a .onnx file to another format or run inference with them, refer to their documentation.","title":"Unity Inference Engine"},{"location":"Unity-Inference-Engine/#unity-inference-engine","text":"The ML-Agents Toolkit allows you to use pre-trained neural network models inside your Unity games. This support is possible thanks to the Unity Inference Engine (codenamed Barracuda). The Unity Inference Engine uses compute shaders to run the neural network within Unity.","title":"Unity Inference Engine"},{"location":"Unity-Inference-Engine/#supported-devices","text":"See the Unity Inference Engine documentation for a list of the supported platforms . Scripting Backends : The Unity Inference Engine is generally faster with IL2CPP than with Mono for Standalone builds. In the Editor, It is not possible to use the Unity Inference Engine with GPU device selected when Editor Graphics Emulation is set to OpenGL(ES) 3.0 or 2.0 emulation . Also there might be non-fatal build time errors when target platform includes Graphics API that does not support Unity Compute Shaders .","title":"Supported devices"},{"location":"Unity-Inference-Engine/#using-the-unity-inference-engine","text":"When using a model, drag the model file into the Model field in the Inspector of the Agent. Select the Inference Device : CPU or GPU you want to use for Inference. Note: For most of the models generated with the ML-Agents Toolkit, CPU will be faster than GPU. You should use the GPU only if you use the ResNet visual encoder or have a large number of agents with visual observations.","title":"Using the Unity Inference Engine"},{"location":"Unity-Inference-Engine/#unsupported-use-cases","text":"","title":"Unsupported use cases"},{"location":"Unity-Inference-Engine/#externally-trained-models","text":"The ML-Agents Toolkit only supports the models created with our trainers. Model loading expects certain conventions for constants and tensor names. While it is possible to construct a model that follows these conventions, we don't provide any additional help for this. More details can be found in TensorNames.cs and BarracudaModelParamLoader.cs . If you wish to run inference on an externally trained model, you should use Barracuda directly, instead of trying to run it through ML-Agents.","title":"Externally trained models"},{"location":"Unity-Inference-Engine/#model-inference-outside-of-unity","text":"We do not provide support for inference anywhere outside of Unity. The .onnx files produced by training use the open format ONNX; if you wish to convert a .onnx file to another format or run inference with them, refer to their documentation.","title":"Model inference outside of Unity"},{"location":"Using-Docker/","text":"Using Docker For ML-Agents (Deprecated) :warning: Note: We no longer use this guide ourselves and so it may not work correctly. We've decided to keep it up just in case it is helpful to you. We currently offer a solution for Windows and Mac users who would like to do training or inference using Docker. This option may be appealing to those who would like to avoid installing Python and TensorFlow themselves. The current setup forces both TensorFlow and Unity to only rely on the CPU for computations. Consequently, our Docker simulation does not use a GPU and uses Xvfb to do visual rendering. Xvfb is a utility that enables ML-Agents (or any other application) to do rendering virtually i.e. it does not assume that the machine running ML-Agents has a GPU or a display attached to it. This means that rich environments which involve agents using camera-based visual observations might be slower. Requirements Docker Unity Linux Build Support Component. Make sure to select the Linux Build Support component when installing Unity. Setup Download the Unity Installer and add the Linux Build Support Component Download and install Docker if you don't have it setup on your machine. Since Docker runs a container in an environment that is isolated from the host machine, a mounted directory in your host machine is used to share data, e.g. the trainer configuration file, Unity executable and TensorFlow graph. For convenience, we created an empty unity-volume directory at the root of the repository for this purpose, but feel free to use any other directory. The remainder of this guide assumes that the unity-volume directory is the one used. Usage Using Docker for ML-Agents involves three steps: building the Unity environment with specific flags, building a Docker container and, finally, running the container. If you are not familiar with building a Unity environment for ML-Agents, please read through our Getting Started with the 3D Balance Ball Example guide first. Build the Environment (Optional) If you want to used the Editor to perform training, you can skip this step. Since Docker typically runs a container sharing a (linux) kernel with the host machine, the Unity environment has to be built for the linux platform . When building a Unity environment, please select the following options from the the Build Settings window: Set the Target Platform to Linux Set the Architecture to x86_64 Then click Build , pick an environment name (e.g. 3DBall ) and set the output directory to unity-volume . After building, ensure that the file <environment-name>.x86_64 and subdirectory <environment-name>_Data/ are created under unity-volume . Build the Docker Container First, make sure the Docker engine is running on your machine. Then build the Docker container by calling the following command at the top-level of the repository: docker build -t <image-name> . Replace <image-name> with a name for the Docker image, e.g. balance.ball.v0.1 . Run the Docker Container Run the Docker container by calling the following command at the top-level of the repository: docker run -it --name <container-name> \\ --mount type=bind,source=\"$(pwd)\"/unity-volume,target=/unity-volume \\ -p 5005:5005 \\ -p 6006:6006 \\ <image-name>:latest \\ <trainer-config-file> \\ --env=<environment-name> \\ --train \\ --run-id=<run-id> Notes on argument values: <container-name> is used to identify the container (in case you want to interrupt and terminate it). This is optional and Docker will generate a random name if this is not set. Note that this must be unique for every run of a Docker image. <image-name> references the image name used when building the container. <environment-name> (Optional) : If you are training with a linux executable, this is the name of the executable. If you are training in the Editor, do not pass a <environment-name> argument and press the Play button in Unity when the message \"Start training by pressing the Play button in the Unity Editor\" is displayed on the screen. source : Reference to the path in your host OS where you will store the Unity executable. target : Tells Docker to mount the source path as a disk with this name. trainer-config-file , train , run-id : ML-Agents arguments passed to mlagents-learn . trainer-config-file is the filename of the trainer config file, train trains the algorithm, and run-id is used to tag each experiment with a unique identifier. We recommend placing the trainer-config file inside unity-volume so that the container has access to the file. To train with a 3DBall environment executable, the command would be: docker run -it --name 3DBallContainer.first.trial \\ --mount type=bind,source=\"$(pwd)\"/unity-volume,target=/unity-volume \\ -p 5005:5005 \\ -p 6006:6006 \\ balance.ball.v0.1:latest 3DBall \\ /unity-volume/trainer_config.yaml \\ --env=/unity-volume/3DBall \\ --train \\ --run-id=3dball_first_trial For more detail on Docker mounts, check out these docs from Docker. NOTE If you are training using docker for environments that use visual observations, you may need to increase the default memory that Docker allocates for the container. For example, see here for instructions for Docker for Mac. Running Tensorboard You can run Tensorboard to monitor your training instance on http://localhost:6006: docker exec -it <container-name> tensorboard --logdir /unity-volume/results --host 0.0.0.0 With our previous 3DBall example, this command would look like this: docker exec -it 3DBallContainer.first.trial tensorboard --logdir /unity-volume/results --host 0.0.0.0 For more details on Tensorboard, check out the documentation about Using Tensorboard . Stopping Container and Saving State If you are satisfied with the training progress, you can stop the Docker container while saving state by either using Ctrl+C or \u2318+C (Mac) or by using the following command: docker kill --signal=SIGINT <container-name> <container-name> is the name of the container specified in the earlier docker run command. If you didn't specify one, you can find the randomly generated identifier by running docker container ls .","title":"Using Docker For ML-Agents (Deprecated)"},{"location":"Using-Docker/#using-docker-for-ml-agents-deprecated","text":":warning: Note: We no longer use this guide ourselves and so it may not work correctly. We've decided to keep it up just in case it is helpful to you. We currently offer a solution for Windows and Mac users who would like to do training or inference using Docker. This option may be appealing to those who would like to avoid installing Python and TensorFlow themselves. The current setup forces both TensorFlow and Unity to only rely on the CPU for computations. Consequently, our Docker simulation does not use a GPU and uses Xvfb to do visual rendering. Xvfb is a utility that enables ML-Agents (or any other application) to do rendering virtually i.e. it does not assume that the machine running ML-Agents has a GPU or a display attached to it. This means that rich environments which involve agents using camera-based visual observations might be slower.","title":"Using Docker For ML-Agents (Deprecated)"},{"location":"Using-Docker/#requirements","text":"Docker Unity Linux Build Support Component. Make sure to select the Linux Build Support component when installing Unity.","title":"Requirements"},{"location":"Using-Docker/#setup","text":"Download the Unity Installer and add the Linux Build Support Component Download and install Docker if you don't have it setup on your machine. Since Docker runs a container in an environment that is isolated from the host machine, a mounted directory in your host machine is used to share data, e.g. the trainer configuration file, Unity executable and TensorFlow graph. For convenience, we created an empty unity-volume directory at the root of the repository for this purpose, but feel free to use any other directory. The remainder of this guide assumes that the unity-volume directory is the one used.","title":"Setup"},{"location":"Using-Docker/#usage","text":"Using Docker for ML-Agents involves three steps: building the Unity environment with specific flags, building a Docker container and, finally, running the container. If you are not familiar with building a Unity environment for ML-Agents, please read through our Getting Started with the 3D Balance Ball Example guide first.","title":"Usage"},{"location":"Using-Docker/#build-the-environment-optional","text":"If you want to used the Editor to perform training, you can skip this step. Since Docker typically runs a container sharing a (linux) kernel with the host machine, the Unity environment has to be built for the linux platform . When building a Unity environment, please select the following options from the the Build Settings window: Set the Target Platform to Linux Set the Architecture to x86_64 Then click Build , pick an environment name (e.g. 3DBall ) and set the output directory to unity-volume . After building, ensure that the file <environment-name>.x86_64 and subdirectory <environment-name>_Data/ are created under unity-volume .","title":"Build the Environment (Optional)"},{"location":"Using-Docker/#build-the-docker-container","text":"First, make sure the Docker engine is running on your machine. Then build the Docker container by calling the following command at the top-level of the repository: docker build -t <image-name> . Replace <image-name> with a name for the Docker image, e.g. balance.ball.v0.1 .","title":"Build the Docker Container"},{"location":"Using-Docker/#run-the-docker-container","text":"Run the Docker container by calling the following command at the top-level of the repository: docker run -it --name <container-name> \\ --mount type=bind,source=\"$(pwd)\"/unity-volume,target=/unity-volume \\ -p 5005:5005 \\ -p 6006:6006 \\ <image-name>:latest \\ <trainer-config-file> \\ --env=<environment-name> \\ --train \\ --run-id=<run-id> Notes on argument values: <container-name> is used to identify the container (in case you want to interrupt and terminate it). This is optional and Docker will generate a random name if this is not set. Note that this must be unique for every run of a Docker image. <image-name> references the image name used when building the container. <environment-name> (Optional) : If you are training with a linux executable, this is the name of the executable. If you are training in the Editor, do not pass a <environment-name> argument and press the Play button in Unity when the message \"Start training by pressing the Play button in the Unity Editor\" is displayed on the screen. source : Reference to the path in your host OS where you will store the Unity executable. target : Tells Docker to mount the source path as a disk with this name. trainer-config-file , train , run-id : ML-Agents arguments passed to mlagents-learn . trainer-config-file is the filename of the trainer config file, train trains the algorithm, and run-id is used to tag each experiment with a unique identifier. We recommend placing the trainer-config file inside unity-volume so that the container has access to the file. To train with a 3DBall environment executable, the command would be: docker run -it --name 3DBallContainer.first.trial \\ --mount type=bind,source=\"$(pwd)\"/unity-volume,target=/unity-volume \\ -p 5005:5005 \\ -p 6006:6006 \\ balance.ball.v0.1:latest 3DBall \\ /unity-volume/trainer_config.yaml \\ --env=/unity-volume/3DBall \\ --train \\ --run-id=3dball_first_trial For more detail on Docker mounts, check out these docs from Docker. NOTE If you are training using docker for environments that use visual observations, you may need to increase the default memory that Docker allocates for the container. For example, see here for instructions for Docker for Mac.","title":"Run the Docker Container"},{"location":"Using-Docker/#running-tensorboard","text":"You can run Tensorboard to monitor your training instance on http://localhost:6006: docker exec -it <container-name> tensorboard --logdir /unity-volume/results --host 0.0.0.0 With our previous 3DBall example, this command would look like this: docker exec -it 3DBallContainer.first.trial tensorboard --logdir /unity-volume/results --host 0.0.0.0 For more details on Tensorboard, check out the documentation about Using Tensorboard .","title":"Running Tensorboard"},{"location":"Using-Docker/#stopping-container-and-saving-state","text":"If you are satisfied with the training progress, you can stop the Docker container while saving state by either using Ctrl+C or \u2318+C (Mac) or by using the following command: docker kill --signal=SIGINT <container-name> <container-name> is the name of the container specified in the earlier docker run command. If you didn't specify one, you can find the randomly generated identifier by running docker container ls .","title":"Stopping Container and Saving State"},{"location":"Using-Tensorboard/","text":"Using TensorBoard to Observe Training The ML-Agents Toolkit saves statistics during learning session that you can view with a TensorFlow utility named, TensorBoard . The mlagents-learn command saves training statistics to a folder named results , organized by the run-id value you assign to a training session. In order to observe the training process, either during training or afterward, start TensorBoard: Open a terminal or console window: Navigate to the directory where the ML-Agents Toolkit is installed. From the command line run: tensorboard --logdir results --port 6006 Open a browser window and navigate to localhost:6006 . Note: The default port TensorBoard uses is 6006. If there is an existing session running on port 6006 a new session can be launched on an open port using the --port option. Note: If you don't assign a run-id identifier, mlagents-learn uses the default string, \"ppo\". You can delete the folders under the results directory to clear out old statistics. On the left side of the TensorBoard window, you can select which of the training runs you want to display. You can select multiple run-ids to compare statistics. The TensorBoard window also provides options for how to display and smooth graphs. The ML-Agents Toolkit training statistics The ML-Agents training program saves the following statistics: Environment Statistics Environment/Lesson - Plots the progress from lesson to lesson. Only interesting when performing curriculum training. Environment/Cumulative Reward - The mean cumulative episode reward over all agents. Should increase during a successful training session. Environment/Episode Length - The mean length of each episode in the environment for all agents. Is Training Is Training - A boolean indicating if the agent is updating its model. Policy Statistics Policy/Entropy (PPO; SAC) - How random the decisions of the model are. Should slowly decrease during a successful training process. If it decreases too quickly, the beta hyperparameter should be increased. Policy/Learning Rate (PPO; SAC) - How large a step the training algorithm takes as it searches for the optimal policy. Should decrease over time. Policy/Entropy Coefficient (SAC) - Determines the relative importance of the entropy term. This value is adjusted automatically so that the agent retains some amount of randomness during training. Policy/Extrinsic Reward (PPO; SAC) - This corresponds to the mean cumulative reward received from the environment per-episode. Policy/Value Estimate (PPO; SAC) - The mean value estimate for all states visited by the agent. Should increase during a successful training session. Policy/Curiosity Reward (PPO/SAC+Curiosity) - This corresponds to the mean cumulative intrinsic reward generated per-episode. Policy/Curiosity Value Estimate (PPO/SAC+Curiosity) - The agent's value estimate for the curiosity reward. Policy/GAIL Reward (PPO/SAC+GAIL) - This corresponds to the mean cumulative discriminator-based reward generated per-episode. Policy/GAIL Value Estimate (PPO/SAC+GAIL) - The agent's value estimate for the GAIL reward. Policy/GAIL Policy Estimate (PPO/SAC+GAIL) - The discriminator's estimate for states and actions generated by the policy. Policy/GAIL Expert Estimate (PPO/SAC+GAIL) - The discriminator's estimate for states and actions drawn from expert demonstrations. Learning Loss Functions Losses/Policy Loss (PPO; SAC) - The mean magnitude of policy loss function. Correlates to how much the policy (process for deciding actions) is changing. The magnitude of this should decrease during a successful training session. Losses/Value Loss (PPO; SAC) - The mean loss of the value function update. Correlates to how well the model is able to predict the value of each state. This should increase while the agent is learning, and then decrease once the reward stabilizes. Losses/Forward Loss (PPO/SAC+Curiosity) - The mean magnitude of the forward model loss function. Corresponds to how well the model is able to predict the new observation encoding. Losses/Inverse Loss (PPO/SAC+Curiosity) - The mean magnitude of the inverse model loss function. Corresponds to how well the model is able to predict the action taken between two observations. Losses/Pretraining Loss (BC) - The mean magnitude of the behavioral cloning loss. Corresponds to how well the model imitates the demonstration data. Losses/GAIL Loss (GAIL) - The mean magnitude of the GAIL discriminator loss. Corresponds to how well the model imitates the demonstration data. Self-Play Self-Play/ELO (Self-Play) - ELO measures the relative skill level between two players. In a proper training run, the ELO of the agent should steadily increase. Exporting Data from TensorBoard To export timeseries data in CSV or JSON format, check the \"Show data download links\" in the upper left. This will enable download links below each chart. Custom Metrics from Unity To get custom metrics from a C# environment into TensorBoard, you can use the StatsRecorder : var statsRecorder = Academy.Instance.StatsRecorder; statsRecorder.Add(\"MyMetric\", 1.0);","title":"Using TensorBoard to Observe Training"},{"location":"Using-Tensorboard/#using-tensorboard-to-observe-training","text":"The ML-Agents Toolkit saves statistics during learning session that you can view with a TensorFlow utility named, TensorBoard . The mlagents-learn command saves training statistics to a folder named results , organized by the run-id value you assign to a training session. In order to observe the training process, either during training or afterward, start TensorBoard: Open a terminal or console window: Navigate to the directory where the ML-Agents Toolkit is installed. From the command line run: tensorboard --logdir results --port 6006 Open a browser window and navigate to localhost:6006 . Note: The default port TensorBoard uses is 6006. If there is an existing session running on port 6006 a new session can be launched on an open port using the --port option. Note: If you don't assign a run-id identifier, mlagents-learn uses the default string, \"ppo\". You can delete the folders under the results directory to clear out old statistics. On the left side of the TensorBoard window, you can select which of the training runs you want to display. You can select multiple run-ids to compare statistics. The TensorBoard window also provides options for how to display and smooth graphs.","title":"Using TensorBoard to Observe Training"},{"location":"Using-Tensorboard/#the-ml-agents-toolkit-training-statistics","text":"The ML-Agents training program saves the following statistics:","title":"The ML-Agents Toolkit training statistics"},{"location":"Using-Tensorboard/#environment-statistics","text":"Environment/Lesson - Plots the progress from lesson to lesson. Only interesting when performing curriculum training. Environment/Cumulative Reward - The mean cumulative episode reward over all agents. Should increase during a successful training session. Environment/Episode Length - The mean length of each episode in the environment for all agents.","title":"Environment Statistics"},{"location":"Using-Tensorboard/#is-training","text":"Is Training - A boolean indicating if the agent is updating its model.","title":"Is Training"},{"location":"Using-Tensorboard/#policy-statistics","text":"Policy/Entropy (PPO; SAC) - How random the decisions of the model are. Should slowly decrease during a successful training process. If it decreases too quickly, the beta hyperparameter should be increased. Policy/Learning Rate (PPO; SAC) - How large a step the training algorithm takes as it searches for the optimal policy. Should decrease over time. Policy/Entropy Coefficient (SAC) - Determines the relative importance of the entropy term. This value is adjusted automatically so that the agent retains some amount of randomness during training. Policy/Extrinsic Reward (PPO; SAC) - This corresponds to the mean cumulative reward received from the environment per-episode. Policy/Value Estimate (PPO; SAC) - The mean value estimate for all states visited by the agent. Should increase during a successful training session. Policy/Curiosity Reward (PPO/SAC+Curiosity) - This corresponds to the mean cumulative intrinsic reward generated per-episode. Policy/Curiosity Value Estimate (PPO/SAC+Curiosity) - The agent's value estimate for the curiosity reward. Policy/GAIL Reward (PPO/SAC+GAIL) - This corresponds to the mean cumulative discriminator-based reward generated per-episode. Policy/GAIL Value Estimate (PPO/SAC+GAIL) - The agent's value estimate for the GAIL reward. Policy/GAIL Policy Estimate (PPO/SAC+GAIL) - The discriminator's estimate for states and actions generated by the policy. Policy/GAIL Expert Estimate (PPO/SAC+GAIL) - The discriminator's estimate for states and actions drawn from expert demonstrations.","title":"Policy Statistics"},{"location":"Using-Tensorboard/#learning-loss-functions","text":"Losses/Policy Loss (PPO; SAC) - The mean magnitude of policy loss function. Correlates to how much the policy (process for deciding actions) is changing. The magnitude of this should decrease during a successful training session. Losses/Value Loss (PPO; SAC) - The mean loss of the value function update. Correlates to how well the model is able to predict the value of each state. This should increase while the agent is learning, and then decrease once the reward stabilizes. Losses/Forward Loss (PPO/SAC+Curiosity) - The mean magnitude of the forward model loss function. Corresponds to how well the model is able to predict the new observation encoding. Losses/Inverse Loss (PPO/SAC+Curiosity) - The mean magnitude of the inverse model loss function. Corresponds to how well the model is able to predict the action taken between two observations. Losses/Pretraining Loss (BC) - The mean magnitude of the behavioral cloning loss. Corresponds to how well the model imitates the demonstration data. Losses/GAIL Loss (GAIL) - The mean magnitude of the GAIL discriminator loss. Corresponds to how well the model imitates the demonstration data.","title":"Learning Loss Functions"},{"location":"Using-Tensorboard/#self-play","text":"Self-Play/ELO (Self-Play) - ELO measures the relative skill level between two players. In a proper training run, the ELO of the agent should steadily increase.","title":"Self-Play"},{"location":"Using-Tensorboard/#exporting-data-from-tensorboard","text":"To export timeseries data in CSV or JSON format, check the \"Show data download links\" in the upper left. This will enable download links below each chart.","title":"Exporting Data from TensorBoard"},{"location":"Using-Tensorboard/#custom-metrics-from-unity","text":"To get custom metrics from a C# environment into TensorBoard, you can use the StatsRecorder : var statsRecorder = Academy.Instance.StatsRecorder; statsRecorder.Add(\"MyMetric\", 1.0);","title":"Custom Metrics from Unity"},{"location":"Using-Virtual-Environment/","text":"Using Virtual Environment What is a Virtual Environment? A Virtual Environment is a self contained directory tree that contains a Python installation for a particular version of Python, plus a number of additional packages. To learn more about Virtual Environments see here . Why should I use a Virtual Environment? A Virtual Environment keeps all dependencies for the Python project separate from dependencies of other projects. This has a few advantages: It makes dependency management for the project easy. It enables using and testing of different library versions by quickly spinning up a new environment and verifying the compatibility of the code with the different version. Python Version Requirement (Required) This guide has been tested with Python 3.6 through Python 3.8. Newer versions might not have support for the dependent libraries, so are not recommended. Installing Pip (Required) Download the get-pip.py file using the command curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py Run the following python3 get-pip.py Check pip version using pip3 -V Note (for Ubuntu users): If the ModuleNotFoundError: No module named 'distutils.util' error is encountered, then python3-distutils needs to be installed. Install python3-distutils using sudo apt-get install python3-distutils Mac OS X Setup Create a folder where the virtual environments will reside $ mkdir ~/python-envs To create a new environment named sample-env execute $ python3 -m venv ~/python-envs/sample-env To activate the environment execute $ source ~/python-envs/sample-env/bin/activate Upgrade to the latest pip version using $ pip3 install --upgrade pip Upgrade to the latest setuptools version using $ pip3 install --upgrade setuptools To deactivate the environment execute $ deactivate (you can reactivate the environment using the same activate command listed above) Ubuntu Setup Install the python3-venv package using $ sudo apt-get install python3-venv Follow the steps in the Mac OS X installation. Windows Setup Create a folder where the virtual environments will reside md python-envs To create a new environment named sample-env execute python -m venv python-envs\\sample-env To activate the environment execute python-envs\\sample-env\\Scripts\\activate Upgrade to the latest pip version using pip install --upgrade pip To deactivate the environment execute deactivate (you can reactivate the environment using the same activate command listed above) Note: Verify that you are using Python 3.6 or Python 3.7. Launch a command prompt using cmd and execute python --version to verify the version. Python3 installation may require admin privileges on Windows. This guide is for Windows 10 using a 64-bit architecture only.","title":"Using Virtual Environment"},{"location":"Using-Virtual-Environment/#using-virtual-environment","text":"","title":"Using Virtual Environment"},{"location":"Using-Virtual-Environment/#what-is-a-virtual-environment","text":"A Virtual Environment is a self contained directory tree that contains a Python installation for a particular version of Python, plus a number of additional packages. To learn more about Virtual Environments see here .","title":"What is a Virtual Environment?"},{"location":"Using-Virtual-Environment/#why-should-i-use-a-virtual-environment","text":"A Virtual Environment keeps all dependencies for the Python project separate from dependencies of other projects. This has a few advantages: It makes dependency management for the project easy. It enables using and testing of different library versions by quickly spinning up a new environment and verifying the compatibility of the code with the different version.","title":"Why should I use a Virtual Environment?"},{"location":"Using-Virtual-Environment/#python-version-requirement-required","text":"This guide has been tested with Python 3.6 through Python 3.8. Newer versions might not have support for the dependent libraries, so are not recommended.","title":"Python Version Requirement (Required)"},{"location":"Using-Virtual-Environment/#installing-pip-required","text":"Download the get-pip.py file using the command curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py Run the following python3 get-pip.py Check pip version using pip3 -V Note (for Ubuntu users): If the ModuleNotFoundError: No module named 'distutils.util' error is encountered, then python3-distutils needs to be installed. Install python3-distutils using sudo apt-get install python3-distutils","title":"Installing Pip (Required)"},{"location":"Using-Virtual-Environment/#mac-os-x-setup","text":"Create a folder where the virtual environments will reside $ mkdir ~/python-envs To create a new environment named sample-env execute $ python3 -m venv ~/python-envs/sample-env To activate the environment execute $ source ~/python-envs/sample-env/bin/activate Upgrade to the latest pip version using $ pip3 install --upgrade pip Upgrade to the latest setuptools version using $ pip3 install --upgrade setuptools To deactivate the environment execute $ deactivate (you can reactivate the environment using the same activate command listed above)","title":"Mac OS X Setup"},{"location":"Using-Virtual-Environment/#ubuntu-setup","text":"Install the python3-venv package using $ sudo apt-get install python3-venv Follow the steps in the Mac OS X installation.","title":"Ubuntu Setup"},{"location":"Using-Virtual-Environment/#windows-setup","text":"Create a folder where the virtual environments will reside md python-envs To create a new environment named sample-env execute python -m venv python-envs\\sample-env To activate the environment execute python-envs\\sample-env\\Scripts\\activate Upgrade to the latest pip version using pip install --upgrade pip To deactivate the environment execute deactivate (you can reactivate the environment using the same activate command listed above) Note: Verify that you are using Python 3.6 or Python 3.7. Launch a command prompt using cmd and execute python --version to verify the version. Python3 installation may require admin privileges on Windows. This guide is for Windows 10 using a 64-bit architecture only.","title":"Windows Setup"},{"location":"Versioning/","text":"ML-Agents Versioning Context As the ML-Agents project evolves into a more mature product, we want to communicate the process we use to version our packages and the data that flows into, through, and out of them clearly. Our project now has four packages (1 Unity, 3 Python) along with artifacts that are produced as well as consumed. This document covers the versioning for these packages and artifacts. GitHub Releases Up until now, all packages were in lockstep in-terms of versioning. As a result, the GitHub releases were tagged with the version of all those packages (e.g. v0.15.0, v0.15.1) and labeled accordingly. With the decoupling of package versions, we now need to revisit our GitHub release tagging. The proposal is that we move towards an integer release numbering for our repo and each such release will call out specific version upgrades of each package. For instance, with the April 30th release , we will have: - GitHub Release 1 (branch name: release_1_branch ) - com.unity.ml-agents release 1.0.0 - ml-agents release 0.16.0 - ml-agents-envs release 0.16.0 - gym-unity release 0.16.0 Our release cadence will not be affected by these versioning changes. We will keep having monthly releases to fix bugs and release new features. Packages All of the software packages, and their generated artifacts will be versioned. Any automation tools will not be versioned. Unity package Package name: com.unity.ml-agents - Versioned following Semantic Versioning Guidelines - This package consumes an artifact of the training process: the .nn file. These files are integer versioned and currently at version 2. The com.unity.ml-agents package will need to support the version of .nn files which existed at its 1.0.0 release. For example, consider that com.unity.ml-agents is at version 1.0.0 and the NN files are at version 2. If the NN files change to version 3, the next release of com.unity.ml-agents at version 1.1.0 guarantees it will be able to read both of these formats. If the NN files were to change to version 4 and com.unity.ml-agents to version 2.0.0, support for NN versions 2 and 3 could be dropped for com.unity.ml-agents version 2.0.0. - This package produces one artifact, the .demo files. These files will have integer versioning. This means their version will increment by 1 at each change. The com.unity.ml-agents package must be backward compatible with version changes that occur between minor versions. - To summarize, the artifacts produced and consumed by com.unity.ml-agents are guaranteed to be supported for 1.x.x versions of com.unity.ml-agents. We intend to provide stability for our users by moving to a 1.0.0 release of com.unity.ml-agents. Python Packages Package names: ml-agents / ml-agents-envs / gym-unity - The python packages remain in \"Beta.\" This means that breaking changes to the public API of the python packages can change without having to have a major version bump. Historically, the python and C# packages were in version lockstep. This is no longer the case. The python packages will remain in lockstep with each other for now, while the C# package will follow its own versioning as is appropriate. However, the python package versions may diverge in the future. - While the python packages will remain in Beta for now, we acknowledge that the most heavily used portion of our python interface is the mlagents-learn CLI and strive to make this part of our API backward compatible. We are actively working on this and expect to have a stable CLI in the next few weeks. Communicator Packages which communicate: com.unity.ml-agents / ml-agents-envs Another entity of the ML-Agents Toolkit that requires versioning is the communication layer between C# and Python, which will follow also semantic versioning. This guarantees a level of backward compatibility between different versions of C# and Python packages which communicate. Any Communicator version 1.x.x of the Unity package should be compatible with any 1.x.x Communicator Version in Python. An RLCapabilities struct keeps track of which features exist. This struct is passed from C# to Python, and another from Python to C#. With this feature level granularity, we can notify users more specifically about feature limitations based on what's available in both C# and Python. These notifications will be logged to the python terminal, or to the Unity Editor Console. Side Channels The communicator is what manages data transfer between Unity and Python for the core training loop. Side Channels are another means of data transfer between Unity and Python. Side Channels are not versioned, but have been designed to support backward compatibility for what they are. As of today, we provide 4 side channels: - FloatProperties: shared float data between Unity - Python (bidirectional) - RawBytes: raw data that can be sent Unity - Python (bidirectional) - EngineConfig: a set of numeric fields in a pre-defined order sent from Python to Unity - Stats: (name, value, agg) messages sent from Unity to Python Aside from the specific implementations of side channels we provide (and use ourselves), the Side Channel interface is made available for users to create their own custom side channels. As such, we guarantee that the built in SideChannel interface between Unity and Python is backward compatible in packages that share the same major version.","title":"Versioning"},{"location":"Versioning/#ml-agents-versioning","text":"","title":"ML-Agents Versioning"},{"location":"Versioning/#context","text":"As the ML-Agents project evolves into a more mature product, we want to communicate the process we use to version our packages and the data that flows into, through, and out of them clearly. Our project now has four packages (1 Unity, 3 Python) along with artifacts that are produced as well as consumed. This document covers the versioning for these packages and artifacts.","title":"Context"},{"location":"Versioning/#github-releases","text":"Up until now, all packages were in lockstep in-terms of versioning. As a result, the GitHub releases were tagged with the version of all those packages (e.g. v0.15.0, v0.15.1) and labeled accordingly. With the decoupling of package versions, we now need to revisit our GitHub release tagging. The proposal is that we move towards an integer release numbering for our repo and each such release will call out specific version upgrades of each package. For instance, with the April 30th release , we will have: - GitHub Release 1 (branch name: release_1_branch ) - com.unity.ml-agents release 1.0.0 - ml-agents release 0.16.0 - ml-agents-envs release 0.16.0 - gym-unity release 0.16.0 Our release cadence will not be affected by these versioning changes. We will keep having monthly releases to fix bugs and release new features.","title":"GitHub Releases"},{"location":"Versioning/#packages","text":"All of the software packages, and their generated artifacts will be versioned. Any automation tools will not be versioned.","title":"Packages"},{"location":"Versioning/#unity-package","text":"Package name: com.unity.ml-agents - Versioned following Semantic Versioning Guidelines - This package consumes an artifact of the training process: the .nn file. These files are integer versioned and currently at version 2. The com.unity.ml-agents package will need to support the version of .nn files which existed at its 1.0.0 release. For example, consider that com.unity.ml-agents is at version 1.0.0 and the NN files are at version 2. If the NN files change to version 3, the next release of com.unity.ml-agents at version 1.1.0 guarantees it will be able to read both of these formats. If the NN files were to change to version 4 and com.unity.ml-agents to version 2.0.0, support for NN versions 2 and 3 could be dropped for com.unity.ml-agents version 2.0.0. - This package produces one artifact, the .demo files. These files will have integer versioning. This means their version will increment by 1 at each change. The com.unity.ml-agents package must be backward compatible with version changes that occur between minor versions. - To summarize, the artifacts produced and consumed by com.unity.ml-agents are guaranteed to be supported for 1.x.x versions of com.unity.ml-agents. We intend to provide stability for our users by moving to a 1.0.0 release of com.unity.ml-agents.","title":"Unity package"},{"location":"Versioning/#python-packages","text":"Package names: ml-agents / ml-agents-envs / gym-unity - The python packages remain in \"Beta.\" This means that breaking changes to the public API of the python packages can change without having to have a major version bump. Historically, the python and C# packages were in version lockstep. This is no longer the case. The python packages will remain in lockstep with each other for now, while the C# package will follow its own versioning as is appropriate. However, the python package versions may diverge in the future. - While the python packages will remain in Beta for now, we acknowledge that the most heavily used portion of our python interface is the mlagents-learn CLI and strive to make this part of our API backward compatible. We are actively working on this and expect to have a stable CLI in the next few weeks.","title":"Python Packages"},{"location":"Versioning/#communicator","text":"Packages which communicate: com.unity.ml-agents / ml-agents-envs Another entity of the ML-Agents Toolkit that requires versioning is the communication layer between C# and Python, which will follow also semantic versioning. This guarantees a level of backward compatibility between different versions of C# and Python packages which communicate. Any Communicator version 1.x.x of the Unity package should be compatible with any 1.x.x Communicator Version in Python. An RLCapabilities struct keeps track of which features exist. This struct is passed from C# to Python, and another from Python to C#. With this feature level granularity, we can notify users more specifically about feature limitations based on what's available in both C# and Python. These notifications will be logged to the python terminal, or to the Unity Editor Console.","title":"Communicator"},{"location":"Versioning/#side-channels","text":"The communicator is what manages data transfer between Unity and Python for the core training loop. Side Channels are another means of data transfer between Unity and Python. Side Channels are not versioned, but have been designed to support backward compatibility for what they are. As of today, we provide 4 side channels: - FloatProperties: shared float data between Unity - Python (bidirectional) - RawBytes: raw data that can be sent Unity - Python (bidirectional) - EngineConfig: a set of numeric fields in a pre-defined order sent from Python to Unity - Stats: (name, value, agg) messages sent from Unity to Python Aside from the specific implementations of side channels we provide (and use ourselves), the Side Channel interface is made available for users to create their own custom side channels. As such, we guarantee that the built in SideChannel interface between Unity and Python is backward compatible in packages that share the same major version.","title":"Side Channels"},{"location":"doxygen/Readme/","text":"Doxygen files To generate the API reference as HTML files, run: doxygen dox-ml-agents.conf","title":"Doxygen files"},{"location":"doxygen/Readme/#doxygen-files","text":"To generate the API reference as HTML files, run: doxygen dox-ml-agents.conf","title":"Doxygen files"}]}